{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 19 05:26:16 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      6%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.7.6\n",
      "IPython 7.13.0\n",
      "\n",
      "numpy 1.18.5\n",
      "pandas 1.0.3\n",
      "torch 1.5.1\n",
      "transformers 2.11.0\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -v -p numpy,pandas,torch,transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.45.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data and Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "train = pd.read_csv(\"../input/sentiment-analysis-of-tweets/train.txt\")\n",
    "test = pd.read_csv(\"../input/sentiment-analysis-of-tweets/test_samples.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I\\u2019m going t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263405084770172928</td>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit\\u002c watch Rafa an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262163168678248449</td>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I\\u2019m a GSP fan\\u002c i just h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel\\u2019s Iron Dome c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>262682041215234048</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Tehran\\u002c Mon Amour: Obama Tried to Establi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id sentiment  \\\n",
       "0  264183816548130816  positive   \n",
       "1  263405084770172928  negative   \n",
       "2  262163168678248449  negative   \n",
       "3  264249301910310912  negative   \n",
       "4  262682041215234048   neutral   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  Gas by my house hit $3.39!!!! I\\u2019m going t...  \n",
       "1  Theo Walcott is still shit\\u002c watch Rafa an...  \n",
       "2  its not that I\\u2019m a GSP fan\\u002c i just h...  \n",
       "3  Iranian general says Israel\\u2019s Iron Dome c...  \n",
       "4  Tehran\\u002c Mon Amour: Obama Tried to Establi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264183816548130816</th>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I\\u2019m going t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263405084770172928</th>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit\\u002c watch Rafa an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262163168678248449</th>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I\\u2019m a GSP fan\\u002c i just h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264249301910310912</th>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel\\u2019s Iron Dome c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262682041215234048</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Tehran\\u002c Mon Amour: Obama Tried to Establi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sentiment  \\\n",
       "tweet_id                       \n",
       "264183816548130816  positive   \n",
       "263405084770172928  negative   \n",
       "262163168678248449  negative   \n",
       "264249301910310912  negative   \n",
       "262682041215234048   neutral   \n",
       "\n",
       "                                                           tweet_text  \n",
       "tweet_id                                                               \n",
       "264183816548130816  Gas by my house hit $3.39!!!! I\\u2019m going t...  \n",
       "263405084770172928  Theo Walcott is still shit\\u002c watch Rafa an...  \n",
       "262163168678248449  its not that I\\u2019m a GSP fan\\u002c i just h...  \n",
       "264249301910310912  Iranian general says Israel\\u2019s Iron Dome c...  \n",
       "262682041215234048  Tehran\\u002c Mon Amour: Obama Tried to Establi...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.set_index('tweet_id', inplace = True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21465</td>\n",
       "      <td>21465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3</td>\n",
       "      <td>21454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>positive</td>\n",
       "      <td>Perseverance is failing 19 times and succeedin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>9064</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                         tweet_text\n",
       "count      21465                                              21465\n",
       "unique         3                                              21454\n",
       "top     positive  Perseverance is failing 19 times and succeedin...\n",
       "freq        9064                                                  6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 21465 entries, 264183816548130816 to 518290874300514304\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   sentiment   21465 non-null  object\n",
      " 1   tweet_text  21465 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 503.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f90e9d7a990>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize = (10, 7))\n",
    "sns.countplot(data = train, x = 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    9064\n",
       "neutral     9014\n",
       "negative    3387\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_labels = train.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 0, 'negative': 1, 'neutral': 2}\n"
     ]
    }
   ],
   "source": [
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "  label_dict[possible_label] = index\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'] = train.sentiment.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264183816548130816</th>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I\\u2019m going t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263405084770172928</th>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit\\u002c watch Rafa an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262163168678248449</th>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I\\u2019m a GSP fan\\u002c i just h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264249301910310912</th>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel\\u2019s Iron Dome c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262682041215234048</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Tehran\\u002c Mon Amour: Obama Tried to Establi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sentiment  \\\n",
       "tweet_id                       \n",
       "264183816548130816  positive   \n",
       "263405084770172928  negative   \n",
       "262163168678248449  negative   \n",
       "264249301910310912  negative   \n",
       "262682041215234048   neutral   \n",
       "\n",
       "                                                           tweet_text  label  \n",
       "tweet_id                                                                      \n",
       "264183816548130816  Gas by my house hit $3.39!!!! I\\u2019m going t...      0  \n",
       "263405084770172928  Theo Walcott is still shit\\u002c watch Rafa an...      1  \n",
       "262163168678248449  its not that I\\u2019m a GSP fan\\u002c i just h...      1  \n",
       "264249301910310912  Iranian general says Israel\\u2019s Iron Dome c...      1  \n",
       "262682041215234048  Tehran\\u002c Mon Amour: Obama Tried to Establi...      2  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train.index.values,\n",
    "    train.label.values,\n",
    "    test_size = 0.1,\n",
    "    random_state = 17,\n",
    "    stratify = train.label.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['data_type'] = ['null'] * train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264183816548130816</th>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I\\u2019m going t...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263405084770172928</th>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit\\u002c watch Rafa an...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262163168678248449</th>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I\\u2019m a GSP fan\\u002c i just h...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264249301910310912</th>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel\\u2019s Iron Dome c...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262682041215234048</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Tehran\\u002c Mon Amour: Obama Tried to Establi...</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sentiment  \\\n",
       "tweet_id                       \n",
       "264183816548130816  positive   \n",
       "263405084770172928  negative   \n",
       "262163168678248449  negative   \n",
       "264249301910310912  negative   \n",
       "262682041215234048   neutral   \n",
       "\n",
       "                                                           tweet_text  label  \\\n",
       "tweet_id                                                                       \n",
       "264183816548130816  Gas by my house hit $3.39!!!! I\\u2019m going t...      0   \n",
       "263405084770172928  Theo Walcott is still shit\\u002c watch Rafa an...      1   \n",
       "262163168678248449  its not that I\\u2019m a GSP fan\\u002c i just h...      1   \n",
       "264249301910310912  Iranian general says Israel\\u2019s Iron Dome c...      1   \n",
       "262682041215234048  Tehran\\u002c Mon Amour: Obama Tried to Establi...      2   \n",
       "\n",
       "                   data_type  \n",
       "tweet_id                      \n",
       "264183816548130816     train  \n",
       "263405084770172928     train  \n",
       "262163168678248449     train  \n",
       "264249301910310912     train  \n",
       "262682041215234048     train  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[X_train, 'data_type'] = 'train'\n",
    "train.loc[X_val, 'data_type'] = 'val'\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">negative</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>3048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">neutral</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>8113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">positive</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>8157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           tweet_text\n",
       "sentiment label data_type            \n",
       "negative  1     train            3048\n",
       "                val               339\n",
       "neutral   2     train            8113\n",
       "                val               901\n",
       "positive  0     train            8157\n",
       "                val               907"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(['sentiment', 'label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Tokenizer and Encoding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf7e61c97194141bfb1b2eb75e5484f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    train[train['data_type'] == 'train'].tweet_text.values,\n",
    "    add_special_tokens = True,\n",
    "    return_attention_mask = True,\n",
    "    pad_to_max_length = True,\n",
    "    max_length = 128,\n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    train[train['data_type'] == 'val'].tweet_text.values,\n",
    "    add_special_tokens = True,\n",
    "    return_attention_mask = True,\n",
    "    pad_to_max_length = True,\n",
    "    max_length = 128,\n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_mask_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(train[train['data_type'] == 'train'].label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_mask_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(train[train['data_type'] == 'val'].label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TensorDataset(input_ids_train,\n",
    "                              attention_mask_train, labels_train)\n",
    "\n",
    "dataset_val = TensorDataset(input_ids_val,\n",
    "                              attention_mask_val, labels_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19318\n",
      "2147\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train))\n",
    "print(len(dataset_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up BERT pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2dc9643c04545e88f6f60d51903aa7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53e1f9f16964049886f82e263e6c0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = len(label_dict),\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    sampler = RandomSampler(dataset_train),\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    sampler = RandomSampler(dataset_val),\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Optimizer and Schduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 1e-5,\n",
    "    eps = 1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(dataloader_train) * epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Performence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_func(preds, labels):\n",
    "  preds_flat = np.argmax(preds, axis = 1).flatten()\n",
    "  labels_flat = labels.flatten()\n",
    "  return f1_score(labels_flat, preds_flat, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_class(preds, labels):\n",
    "  label_dict_inverse = { v:k for k, v in label_dict.items() }\n",
    "  preds_flat = np.argmax(preds, axis = 1).flatten()\n",
    "  labels_flat = labels.flatten()\n",
    "  for label in np.unique(labels_flat):\n",
    "    y_preds = preds_flat[ labels_flat == label ]\n",
    "    y_true = labels_flat[ labels_flat == label ]\n",
    "    print(f'class: {label_dict_inverse[label]}')\n",
    "    print(f'accuracy: {len(y_preds[y_preds == label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  loss_val_total = 0\n",
    "  predictions, true_vals = [], []\n",
    "\n",
    "  for batch in dataloader_val:\n",
    "\n",
    "    batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "    inputs = {\n",
    "        'input_ids': batch[0],\n",
    "        'attention_mask': batch[1],\n",
    "        'labels': batch[2],\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**inputs)\n",
    "\n",
    "    loss = outputs[0]\n",
    "    logits = outputs[1]\n",
    "    loss_val_total += loss.item()\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = inputs['labels'].cpu().numpy()\n",
    "    predictions.append(logits)\n",
    "    true_vals.append(label_ids)\n",
    "\n",
    "  loss_val_avg = loss_val_total/len(dataloader_val)\n",
    "\n",
    "  predictions = np.concatenate(predictions, axis= 0)\n",
    "  true_vals = np.concatenate(true_vals, axis = 0)\n",
    "\n",
    "  return loss_val_avg, predictions, true_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|          | 0/604 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 0/604 [00:01<?, ?it/s, training loss=0.355]\u001b[A\n",
      "Epoch 1:   0%|          | 1/604 [00:01<12:27,  1.24s/it, training loss=0.355]\u001b[A\n",
      "Epoch 1:   0%|          | 1/604 [00:01<12:27,  1.24s/it, training loss=0.338]\u001b[A\n",
      "Epoch 1:   0%|          | 2/604 [00:01<09:57,  1.01it/s, training loss=0.338]\u001b[A\n",
      "Epoch 1:   0%|          | 2/604 [00:02<09:57,  1.01it/s, training loss=0.392]\u001b[A\n",
      "Epoch 1:   0%|          | 3/604 [00:02<08:09,  1.23it/s, training loss=0.392]\u001b[A\n",
      "Epoch 1:   0%|          | 3/604 [00:02<08:09,  1.23it/s, training loss=0.343]\u001b[A\n",
      "Epoch 1:   1%|          | 4/604 [00:02<06:52,  1.45it/s, training loss=0.343]\u001b[A\n",
      "Epoch 1:   1%|          | 4/604 [00:02<06:52,  1.45it/s, training loss=0.326]\u001b[A\n",
      "Epoch 1:   1%|          | 5/604 [00:02<05:57,  1.67it/s, training loss=0.326]\u001b[A\n",
      "Epoch 1:   1%|          | 5/604 [00:03<05:57,  1.67it/s, training loss=0.338]\u001b[A\n",
      "Epoch 1:   1%|          | 6/604 [00:03<05:19,  1.87it/s, training loss=0.338]\u001b[A\n",
      "Epoch 1:   1%|          | 6/604 [00:03<05:19,  1.87it/s, training loss=0.360]\u001b[A\n",
      "Epoch 1:   1%|          | 7/604 [00:03<04:52,  2.04it/s, training loss=0.360]\u001b[A\n",
      "Epoch 1:   1%|          | 7/604 [00:03<04:52,  2.04it/s, training loss=0.343]\u001b[A\n",
      "Epoch 1:   1%|▏         | 8/604 [00:03<04:33,  2.18it/s, training loss=0.343]\u001b[A\n",
      "Epoch 1:   1%|▏         | 8/604 [00:04<04:33,  2.18it/s, training loss=0.359]\u001b[A\n",
      "Epoch 1:   1%|▏         | 9/604 [00:04<04:20,  2.29it/s, training loss=0.359]\u001b[A\n",
      "Epoch 1:   1%|▏         | 9/604 [00:04<04:20,  2.29it/s, training loss=0.343]\u001b[A\n",
      "Epoch 1:   2%|▏         | 10/604 [00:04<04:10,  2.37it/s, training loss=0.343]\u001b[A\n",
      "Epoch 1:   2%|▏         | 10/604 [00:05<04:10,  2.37it/s, training loss=0.351]\u001b[A\n",
      "Epoch 1:   2%|▏         | 11/604 [00:05<04:03,  2.43it/s, training loss=0.351]\u001b[A\n",
      "Epoch 1:   2%|▏         | 11/604 [00:05<04:03,  2.43it/s, training loss=0.348]\u001b[A\n",
      "Epoch 1:   2%|▏         | 12/604 [00:05<03:58,  2.48it/s, training loss=0.348]\u001b[A\n",
      "Epoch 1:   2%|▏         | 12/604 [00:05<03:58,  2.48it/s, training loss=0.300]\u001b[A\n",
      "Epoch 1:   2%|▏         | 13/604 [00:05<03:55,  2.51it/s, training loss=0.300]\u001b[A\n",
      "Epoch 1:   2%|▏         | 13/604 [00:06<03:55,  2.51it/s, training loss=0.329]\u001b[A\n",
      "Epoch 1:   2%|▏         | 14/604 [00:06<03:53,  2.53it/s, training loss=0.329]\u001b[A\n",
      "Epoch 1:   2%|▏         | 14/604 [00:06<03:53,  2.53it/s, training loss=0.353]\u001b[A\n",
      "Epoch 1:   2%|▏         | 15/604 [00:06<03:51,  2.55it/s, training loss=0.353]\u001b[A\n",
      "Epoch 1:   2%|▏         | 15/604 [00:07<03:51,  2.55it/s, training loss=0.326]\u001b[A\n",
      "Epoch 1:   3%|▎         | 16/604 [00:07<03:49,  2.56it/s, training loss=0.326]\u001b[A\n",
      "Epoch 1:   3%|▎         | 16/604 [00:07<03:49,  2.56it/s, training loss=0.327]\u001b[A\n",
      "Epoch 1:   3%|▎         | 17/604 [00:07<03:48,  2.57it/s, training loss=0.327]\u001b[A\n",
      "Epoch 1:   3%|▎         | 17/604 [00:07<03:48,  2.57it/s, training loss=0.345]\u001b[A\n",
      "Epoch 1:   3%|▎         | 18/604 [00:07<03:48,  2.57it/s, training loss=0.345]\u001b[A\n",
      "Epoch 1:   3%|▎         | 18/604 [00:08<03:48,  2.57it/s, training loss=0.349]\u001b[A\n",
      "Epoch 1:   3%|▎         | 19/604 [00:08<03:49,  2.55it/s, training loss=0.349]\u001b[A\n",
      "Epoch 1:   3%|▎         | 19/604 [00:08<03:49,  2.55it/s, training loss=0.308]\u001b[A\n",
      "Epoch 1:   3%|▎         | 20/604 [00:08<03:48,  2.56it/s, training loss=0.308]\u001b[A\n",
      "Epoch 1:   3%|▎         | 20/604 [00:09<03:48,  2.56it/s, training loss=0.343]\u001b[A\n",
      "Epoch 1:   3%|▎         | 21/604 [00:09<03:47,  2.57it/s, training loss=0.343]\u001b[A\n",
      "Epoch 1:   3%|▎         | 21/604 [00:09<03:47,  2.57it/s, training loss=0.338]\u001b[A\n",
      "Epoch 1:   4%|▎         | 22/604 [00:09<03:46,  2.57it/s, training loss=0.338]\u001b[A\n",
      "Epoch 1:   4%|▎         | 22/604 [00:09<03:46,  2.57it/s, training loss=0.365]\u001b[A\n",
      "Epoch 1:   4%|▍         | 23/604 [00:09<03:45,  2.57it/s, training loss=0.365]\u001b[A\n",
      "Epoch 1:   4%|▍         | 23/604 [00:10<03:45,  2.57it/s, training loss=0.312]\u001b[A\n",
      "Epoch 1:   4%|▍         | 24/604 [00:10<03:45,  2.58it/s, training loss=0.312]\u001b[A\n",
      "Epoch 1:   4%|▍         | 24/604 [00:10<03:45,  2.58it/s, training loss=0.328]\u001b[A\n",
      "Epoch 1:   4%|▍         | 25/604 [00:10<03:44,  2.58it/s, training loss=0.328]\u001b[A\n",
      "Epoch 1:   4%|▍         | 25/604 [00:10<03:44,  2.58it/s, training loss=0.352]\u001b[A\n",
      "Epoch 1:   4%|▍         | 26/604 [00:10<03:47,  2.54it/s, training loss=0.352]\u001b[A\n",
      "Epoch 1:   4%|▍         | 26/604 [00:11<03:47,  2.54it/s, training loss=0.353]\u001b[A\n",
      "Epoch 1:   4%|▍         | 27/604 [00:11<03:46,  2.55it/s, training loss=0.353]\u001b[A\n",
      "Epoch 1:   4%|▍         | 27/604 [00:11<03:46,  2.55it/s, training loss=0.341]\u001b[A\n",
      "Epoch 1:   5%|▍         | 28/604 [00:11<03:44,  2.56it/s, training loss=0.341]\u001b[A\n",
      "Epoch 1:   5%|▍         | 28/604 [00:12<03:44,  2.56it/s, training loss=0.346]\u001b[A\n",
      "Epoch 1:   5%|▍         | 29/604 [00:12<03:45,  2.55it/s, training loss=0.346]\u001b[A\n",
      "Epoch 1:   5%|▍         | 29/604 [00:12<03:45,  2.55it/s, training loss=0.340]\u001b[A\n",
      "Epoch 1:   5%|▍         | 30/604 [00:12<03:44,  2.56it/s, training loss=0.340]\u001b[A\n",
      "Epoch 1:   5%|▍         | 30/604 [00:12<03:44,  2.56it/s, training loss=0.311]\u001b[A\n",
      "Epoch 1:   5%|▌         | 31/604 [00:12<03:49,  2.49it/s, training loss=0.311]\u001b[A\n",
      "Epoch 1:   5%|▌         | 31/604 [00:13<03:49,  2.49it/s, training loss=0.324]\u001b[A\n",
      "Epoch 1:   5%|▌         | 32/604 [00:13<03:47,  2.51it/s, training loss=0.324]\u001b[A\n",
      "Epoch 1:   5%|▌         | 32/604 [00:13<03:47,  2.51it/s, training loss=0.325]\u001b[A\n",
      "Epoch 1:   5%|▌         | 33/604 [00:13<03:45,  2.53it/s, training loss=0.325]\u001b[A\n",
      "Epoch 1:   5%|▌         | 33/604 [00:14<03:45,  2.53it/s, training loss=0.320]\u001b[A\n",
      "Epoch 1:   6%|▌         | 34/604 [00:14<03:44,  2.54it/s, training loss=0.320]\u001b[A\n",
      "Epoch 1:   6%|▌         | 34/604 [00:14<03:44,  2.54it/s, training loss=0.349]\u001b[A\n",
      "Epoch 1:   6%|▌         | 35/604 [00:14<03:42,  2.56it/s, training loss=0.349]\u001b[A\n",
      "Epoch 1:   6%|▌         | 35/604 [00:14<03:42,  2.56it/s, training loss=0.347]\u001b[A\n",
      "Epoch 1:   6%|▌         | 36/604 [00:14<03:41,  2.57it/s, training loss=0.347]\u001b[A\n",
      "Epoch 1:   6%|▌         | 36/604 [00:15<03:41,  2.57it/s, training loss=0.315]\u001b[A\n",
      "Epoch 1:   6%|▌         | 37/604 [00:15<03:40,  2.57it/s, training loss=0.315]\u001b[A\n",
      "Epoch 1:   6%|▌         | 37/604 [00:15<03:40,  2.57it/s, training loss=0.316]\u001b[A\n",
      "Epoch 1:   6%|▋         | 38/604 [00:15<03:39,  2.58it/s, training loss=0.316]\u001b[A\n",
      "Epoch 1:   6%|▋         | 38/604 [00:16<03:39,  2.58it/s, training loss=0.335]\u001b[A\n",
      "Epoch 1:   6%|▋         | 39/604 [00:16<03:39,  2.58it/s, training loss=0.335]\u001b[A\n",
      "Epoch 1:   6%|▋         | 39/604 [00:16<03:39,  2.58it/s, training loss=0.348]\u001b[A\n",
      "Epoch 1:   7%|▋         | 40/604 [00:16<03:39,  2.57it/s, training loss=0.348]\u001b[A\n",
      "Epoch 1:   7%|▋         | 40/604 [00:16<03:39,  2.57it/s, training loss=0.354]\u001b[A\n",
      "Epoch 1:   7%|▋         | 41/604 [00:16<03:38,  2.57it/s, training loss=0.354]\u001b[A\n",
      "Epoch 1:   7%|▋         | 41/604 [00:17<03:38,  2.57it/s, training loss=0.333]\u001b[A\n",
      "Epoch 1:   7%|▋         | 42/604 [00:17<03:38,  2.57it/s, training loss=0.333]\u001b[A\n",
      "Epoch 1:   7%|▋         | 42/604 [00:17<03:38,  2.57it/s, training loss=0.321]\u001b[A\n",
      "Epoch 1:   7%|▋         | 43/604 [00:17<03:37,  2.58it/s, training loss=0.321]\u001b[A\n",
      "Epoch 1:   7%|▋         | 43/604 [00:18<03:37,  2.58it/s, training loss=0.348]\u001b[A\n",
      "Epoch 1:   7%|▋         | 44/604 [00:18<03:36,  2.58it/s, training loss=0.348]\u001b[A\n",
      "Epoch 1:   7%|▋         | 44/604 [00:18<03:36,  2.58it/s, training loss=0.306]\u001b[A\n",
      "Epoch 1:   7%|▋         | 45/604 [00:18<03:37,  2.57it/s, training loss=0.306]\u001b[A\n",
      "Epoch 1:   7%|▋         | 45/604 [00:18<03:37,  2.57it/s, training loss=0.320]\u001b[A\n",
      "Epoch 1:   8%|▊         | 46/604 [00:18<03:36,  2.57it/s, training loss=0.320]\u001b[A\n",
      "Epoch 1:   8%|▊         | 46/604 [00:19<03:36,  2.57it/s, training loss=0.346]\u001b[A\n",
      "Epoch 1:   8%|▊         | 47/604 [00:19<03:35,  2.58it/s, training loss=0.346]\u001b[A\n",
      "Epoch 1:   8%|▊         | 47/604 [00:19<03:35,  2.58it/s, training loss=0.345]\u001b[A\n",
      "Epoch 1:   8%|▊         | 48/604 [00:19<03:35,  2.58it/s, training loss=0.345]\u001b[A\n",
      "Epoch 1:   8%|▊         | 48/604 [00:19<03:35,  2.58it/s, training loss=0.284]\u001b[A\n",
      "Epoch 1:   8%|▊         | 49/604 [00:19<03:34,  2.59it/s, training loss=0.284]\u001b[A\n",
      "Epoch 1:   8%|▊         | 49/604 [00:20<03:34,  2.59it/s, training loss=0.316]\u001b[A\n",
      "Epoch 1:   8%|▊         | 50/604 [00:20<03:33,  2.59it/s, training loss=0.316]\u001b[A\n",
      "Epoch 1:   8%|▊         | 50/604 [00:20<03:33,  2.59it/s, training loss=0.296]\u001b[A\n",
      "Epoch 1:   8%|▊         | 51/604 [00:20<03:33,  2.59it/s, training loss=0.296]\u001b[A\n",
      "Epoch 1:   8%|▊         | 51/604 [00:21<03:33,  2.59it/s, training loss=0.283]\u001b[A\n",
      "Epoch 1:   9%|▊         | 52/604 [00:21<03:32,  2.59it/s, training loss=0.283]\u001b[A\n",
      "Epoch 1:   9%|▊         | 52/604 [00:21<03:32,  2.59it/s, training loss=0.350]\u001b[A\n",
      "Epoch 1:   9%|▉         | 53/604 [00:21<03:32,  2.59it/s, training loss=0.350]\u001b[A\n",
      "Epoch 1:   9%|▉         | 53/604 [00:21<03:32,  2.59it/s, training loss=0.287]\u001b[A\n",
      "Epoch 1:   9%|▉         | 54/604 [00:21<03:32,  2.59it/s, training loss=0.287]\u001b[A\n",
      "Epoch 1:   9%|▉         | 54/604 [00:22<03:32,  2.59it/s, training loss=0.333]\u001b[A\n",
      "Epoch 1:   9%|▉         | 55/604 [00:22<03:33,  2.57it/s, training loss=0.333]\u001b[A\n",
      "Epoch 1:   9%|▉         | 55/604 [00:22<03:33,  2.57it/s, training loss=0.308]\u001b[A\n",
      "Epoch 1:   9%|▉         | 56/604 [00:22<03:32,  2.58it/s, training loss=0.308]\u001b[A\n",
      "Epoch 1:   9%|▉         | 56/604 [00:23<03:32,  2.58it/s, training loss=0.313]\u001b[A\n",
      "Epoch 1:   9%|▉         | 57/604 [00:23<03:31,  2.59it/s, training loss=0.313]\u001b[A\n",
      "Epoch 1:   9%|▉         | 57/604 [00:23<03:31,  2.59it/s, training loss=0.324]\u001b[A\n",
      "Epoch 1:  10%|▉         | 58/604 [00:23<03:31,  2.58it/s, training loss=0.324]\u001b[A\n",
      "Epoch 1:  10%|▉         | 58/604 [00:23<03:31,  2.58it/s, training loss=0.261]\u001b[A\n",
      "Epoch 1:  10%|▉         | 59/604 [00:23<03:32,  2.57it/s, training loss=0.261]\u001b[A\n",
      "Epoch 1:  10%|▉         | 59/604 [00:24<03:32,  2.57it/s, training loss=0.321]\u001b[A\n",
      "Epoch 1:  10%|▉         | 60/604 [00:24<03:31,  2.57it/s, training loss=0.321]\u001b[A\n",
      "Epoch 1:  10%|▉         | 60/604 [00:24<03:31,  2.57it/s, training loss=0.315]\u001b[A\n",
      "Epoch 1:  10%|█         | 61/604 [00:24<03:30,  2.58it/s, training loss=0.315]\u001b[A\n",
      "Epoch 1:  10%|█         | 61/604 [00:24<03:30,  2.58it/s, training loss=0.300]\u001b[A\n",
      "Epoch 1:  10%|█         | 62/604 [00:24<03:29,  2.58it/s, training loss=0.300]\u001b[A\n",
      "Epoch 1:  10%|█         | 62/604 [00:25<03:29,  2.58it/s, training loss=0.305]\u001b[A\n",
      "Epoch 1:  10%|█         | 63/604 [00:25<03:29,  2.58it/s, training loss=0.305]\u001b[A\n",
      "Epoch 1:  10%|█         | 63/604 [00:25<03:29,  2.58it/s, training loss=0.321]\u001b[A\n",
      "Epoch 1:  11%|█         | 64/604 [00:25<03:28,  2.58it/s, training loss=0.321]\u001b[A\n",
      "Epoch 1:  11%|█         | 64/604 [00:26<03:28,  2.58it/s, training loss=0.300]\u001b[A\n",
      "Epoch 1:  11%|█         | 65/604 [00:26<03:28,  2.59it/s, training loss=0.300]\u001b[A\n",
      "Epoch 1:  11%|█         | 65/604 [00:26<03:28,  2.59it/s, training loss=0.351]\u001b[A\n",
      "Epoch 1:  11%|█         | 66/604 [00:26<03:27,  2.59it/s, training loss=0.351]\u001b[A\n",
      "Epoch 1:  11%|█         | 66/604 [00:26<03:27,  2.59it/s, training loss=0.264]\u001b[A\n",
      "Epoch 1:  11%|█         | 67/604 [00:26<03:27,  2.59it/s, training loss=0.264]\u001b[A\n",
      "Epoch 1:  11%|█         | 67/604 [00:27<03:27,  2.59it/s, training loss=0.271]\u001b[A\n",
      "Epoch 1:  11%|█▏        | 68/604 [00:27<03:26,  2.59it/s, training loss=0.271]\u001b[A\n",
      "Epoch 1:  11%|█▏        | 68/604 [00:27<03:26,  2.59it/s, training loss=0.336]\u001b[A\n",
      "Epoch 1:  11%|█▏        | 69/604 [00:27<03:26,  2.59it/s, training loss=0.336]\u001b[A\n",
      "Epoch 1:  11%|█▏        | 69/604 [00:28<03:26,  2.59it/s, training loss=0.321]\u001b[A\n",
      "Epoch 1:  12%|█▏        | 70/604 [00:28<03:26,  2.59it/s, training loss=0.321]\u001b[A\n",
      "Epoch 1:  12%|█▏        | 70/604 [00:28<03:26,  2.59it/s, training loss=0.300]\u001b[A\n",
      "Epoch 1:  12%|█▏        | 71/604 [00:28<03:25,  2.59it/s, training loss=0.300]\u001b[A\n",
      "Epoch 1:  12%|█▏        | 71/604 [00:28<03:25,  2.59it/s, training loss=0.285]\u001b[A\n",
      "Epoch 1:  12%|█▏        | 72/604 [00:28<03:25,  2.59it/s, training loss=0.285]\u001b[A\n",
      "Epoch 1:  12%|█▏        | 72/604 [00:29<03:25,  2.59it/s, training loss=0.351]\u001b[A\n",
      "Epoch 1:  12%|█▏        | 73/604 [00:29<03:24,  2.60it/s, training loss=0.351]\u001b[A\n",
      "Epoch 1:  12%|█▏        | 73/604 [00:29<03:24,  2.60it/s, training loss=0.291]\u001b[A\n",
      "Epoch 1:  12%|█▏        | 74/604 [00:29<03:24,  2.60it/s, training loss=0.291]\u001b[A\n",
      "Epoch 1:  12%|█▏        | 74/604 [00:29<03:24,  2.60it/s, training loss=0.334]\u001b[A\n",
      "Epoch 1:  12%|█▏        | 75/604 [00:29<03:23,  2.60it/s, training loss=0.334]\u001b[A\n",
      "Epoch 1:  12%|█▏        | 75/604 [00:30<03:23,  2.60it/s, training loss=0.288]\u001b[A\n",
      "Epoch 1:  13%|█▎        | 76/604 [00:30<03:23,  2.59it/s, training loss=0.288]\u001b[A\n",
      "Epoch 1:  13%|█▎        | 76/604 [00:30<03:23,  2.59it/s, training loss=0.270]\u001b[A\n",
      "Epoch 1:  13%|█▎        | 77/604 [00:30<03:23,  2.59it/s, training loss=0.270]\u001b[A\n",
      "Epoch 1:  13%|█▎        | 77/604 [00:31<03:23,  2.59it/s, training loss=0.299]\u001b[A\n",
      "Epoch 1:  13%|█▎        | 78/604 [00:31<03:22,  2.59it/s, training loss=0.299]\u001b[A\n",
      "Epoch 1:  13%|█▎        | 78/604 [00:31<03:22,  2.59it/s, training loss=0.309]\u001b[A\n",
      "Epoch 1:  13%|█▎        | 79/604 [00:31<03:22,  2.59it/s, training loss=0.309]\u001b[A\n",
      "Epoch 1:  13%|█▎        | 79/604 [00:31<03:22,  2.59it/s, training loss=0.258]\u001b[A\n",
      "Epoch 1:  13%|█▎        | 80/604 [00:31<03:22,  2.59it/s, training loss=0.258]\u001b[A\n",
      "Epoch 1:  13%|█▎        | 80/604 [00:32<03:22,  2.59it/s, training loss=0.282]\u001b[A\n",
      "Epoch 1:  13%|█▎        | 81/604 [00:32<03:23,  2.58it/s, training loss=0.282]\u001b[A\n",
      "Epoch 1:  13%|█▎        | 81/604 [00:32<03:23,  2.58it/s, training loss=0.269]\u001b[A\n",
      "Epoch 1:  14%|█▎        | 82/604 [00:32<03:22,  2.58it/s, training loss=0.269]\u001b[A\n",
      "Epoch 1:  14%|█▎        | 82/604 [00:33<03:22,  2.58it/s, training loss=0.279]\u001b[A\n",
      "Epoch 1:  14%|█▎        | 83/604 [00:33<03:21,  2.58it/s, training loss=0.279]\u001b[A\n",
      "Epoch 1:  14%|█▎        | 83/604 [00:33<03:21,  2.58it/s, training loss=0.276]\u001b[A\n",
      "Epoch 1:  14%|█▍        | 84/604 [00:33<03:22,  2.57it/s, training loss=0.276]\u001b[A\n",
      "Epoch 1:  14%|█▍        | 84/604 [00:33<03:22,  2.57it/s, training loss=0.283]\u001b[A\n",
      "Epoch 1:  14%|█▍        | 85/604 [00:33<03:21,  2.58it/s, training loss=0.283]\u001b[A\n",
      "Epoch 1:  14%|█▍        | 85/604 [00:34<03:21,  2.58it/s, training loss=0.277]\u001b[A\n",
      "Epoch 1:  14%|█▍        | 86/604 [00:34<03:22,  2.56it/s, training loss=0.277]\u001b[A\n",
      "Epoch 1:  14%|█▍        | 86/604 [00:34<03:22,  2.56it/s, training loss=0.285]\u001b[A\n",
      "Epoch 1:  14%|█▍        | 87/604 [00:34<03:24,  2.53it/s, training loss=0.285]\u001b[A\n",
      "Epoch 1:  14%|█▍        | 87/604 [00:35<03:24,  2.53it/s, training loss=0.262]\u001b[A\n",
      "Epoch 1:  15%|█▍        | 88/604 [00:35<03:24,  2.52it/s, training loss=0.262]\u001b[A\n",
      "Epoch 1:  15%|█▍        | 88/604 [00:35<03:24,  2.52it/s, training loss=0.339]\u001b[A\n",
      "Epoch 1:  15%|█▍        | 89/604 [00:35<03:22,  2.54it/s, training loss=0.339]\u001b[A\n",
      "Epoch 1:  15%|█▍        | 89/604 [00:35<03:22,  2.54it/s, training loss=0.274]\u001b[A\n",
      "Epoch 1:  15%|█▍        | 90/604 [00:35<03:21,  2.56it/s, training loss=0.274]\u001b[A\n",
      "Epoch 1:  15%|█▍        | 90/604 [00:36<03:21,  2.56it/s, training loss=0.270]\u001b[A\n",
      "Epoch 1:  15%|█▌        | 91/604 [00:36<03:20,  2.56it/s, training loss=0.270]\u001b[A\n",
      "Epoch 1:  15%|█▌        | 91/604 [00:36<03:20,  2.56it/s, training loss=0.305]\u001b[A\n",
      "Epoch 1:  15%|█▌        | 92/604 [00:36<03:19,  2.57it/s, training loss=0.305]\u001b[A\n",
      "Epoch 1:  15%|█▌        | 92/604 [00:37<03:19,  2.57it/s, training loss=0.304]\u001b[A\n",
      "Epoch 1:  15%|█▌        | 93/604 [00:37<03:18,  2.58it/s, training loss=0.304]\u001b[A\n",
      "Epoch 1:  15%|█▌        | 93/604 [00:37<03:18,  2.58it/s, training loss=0.308]\u001b[A\n",
      "Epoch 1:  16%|█▌        | 94/604 [00:37<03:18,  2.57it/s, training loss=0.308]\u001b[A\n",
      "Epoch 1:  16%|█▌        | 94/604 [00:37<03:18,  2.57it/s, training loss=0.296]\u001b[A\n",
      "Epoch 1:  16%|█▌        | 95/604 [00:37<03:17,  2.58it/s, training loss=0.296]\u001b[A\n",
      "Epoch 1:  16%|█▌        | 95/604 [00:38<03:17,  2.58it/s, training loss=0.295]\u001b[A\n",
      "Epoch 1:  16%|█▌        | 96/604 [00:38<03:17,  2.57it/s, training loss=0.295]\u001b[A\n",
      "Epoch 1:  16%|█▌        | 96/604 [00:38<03:17,  2.57it/s, training loss=0.290]\u001b[A\n",
      "Epoch 1:  16%|█▌        | 97/604 [00:38<03:16,  2.58it/s, training loss=0.290]\u001b[A\n",
      "Epoch 1:  16%|█▌        | 97/604 [00:38<03:16,  2.58it/s, training loss=0.279]\u001b[A\n",
      "Epoch 1:  16%|█▌        | 98/604 [00:38<03:16,  2.58it/s, training loss=0.279]\u001b[A\n",
      "Epoch 1:  16%|█▌        | 98/604 [00:39<03:16,  2.58it/s, training loss=0.264]\u001b[A\n",
      "Epoch 1:  16%|█▋        | 99/604 [00:39<03:15,  2.58it/s, training loss=0.264]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  16%|█▋        | 99/604 [00:39<03:15,  2.58it/s, training loss=0.262]\u001b[A\n",
      "Epoch 1:  17%|█▋        | 100/604 [00:39<03:14,  2.59it/s, training loss=0.262]\u001b[A\n",
      "Epoch 1:  17%|█▋        | 100/604 [00:40<03:14,  2.59it/s, training loss=0.262]\u001b[A\n",
      "Epoch 1:  17%|█▋        | 101/604 [00:40<03:18,  2.54it/s, training loss=0.262]\u001b[A\n",
      "Epoch 1:  17%|█▋        | 101/604 [00:40<03:18,  2.54it/s, training loss=0.314]\u001b[A\n",
      "Epoch 1:  17%|█▋        | 102/604 [00:40<03:18,  2.53it/s, training loss=0.314]\u001b[A\n",
      "Epoch 1:  17%|█▋        | 102/604 [00:40<03:18,  2.53it/s, training loss=0.316]\u001b[A\n",
      "Epoch 1:  17%|█▋        | 103/604 [00:40<03:18,  2.53it/s, training loss=0.316]\u001b[A\n",
      "Epoch 1:  17%|█▋        | 103/604 [00:41<03:18,  2.53it/s, training loss=0.279]\u001b[A\n",
      "Epoch 1:  17%|█▋        | 104/604 [00:41<03:16,  2.55it/s, training loss=0.279]\u001b[A\n",
      "Epoch 1:  17%|█▋        | 104/604 [00:41<03:16,  2.55it/s, training loss=0.315]\u001b[A\n",
      "Epoch 1:  17%|█▋        | 105/604 [00:41<03:15,  2.56it/s, training loss=0.315]\u001b[A\n",
      "Epoch 1:  17%|█▋        | 105/604 [00:42<03:15,  2.56it/s, training loss=0.278]\u001b[A\n",
      "Epoch 1:  18%|█▊        | 106/604 [00:42<03:13,  2.57it/s, training loss=0.278]\u001b[A\n",
      "Epoch 1:  18%|█▊        | 106/604 [00:42<03:13,  2.57it/s, training loss=0.235]\u001b[A\n",
      "Epoch 1:  18%|█▊        | 107/604 [00:42<03:15,  2.54it/s, training loss=0.235]\u001b[A\n",
      "Epoch 1:  18%|█▊        | 107/604 [00:42<03:15,  2.54it/s, training loss=0.261]\u001b[A\n",
      "Epoch 1:  18%|█▊        | 108/604 [00:42<03:14,  2.55it/s, training loss=0.261]\u001b[A\n",
      "Epoch 1:  18%|█▊        | 108/604 [00:43<03:14,  2.55it/s, training loss=0.270]\u001b[A\n",
      "Epoch 1:  18%|█▊        | 109/604 [00:43<03:13,  2.56it/s, training loss=0.270]\u001b[A\n",
      "Epoch 1:  18%|█▊        | 109/604 [00:43<03:13,  2.56it/s, training loss=0.263]\u001b[A\n",
      "Epoch 1:  18%|█▊        | 110/604 [00:43<03:16,  2.52it/s, training loss=0.263]\u001b[A\n",
      "Epoch 1:  18%|█▊        | 110/604 [00:44<03:16,  2.52it/s, training loss=0.254]\u001b[A\n",
      "Epoch 1:  18%|█▊        | 111/604 [00:44<03:14,  2.54it/s, training loss=0.254]\u001b[A\n",
      "Epoch 1:  18%|█▊        | 111/604 [00:44<03:14,  2.54it/s, training loss=0.292]\u001b[A\n",
      "Epoch 1:  19%|█▊        | 112/604 [00:44<03:12,  2.55it/s, training loss=0.292]\u001b[A\n",
      "Epoch 1:  19%|█▊        | 112/604 [00:44<03:12,  2.55it/s, training loss=0.247]\u001b[A\n",
      "Epoch 1:  19%|█▊        | 113/604 [00:44<03:11,  2.56it/s, training loss=0.247]\u001b[A\n",
      "Epoch 1:  19%|█▊        | 113/604 [00:45<03:11,  2.56it/s, training loss=0.263]\u001b[A\n",
      "Epoch 1:  19%|█▉        | 114/604 [00:45<03:16,  2.50it/s, training loss=0.263]\u001b[A\n",
      "Epoch 1:  19%|█▉        | 114/604 [00:45<03:16,  2.50it/s, training loss=0.255]\u001b[A\n",
      "Epoch 1:  19%|█▉        | 115/604 [00:45<03:18,  2.47it/s, training loss=0.255]\u001b[A\n",
      "Epoch 1:  19%|█▉        | 115/604 [00:46<03:18,  2.47it/s, training loss=0.303]\u001b[A\n",
      "Epoch 1:  19%|█▉        | 116/604 [00:46<03:18,  2.46it/s, training loss=0.303]\u001b[A\n",
      "Epoch 1:  19%|█▉        | 116/604 [00:46<03:18,  2.46it/s, training loss=0.230]\u001b[A\n",
      "Epoch 1:  19%|█▉        | 117/604 [00:46<03:15,  2.49it/s, training loss=0.230]\u001b[A\n",
      "Epoch 1:  19%|█▉        | 117/604 [00:46<03:15,  2.49it/s, training loss=0.300]\u001b[A\n",
      "Epoch 1:  20%|█▉        | 118/604 [00:46<03:13,  2.51it/s, training loss=0.300]\u001b[A\n",
      "Epoch 1:  20%|█▉        | 118/604 [00:47<03:13,  2.51it/s, training loss=0.237]\u001b[A\n",
      "Epoch 1:  20%|█▉        | 119/604 [00:47<03:11,  2.53it/s, training loss=0.237]\u001b[A\n",
      "Epoch 1:  20%|█▉        | 119/604 [00:47<03:11,  2.53it/s, training loss=0.261]\u001b[A\n",
      "Epoch 1:  20%|█▉        | 120/604 [00:47<03:12,  2.52it/s, training loss=0.261]\u001b[A\n",
      "Epoch 1:  20%|█▉        | 120/604 [00:48<03:12,  2.52it/s, training loss=0.282]\u001b[A\n",
      "Epoch 1:  20%|██        | 121/604 [00:48<03:10,  2.54it/s, training loss=0.282]\u001b[A\n",
      "Epoch 1:  20%|██        | 121/604 [00:48<03:10,  2.54it/s, training loss=0.285]\u001b[A\n",
      "Epoch 1:  20%|██        | 122/604 [00:48<03:09,  2.55it/s, training loss=0.285]\u001b[A\n",
      "Epoch 1:  20%|██        | 122/604 [00:48<03:09,  2.55it/s, training loss=0.244]\u001b[A\n",
      "Epoch 1:  20%|██        | 123/604 [00:48<03:08,  2.56it/s, training loss=0.244]\u001b[A\n",
      "Epoch 1:  20%|██        | 123/604 [00:49<03:08,  2.56it/s, training loss=0.289]\u001b[A\n",
      "Epoch 1:  21%|██        | 124/604 [00:49<03:07,  2.56it/s, training loss=0.289]\u001b[A\n",
      "Epoch 1:  21%|██        | 124/604 [00:49<03:07,  2.56it/s, training loss=0.252]\u001b[A\n",
      "Epoch 1:  21%|██        | 125/604 [00:49<03:06,  2.57it/s, training loss=0.252]\u001b[A\n",
      "Epoch 1:  21%|██        | 125/604 [00:49<03:06,  2.57it/s, training loss=0.234]\u001b[A\n",
      "Epoch 1:  21%|██        | 126/604 [00:49<03:05,  2.58it/s, training loss=0.234]\u001b[A\n",
      "Epoch 1:  21%|██        | 126/604 [00:50<03:05,  2.58it/s, training loss=0.287]\u001b[A\n",
      "Epoch 1:  21%|██        | 127/604 [00:50<03:04,  2.58it/s, training loss=0.287]\u001b[A\n",
      "Epoch 1:  21%|██        | 127/604 [00:50<03:04,  2.58it/s, training loss=0.275]\u001b[A\n",
      "Epoch 1:  21%|██        | 128/604 [00:50<03:04,  2.58it/s, training loss=0.275]\u001b[A\n",
      "Epoch 1:  21%|██        | 128/604 [00:51<03:04,  2.58it/s, training loss=0.242]\u001b[A\n",
      "Epoch 1:  21%|██▏       | 129/604 [00:51<03:04,  2.58it/s, training loss=0.242]\u001b[A\n",
      "Epoch 1:  21%|██▏       | 129/604 [00:51<03:04,  2.58it/s, training loss=0.266]\u001b[A\n",
      "Epoch 1:  22%|██▏       | 130/604 [00:51<03:03,  2.58it/s, training loss=0.266]\u001b[A\n",
      "Epoch 1:  22%|██▏       | 130/604 [00:51<03:03,  2.58it/s, training loss=0.293]\u001b[A\n",
      "Epoch 1:  22%|██▏       | 131/604 [00:51<03:03,  2.58it/s, training loss=0.293]\u001b[A\n",
      "Epoch 1:  22%|██▏       | 131/604 [00:52<03:03,  2.58it/s, training loss=0.360]\u001b[A\n",
      "Epoch 1:  22%|██▏       | 132/604 [00:52<03:02,  2.59it/s, training loss=0.360]\u001b[A\n",
      "Epoch 1:  22%|██▏       | 132/604 [00:52<03:02,  2.59it/s, training loss=0.290]\u001b[A\n",
      "Epoch 1:  22%|██▏       | 133/604 [00:52<03:01,  2.59it/s, training loss=0.290]\u001b[A\n",
      "Epoch 1:  22%|██▏       | 133/604 [00:53<03:01,  2.59it/s, training loss=0.254]\u001b[A\n",
      "Epoch 1:  22%|██▏       | 134/604 [00:53<03:01,  2.59it/s, training loss=0.254]\u001b[A\n",
      "Epoch 1:  22%|██▏       | 134/604 [00:53<03:01,  2.59it/s, training loss=0.287]\u001b[A\n",
      "Epoch 1:  22%|██▏       | 135/604 [00:53<03:01,  2.59it/s, training loss=0.287]\u001b[A\n",
      "Epoch 1:  22%|██▏       | 135/604 [00:53<03:01,  2.59it/s, training loss=0.260]\u001b[A\n",
      "Epoch 1:  23%|██▎       | 136/604 [00:53<03:00,  2.59it/s, training loss=0.260]\u001b[A\n",
      "Epoch 1:  23%|██▎       | 136/604 [00:54<03:00,  2.59it/s, training loss=0.237]\u001b[A\n",
      "Epoch 1:  23%|██▎       | 137/604 [00:54<03:00,  2.59it/s, training loss=0.237]\u001b[A\n",
      "Epoch 1:  23%|██▎       | 137/604 [00:54<03:00,  2.59it/s, training loss=0.239]\u001b[A\n",
      "Epoch 1:  23%|██▎       | 138/604 [00:54<02:59,  2.59it/s, training loss=0.239]\u001b[A\n",
      "Epoch 1:  23%|██▎       | 138/604 [00:54<02:59,  2.59it/s, training loss=0.271]\u001b[A\n",
      "Epoch 1:  23%|██▎       | 139/604 [00:54<02:59,  2.59it/s, training loss=0.271]\u001b[A\n",
      "Epoch 1:  23%|██▎       | 139/604 [00:55<02:59,  2.59it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  23%|██▎       | 140/604 [00:55<02:58,  2.59it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  23%|██▎       | 140/604 [00:55<02:58,  2.59it/s, training loss=0.282]\u001b[A\n",
      "Epoch 1:  23%|██▎       | 141/604 [00:55<02:58,  2.59it/s, training loss=0.282]\u001b[A\n",
      "Epoch 1:  23%|██▎       | 141/604 [00:56<02:58,  2.59it/s, training loss=0.266]\u001b[A\n",
      "Epoch 1:  24%|██▎       | 142/604 [00:56<02:58,  2.59it/s, training loss=0.266]\u001b[A\n",
      "Epoch 1:  24%|██▎       | 142/604 [00:56<02:58,  2.59it/s, training loss=0.222]\u001b[A\n",
      "Epoch 1:  24%|██▎       | 143/604 [00:56<02:57,  2.59it/s, training loss=0.222]\u001b[A\n",
      "Epoch 1:  24%|██▎       | 143/604 [00:56<02:57,  2.59it/s, training loss=0.256]\u001b[A\n",
      "Epoch 1:  24%|██▍       | 144/604 [00:56<03:00,  2.55it/s, training loss=0.256]\u001b[A\n",
      "Epoch 1:  24%|██▍       | 144/604 [00:57<03:00,  2.55it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  24%|██▍       | 145/604 [00:57<02:59,  2.55it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  24%|██▍       | 145/604 [00:57<02:59,  2.55it/s, training loss=0.321]\u001b[A\n",
      "Epoch 1:  24%|██▍       | 146/604 [00:57<02:58,  2.56it/s, training loss=0.321]\u001b[A\n",
      "Epoch 1:  24%|██▍       | 146/604 [00:58<02:58,  2.56it/s, training loss=0.238]\u001b[A\n",
      "Epoch 1:  24%|██▍       | 147/604 [00:58<02:57,  2.57it/s, training loss=0.238]\u001b[A\n",
      "Epoch 1:  24%|██▍       | 147/604 [00:58<02:57,  2.57it/s, training loss=0.224]\u001b[A\n",
      "Epoch 1:  25%|██▍       | 148/604 [00:58<02:57,  2.58it/s, training loss=0.224]\u001b[A\n",
      "Epoch 1:  25%|██▍       | 148/604 [00:58<02:57,  2.58it/s, training loss=0.279]\u001b[A\n",
      "Epoch 1:  25%|██▍       | 149/604 [00:58<02:56,  2.58it/s, training loss=0.279]\u001b[A\n",
      "Epoch 1:  25%|██▍       | 149/604 [00:59<02:56,  2.58it/s, training loss=0.270]\u001b[A\n",
      "Epoch 1:  25%|██▍       | 150/604 [00:59<02:55,  2.58it/s, training loss=0.270]\u001b[A\n",
      "Epoch 1:  25%|██▍       | 150/604 [00:59<02:55,  2.58it/s, training loss=0.240]\u001b[A\n",
      "Epoch 1:  25%|██▌       | 151/604 [00:59<02:55,  2.58it/s, training loss=0.240]\u001b[A\n",
      "Epoch 1:  25%|██▌       | 151/604 [01:00<02:55,  2.58it/s, training loss=0.266]\u001b[A\n",
      "Epoch 1:  25%|██▌       | 152/604 [01:00<02:55,  2.58it/s, training loss=0.266]\u001b[A\n",
      "Epoch 1:  25%|██▌       | 152/604 [01:00<02:55,  2.58it/s, training loss=0.233]\u001b[A\n",
      "Epoch 1:  25%|██▌       | 153/604 [01:00<02:54,  2.58it/s, training loss=0.233]\u001b[A\n",
      "Epoch 1:  25%|██▌       | 153/604 [01:00<02:54,  2.58it/s, training loss=0.259]\u001b[A\n",
      "Epoch 1:  25%|██▌       | 154/604 [01:00<02:53,  2.59it/s, training loss=0.259]\u001b[A\n",
      "Epoch 1:  25%|██▌       | 154/604 [01:01<02:53,  2.59it/s, training loss=0.293]\u001b[A\n",
      "Epoch 1:  26%|██▌       | 155/604 [01:01<02:53,  2.59it/s, training loss=0.293]\u001b[A\n",
      "Epoch 1:  26%|██▌       | 155/604 [01:01<02:53,  2.59it/s, training loss=0.279]\u001b[A\n",
      "Epoch 1:  26%|██▌       | 156/604 [01:01<02:53,  2.59it/s, training loss=0.279]\u001b[A\n",
      "Epoch 1:  26%|██▌       | 156/604 [01:01<02:53,  2.59it/s, training loss=0.331]\u001b[A\n",
      "Epoch 1:  26%|██▌       | 157/604 [01:01<02:52,  2.59it/s, training loss=0.331]\u001b[A\n",
      "Epoch 1:  26%|██▌       | 157/604 [01:02<02:52,  2.59it/s, training loss=0.213]\u001b[A\n",
      "Epoch 1:  26%|██▌       | 158/604 [01:02<02:52,  2.59it/s, training loss=0.213]\u001b[A\n",
      "Epoch 1:  26%|██▌       | 158/604 [01:02<02:52,  2.59it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  26%|██▋       | 159/604 [01:02<02:51,  2.59it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  26%|██▋       | 159/604 [01:03<02:51,  2.59it/s, training loss=0.239]\u001b[A\n",
      "Epoch 1:  26%|██▋       | 160/604 [01:03<02:51,  2.59it/s, training loss=0.239]\u001b[A\n",
      "Epoch 1:  26%|██▋       | 160/604 [01:03<02:51,  2.59it/s, training loss=0.248]\u001b[A\n",
      "Epoch 1:  27%|██▋       | 161/604 [01:03<02:52,  2.57it/s, training loss=0.248]\u001b[A\n",
      "Epoch 1:  27%|██▋       | 161/604 [01:03<02:52,  2.57it/s, training loss=0.296]\u001b[A\n",
      "Epoch 1:  27%|██▋       | 162/604 [01:03<02:51,  2.58it/s, training loss=0.296]\u001b[A\n",
      "Epoch 1:  27%|██▋       | 162/604 [01:04<02:51,  2.58it/s, training loss=0.322]\u001b[A\n",
      "Epoch 1:  27%|██▋       | 163/604 [01:04<02:51,  2.58it/s, training loss=0.322]\u001b[A\n",
      "Epoch 1:  27%|██▋       | 163/604 [01:04<02:51,  2.58it/s, training loss=0.213]\u001b[A\n",
      "Epoch 1:  27%|██▋       | 164/604 [01:04<02:50,  2.58it/s, training loss=0.213]\u001b[A\n",
      "Epoch 1:  27%|██▋       | 164/604 [01:05<02:50,  2.58it/s, training loss=0.271]\u001b[A\n",
      "Epoch 1:  27%|██▋       | 165/604 [01:05<02:49,  2.59it/s, training loss=0.271]\u001b[A\n",
      "Epoch 1:  27%|██▋       | 165/604 [01:05<02:49,  2.59it/s, training loss=0.280]\u001b[A\n",
      "Epoch 1:  27%|██▋       | 166/604 [01:05<02:49,  2.59it/s, training loss=0.280]\u001b[A\n",
      "Epoch 1:  27%|██▋       | 166/604 [01:05<02:49,  2.59it/s, training loss=0.215]\u001b[A\n",
      "Epoch 1:  28%|██▊       | 167/604 [01:05<02:49,  2.58it/s, training loss=0.215]\u001b[A\n",
      "Epoch 1:  28%|██▊       | 167/604 [01:06<02:49,  2.58it/s, training loss=0.251]\u001b[A\n",
      "Epoch 1:  28%|██▊       | 168/604 [01:06<02:48,  2.59it/s, training loss=0.251]\u001b[A\n",
      "Epoch 1:  28%|██▊       | 168/604 [01:06<02:48,  2.59it/s, training loss=0.261]\u001b[A\n",
      "Epoch 1:  28%|██▊       | 169/604 [01:06<02:47,  2.59it/s, training loss=0.261]\u001b[A\n",
      "Epoch 1:  28%|██▊       | 169/604 [01:06<02:47,  2.59it/s, training loss=0.251]\u001b[A\n",
      "Epoch 1:  28%|██▊       | 170/604 [01:06<02:47,  2.59it/s, training loss=0.251]\u001b[A\n",
      "Epoch 1:  28%|██▊       | 170/604 [01:07<02:47,  2.59it/s, training loss=0.264]\u001b[A\n",
      "Epoch 1:  28%|██▊       | 171/604 [01:07<02:47,  2.59it/s, training loss=0.264]\u001b[A\n",
      "Epoch 1:  28%|██▊       | 171/604 [01:07<02:47,  2.59it/s, training loss=0.238]\u001b[A\n",
      "Epoch 1:  28%|██▊       | 172/604 [01:07<02:50,  2.53it/s, training loss=0.238]\u001b[A\n",
      "Epoch 1:  28%|██▊       | 172/604 [01:08<02:50,  2.53it/s, training loss=0.259]\u001b[A\n",
      "Epoch 1:  29%|██▊       | 173/604 [01:08<02:50,  2.53it/s, training loss=0.259]\u001b[A\n",
      "Epoch 1:  29%|██▊       | 173/604 [01:08<02:50,  2.53it/s, training loss=0.197]\u001b[A\n",
      "Epoch 1:  29%|██▉       | 174/604 [01:08<02:48,  2.55it/s, training loss=0.197]\u001b[A\n",
      "Epoch 1:  29%|██▉       | 174/604 [01:08<02:48,  2.55it/s, training loss=0.301]\u001b[A\n",
      "Epoch 1:  29%|██▉       | 175/604 [01:08<02:47,  2.56it/s, training loss=0.301]\u001b[A\n",
      "Epoch 1:  29%|██▉       | 175/604 [01:09<02:47,  2.56it/s, training loss=0.312]\u001b[A\n",
      "Epoch 1:  29%|██▉       | 176/604 [01:09<02:46,  2.57it/s, training loss=0.312]\u001b[A\n",
      "Epoch 1:  29%|██▉       | 176/604 [01:09<02:46,  2.57it/s, training loss=0.191]\u001b[A\n",
      "Epoch 1:  29%|██▉       | 177/604 [01:09<02:45,  2.58it/s, training loss=0.191]\u001b[A\n",
      "Epoch 1:  29%|██▉       | 177/604 [01:10<02:45,  2.58it/s, training loss=0.240]\u001b[A\n",
      "Epoch 1:  29%|██▉       | 178/604 [01:10<02:45,  2.58it/s, training loss=0.240]\u001b[A\n",
      "Epoch 1:  29%|██▉       | 178/604 [01:10<02:45,  2.58it/s, training loss=0.200]\u001b[A\n",
      "Epoch 1:  30%|██▉       | 179/604 [01:10<02:44,  2.58it/s, training loss=0.200]\u001b[A\n",
      "Epoch 1:  30%|██▉       | 179/604 [01:10<02:44,  2.58it/s, training loss=0.312]\u001b[A\n",
      "Epoch 1:  30%|██▉       | 180/604 [01:10<02:43,  2.59it/s, training loss=0.312]\u001b[A\n",
      "Epoch 1:  30%|██▉       | 180/604 [01:11<02:43,  2.59it/s, training loss=0.274]\u001b[A\n",
      "Epoch 1:  30%|██▉       | 181/604 [01:11<02:43,  2.59it/s, training loss=0.274]\u001b[A\n",
      "Epoch 1:  30%|██▉       | 181/604 [01:11<02:43,  2.59it/s, training loss=0.287]\u001b[A\n",
      "Epoch 1:  30%|███       | 182/604 [01:11<02:43,  2.59it/s, training loss=0.287]\u001b[A\n",
      "Epoch 1:  30%|███       | 182/604 [01:12<02:43,  2.59it/s, training loss=0.225]\u001b[A\n",
      "Epoch 1:  30%|███       | 183/604 [01:12<02:42,  2.59it/s, training loss=0.225]\u001b[A\n",
      "Epoch 1:  30%|███       | 183/604 [01:12<02:42,  2.59it/s, training loss=0.238]\u001b[A\n",
      "Epoch 1:  30%|███       | 184/604 [01:12<02:42,  2.59it/s, training loss=0.238]\u001b[A\n",
      "Epoch 1:  30%|███       | 184/604 [01:12<02:42,  2.59it/s, training loss=0.289]\u001b[A\n",
      "Epoch 1:  31%|███       | 185/604 [01:12<02:41,  2.59it/s, training loss=0.289]\u001b[A\n",
      "Epoch 1:  31%|███       | 185/604 [01:13<02:41,  2.59it/s, training loss=0.206]\u001b[A\n",
      "Epoch 1:  31%|███       | 186/604 [01:13<02:42,  2.57it/s, training loss=0.206]\u001b[A\n",
      "Epoch 1:  31%|███       | 186/604 [01:13<02:42,  2.57it/s, training loss=0.182]\u001b[A\n",
      "Epoch 1:  31%|███       | 187/604 [01:13<02:41,  2.58it/s, training loss=0.182]\u001b[A\n",
      "Epoch 1:  31%|███       | 187/604 [01:14<02:41,  2.58it/s, training loss=0.235]\u001b[A\n",
      "Epoch 1:  31%|███       | 188/604 [01:14<02:43,  2.55it/s, training loss=0.235]\u001b[A\n",
      "Epoch 1:  31%|███       | 188/604 [01:14<02:43,  2.55it/s, training loss=0.215]\u001b[A\n",
      "Epoch 1:  31%|███▏      | 189/604 [01:14<02:42,  2.55it/s, training loss=0.215]\u001b[A\n",
      "Epoch 1:  31%|███▏      | 189/604 [01:14<02:42,  2.55it/s, training loss=0.203]\u001b[A\n",
      "Epoch 1:  31%|███▏      | 190/604 [01:14<02:41,  2.56it/s, training loss=0.203]\u001b[A\n",
      "Epoch 1:  31%|███▏      | 190/604 [01:15<02:41,  2.56it/s, training loss=0.226]\u001b[A\n",
      "Epoch 1:  32%|███▏      | 191/604 [01:15<02:40,  2.57it/s, training loss=0.226]\u001b[A\n",
      "Epoch 1:  32%|███▏      | 191/604 [01:15<02:40,  2.57it/s, training loss=0.243]\u001b[A\n",
      "Epoch 1:  32%|███▏      | 192/604 [01:15<02:40,  2.57it/s, training loss=0.243]\u001b[A\n",
      "Epoch 1:  32%|███▏      | 192/604 [01:15<02:40,  2.57it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  32%|███▏      | 193/604 [01:15<02:39,  2.58it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  32%|███▏      | 193/604 [01:16<02:39,  2.58it/s, training loss=0.237]\u001b[A\n",
      "Epoch 1:  32%|███▏      | 194/604 [01:16<02:39,  2.57it/s, training loss=0.237]\u001b[A\n",
      "Epoch 1:  32%|███▏      | 194/604 [01:16<02:39,  2.57it/s, training loss=0.292]\u001b[A\n",
      "Epoch 1:  32%|███▏      | 195/604 [01:16<02:38,  2.58it/s, training loss=0.292]\u001b[A\n",
      "Epoch 1:  32%|███▏      | 195/604 [01:17<02:38,  2.58it/s, training loss=0.257]\u001b[A\n",
      "Epoch 1:  32%|███▏      | 196/604 [01:17<02:38,  2.58it/s, training loss=0.257]\u001b[A\n",
      "Epoch 1:  32%|███▏      | 196/604 [01:17<02:38,  2.58it/s, training loss=0.216]\u001b[A\n",
      "Epoch 1:  33%|███▎      | 197/604 [01:17<02:39,  2.56it/s, training loss=0.216]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███▎      | 197/604 [01:17<02:39,  2.56it/s, training loss=0.223]\u001b[A\n",
      "Epoch 1:  33%|███▎      | 198/604 [01:17<02:38,  2.57it/s, training loss=0.223]\u001b[A\n",
      "Epoch 1:  33%|███▎      | 198/604 [01:18<02:38,  2.57it/s, training loss=0.208]\u001b[A\n",
      "Epoch 1:  33%|███▎      | 199/604 [01:18<02:37,  2.57it/s, training loss=0.208]\u001b[A\n",
      "Epoch 1:  33%|███▎      | 199/604 [01:18<02:37,  2.57it/s, training loss=0.243]\u001b[A\n",
      "Epoch 1:  33%|███▎      | 200/604 [01:18<02:36,  2.57it/s, training loss=0.243]\u001b[A\n",
      "Epoch 1:  33%|███▎      | 200/604 [01:19<02:36,  2.57it/s, training loss=0.236]\u001b[A\n",
      "Epoch 1:  33%|███▎      | 201/604 [01:19<02:38,  2.54it/s, training loss=0.236]\u001b[A\n",
      "Epoch 1:  33%|███▎      | 201/604 [01:19<02:38,  2.54it/s, training loss=0.266]\u001b[A\n",
      "Epoch 1:  33%|███▎      | 202/604 [01:19<02:37,  2.55it/s, training loss=0.266]\u001b[A\n",
      "Epoch 1:  33%|███▎      | 202/604 [01:19<02:37,  2.55it/s, training loss=0.243]\u001b[A\n",
      "Epoch 1:  34%|███▎      | 203/604 [01:19<02:36,  2.56it/s, training loss=0.243]\u001b[A\n",
      "Epoch 1:  34%|███▎      | 203/604 [01:20<02:36,  2.56it/s, training loss=0.186]\u001b[A\n",
      "Epoch 1:  34%|███▍      | 204/604 [01:20<02:35,  2.57it/s, training loss=0.186]\u001b[A\n",
      "Epoch 1:  34%|███▍      | 204/604 [01:20<02:35,  2.57it/s, training loss=0.333]\u001b[A\n",
      "Epoch 1:  34%|███▍      | 205/604 [01:20<02:34,  2.57it/s, training loss=0.333]\u001b[A\n",
      "Epoch 1:  34%|███▍      | 205/604 [01:21<02:34,  2.57it/s, training loss=0.183]\u001b[A\n",
      "Epoch 1:  34%|███▍      | 206/604 [01:21<02:34,  2.58it/s, training loss=0.183]\u001b[A\n",
      "Epoch 1:  34%|███▍      | 206/604 [01:21<02:34,  2.58it/s, training loss=0.260]\u001b[A\n",
      "Epoch 1:  34%|███▍      | 207/604 [01:21<02:33,  2.59it/s, training loss=0.260]\u001b[A\n",
      "Epoch 1:  34%|███▍      | 207/604 [01:21<02:33,  2.59it/s, training loss=0.276]\u001b[A\n",
      "Epoch 1:  34%|███▍      | 208/604 [01:21<02:32,  2.59it/s, training loss=0.276]\u001b[A\n",
      "Epoch 1:  34%|███▍      | 208/604 [01:22<02:32,  2.59it/s, training loss=0.226]\u001b[A\n",
      "Epoch 1:  35%|███▍      | 209/604 [01:22<02:32,  2.59it/s, training loss=0.226]\u001b[A\n",
      "Epoch 1:  35%|███▍      | 209/604 [01:22<02:32,  2.59it/s, training loss=0.259]\u001b[A\n",
      "Epoch 1:  35%|███▍      | 210/604 [01:22<02:32,  2.58it/s, training loss=0.259]\u001b[A\n",
      "Epoch 1:  35%|███▍      | 210/604 [01:22<02:32,  2.58it/s, training loss=0.209]\u001b[A\n",
      "Epoch 1:  35%|███▍      | 211/604 [01:22<02:31,  2.59it/s, training loss=0.209]\u001b[A\n",
      "Epoch 1:  35%|███▍      | 211/604 [01:23<02:31,  2.59it/s, training loss=0.215]\u001b[A\n",
      "Epoch 1:  35%|███▌      | 212/604 [01:23<02:31,  2.59it/s, training loss=0.215]\u001b[A\n",
      "Epoch 1:  35%|███▌      | 212/604 [01:23<02:31,  2.59it/s, training loss=0.261]\u001b[A\n",
      "Epoch 1:  35%|███▌      | 213/604 [01:23<02:31,  2.59it/s, training loss=0.261]\u001b[A\n",
      "Epoch 1:  35%|███▌      | 213/604 [01:24<02:31,  2.59it/s, training loss=0.236]\u001b[A\n",
      "Epoch 1:  35%|███▌      | 214/604 [01:24<02:30,  2.59it/s, training loss=0.236]\u001b[A\n",
      "Epoch 1:  35%|███▌      | 214/604 [01:24<02:30,  2.59it/s, training loss=0.220]\u001b[A\n",
      "Epoch 1:  36%|███▌      | 215/604 [01:24<02:30,  2.59it/s, training loss=0.220]\u001b[A\n",
      "Epoch 1:  36%|███▌      | 215/604 [01:24<02:30,  2.59it/s, training loss=0.211]\u001b[A\n",
      "Epoch 1:  36%|███▌      | 216/604 [01:24<02:29,  2.59it/s, training loss=0.211]\u001b[A\n",
      "Epoch 1:  36%|███▌      | 216/604 [01:25<02:29,  2.59it/s, training loss=0.167]\u001b[A\n",
      "Epoch 1:  36%|███▌      | 217/604 [01:25<02:29,  2.59it/s, training loss=0.167]\u001b[A\n",
      "Epoch 1:  36%|███▌      | 217/604 [01:25<02:29,  2.59it/s, training loss=0.268]\u001b[A\n",
      "Epoch 1:  36%|███▌      | 218/604 [01:25<02:28,  2.59it/s, training loss=0.268]\u001b[A\n",
      "Epoch 1:  36%|███▌      | 218/604 [01:26<02:28,  2.59it/s, training loss=0.252]\u001b[A\n",
      "Epoch 1:  36%|███▋      | 219/604 [01:26<02:28,  2.59it/s, training loss=0.252]\u001b[A\n",
      "Epoch 1:  36%|███▋      | 219/604 [01:26<02:28,  2.59it/s, training loss=0.238]\u001b[A\n",
      "Epoch 1:  36%|███▋      | 220/604 [01:26<02:28,  2.59it/s, training loss=0.238]\u001b[A\n",
      "Epoch 1:  36%|███▋      | 220/604 [01:26<02:28,  2.59it/s, training loss=0.218]\u001b[A\n",
      "Epoch 1:  37%|███▋      | 221/604 [01:26<02:27,  2.59it/s, training loss=0.218]\u001b[A\n",
      "Epoch 1:  37%|███▋      | 221/604 [01:27<02:27,  2.59it/s, training loss=0.251]\u001b[A\n",
      "Epoch 1:  37%|███▋      | 222/604 [01:27<02:27,  2.59it/s, training loss=0.251]\u001b[A\n",
      "Epoch 1:  37%|███▋      | 222/604 [01:27<02:27,  2.59it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  37%|███▋      | 223/604 [01:27<02:26,  2.59it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  37%|███▋      | 223/604 [01:27<02:26,  2.59it/s, training loss=0.197]\u001b[A\n",
      "Epoch 1:  37%|███▋      | 224/604 [01:27<02:26,  2.59it/s, training loss=0.197]\u001b[A\n",
      "Epoch 1:  37%|███▋      | 224/604 [01:28<02:26,  2.59it/s, training loss=0.213]\u001b[A\n",
      "Epoch 1:  37%|███▋      | 225/604 [01:28<02:26,  2.59it/s, training loss=0.213]\u001b[A\n",
      "Epoch 1:  37%|███▋      | 225/604 [01:28<02:26,  2.59it/s, training loss=0.217]\u001b[A\n",
      "Epoch 1:  37%|███▋      | 226/604 [01:28<02:25,  2.59it/s, training loss=0.217]\u001b[A\n",
      "Epoch 1:  37%|███▋      | 226/604 [01:29<02:25,  2.59it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  38%|███▊      | 227/604 [01:29<02:25,  2.59it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  38%|███▊      | 227/604 [01:29<02:25,  2.59it/s, training loss=0.261]\u001b[A\n",
      "Epoch 1:  38%|███▊      | 228/604 [01:29<02:25,  2.59it/s, training loss=0.261]\u001b[A\n",
      "Epoch 1:  38%|███▊      | 228/604 [01:29<02:25,  2.59it/s, training loss=0.267]\u001b[A\n",
      "Epoch 1:  38%|███▊      | 229/604 [01:29<02:27,  2.55it/s, training loss=0.267]\u001b[A\n",
      "Epoch 1:  38%|███▊      | 229/604 [01:30<02:27,  2.55it/s, training loss=0.250]\u001b[A\n",
      "Epoch 1:  38%|███▊      | 230/604 [01:30<02:26,  2.56it/s, training loss=0.250]\u001b[A\n",
      "Epoch 1:  38%|███▊      | 230/604 [01:30<02:26,  2.56it/s, training loss=0.309]\u001b[A\n",
      "Epoch 1:  38%|███▊      | 231/604 [01:30<02:25,  2.57it/s, training loss=0.309]\u001b[A\n",
      "Epoch 1:  38%|███▊      | 231/604 [01:31<02:25,  2.57it/s, training loss=0.253]\u001b[A\n",
      "Epoch 1:  38%|███▊      | 232/604 [01:31<02:24,  2.58it/s, training loss=0.253]\u001b[A\n",
      "Epoch 1:  38%|███▊      | 232/604 [01:31<02:24,  2.58it/s, training loss=0.273]\u001b[A\n",
      "Epoch 1:  39%|███▊      | 233/604 [01:31<02:24,  2.58it/s, training loss=0.273]\u001b[A\n",
      "Epoch 1:  39%|███▊      | 233/604 [01:31<02:24,  2.58it/s, training loss=0.334]\u001b[A\n",
      "Epoch 1:  39%|███▊      | 234/604 [01:31<02:23,  2.58it/s, training loss=0.334]\u001b[A\n",
      "Epoch 1:  39%|███▊      | 234/604 [01:32<02:23,  2.58it/s, training loss=0.221]\u001b[A\n",
      "Epoch 1:  39%|███▉      | 235/604 [01:32<02:22,  2.58it/s, training loss=0.221]\u001b[A\n",
      "Epoch 1:  39%|███▉      | 235/604 [01:32<02:22,  2.58it/s, training loss=0.264]\u001b[A\n",
      "Epoch 1:  39%|███▉      | 236/604 [01:32<02:22,  2.59it/s, training loss=0.264]\u001b[A\n",
      "Epoch 1:  39%|███▉      | 236/604 [01:33<02:22,  2.59it/s, training loss=0.243]\u001b[A\n",
      "Epoch 1:  39%|███▉      | 237/604 [01:33<02:21,  2.59it/s, training loss=0.243]\u001b[A\n",
      "Epoch 1:  39%|███▉      | 237/604 [01:33<02:21,  2.59it/s, training loss=0.231]\u001b[A\n",
      "Epoch 1:  39%|███▉      | 238/604 [01:33<02:21,  2.58it/s, training loss=0.231]\u001b[A\n",
      "Epoch 1:  39%|███▉      | 238/604 [01:33<02:21,  2.58it/s, training loss=0.346]\u001b[A\n",
      "Epoch 1:  40%|███▉      | 239/604 [01:33<02:21,  2.58it/s, training loss=0.346]\u001b[A\n",
      "Epoch 1:  40%|███▉      | 239/604 [01:34<02:21,  2.58it/s, training loss=0.265]\u001b[A\n",
      "Epoch 1:  40%|███▉      | 240/604 [01:34<02:21,  2.57it/s, training loss=0.265]\u001b[A\n",
      "Epoch 1:  40%|███▉      | 240/604 [01:34<02:21,  2.57it/s, training loss=0.167]\u001b[A\n",
      "Epoch 1:  40%|███▉      | 241/604 [01:34<02:20,  2.58it/s, training loss=0.167]\u001b[A\n",
      "Epoch 1:  40%|███▉      | 241/604 [01:34<02:20,  2.58it/s, training loss=0.240]\u001b[A\n",
      "Epoch 1:  40%|████      | 242/604 [01:34<02:20,  2.58it/s, training loss=0.240]\u001b[A\n",
      "Epoch 1:  40%|████      | 242/604 [01:35<02:20,  2.58it/s, training loss=0.212]\u001b[A\n",
      "Epoch 1:  40%|████      | 243/604 [01:35<02:19,  2.58it/s, training loss=0.212]\u001b[A\n",
      "Epoch 1:  40%|████      | 243/604 [01:35<02:19,  2.58it/s, training loss=0.239]\u001b[A\n",
      "Epoch 1:  40%|████      | 244/604 [01:35<02:19,  2.59it/s, training loss=0.239]\u001b[A\n",
      "Epoch 1:  40%|████      | 244/604 [01:36<02:19,  2.59it/s, training loss=0.250]\u001b[A\n",
      "Epoch 1:  41%|████      | 245/604 [01:36<02:18,  2.59it/s, training loss=0.250]\u001b[A\n",
      "Epoch 1:  41%|████      | 245/604 [01:36<02:18,  2.59it/s, training loss=0.213]\u001b[A\n",
      "Epoch 1:  41%|████      | 246/604 [01:36<02:18,  2.59it/s, training loss=0.213]\u001b[A\n",
      "Epoch 1:  41%|████      | 246/604 [01:36<02:18,  2.59it/s, training loss=0.247]\u001b[A\n",
      "Epoch 1:  41%|████      | 247/604 [01:36<02:17,  2.59it/s, training loss=0.247]\u001b[A\n",
      "Epoch 1:  41%|████      | 247/604 [01:37<02:17,  2.59it/s, training loss=0.227]\u001b[A\n",
      "Epoch 1:  41%|████      | 248/604 [01:37<02:17,  2.59it/s, training loss=0.227]\u001b[A\n",
      "Epoch 1:  41%|████      | 248/604 [01:37<02:17,  2.59it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  41%|████      | 249/604 [01:37<02:17,  2.59it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  41%|████      | 249/604 [01:38<02:17,  2.59it/s, training loss=0.215]\u001b[A\n",
      "Epoch 1:  41%|████▏     | 250/604 [01:38<02:16,  2.59it/s, training loss=0.215]\u001b[A\n",
      "Epoch 1:  41%|████▏     | 250/604 [01:38<02:16,  2.59it/s, training loss=0.250]\u001b[A\n",
      "Epoch 1:  42%|████▏     | 251/604 [01:38<02:16,  2.59it/s, training loss=0.250]\u001b[A\n",
      "Epoch 1:  42%|████▏     | 251/604 [01:38<02:16,  2.59it/s, training loss=0.251]\u001b[A\n",
      "Epoch 1:  42%|████▏     | 252/604 [01:38<02:15,  2.59it/s, training loss=0.251]\u001b[A\n",
      "Epoch 1:  42%|████▏     | 252/604 [01:39<02:15,  2.59it/s, training loss=0.222]\u001b[A\n",
      "Epoch 1:  42%|████▏     | 253/604 [01:39<02:15,  2.59it/s, training loss=0.222]\u001b[A\n",
      "Epoch 1:  42%|████▏     | 253/604 [01:39<02:15,  2.59it/s, training loss=0.158]\u001b[A\n",
      "Epoch 1:  42%|████▏     | 254/604 [01:39<02:15,  2.59it/s, training loss=0.158]\u001b[A\n",
      "Epoch 1:  42%|████▏     | 254/604 [01:39<02:15,  2.59it/s, training loss=0.184]\u001b[A\n",
      "Epoch 1:  42%|████▏     | 255/604 [01:39<02:17,  2.54it/s, training loss=0.184]\u001b[A\n",
      "Epoch 1:  42%|████▏     | 255/604 [01:40<02:17,  2.54it/s, training loss=0.224]\u001b[A\n",
      "Epoch 1:  42%|████▏     | 256/604 [01:40<02:20,  2.48it/s, training loss=0.224]\u001b[A\n",
      "Epoch 1:  42%|████▏     | 256/604 [01:40<02:20,  2.48it/s, training loss=0.223]\u001b[A\n",
      "Epoch 1:  43%|████▎     | 257/604 [01:40<02:21,  2.46it/s, training loss=0.223]\u001b[A\n",
      "Epoch 1:  43%|████▎     | 257/604 [01:41<02:21,  2.46it/s, training loss=0.244]\u001b[A\n",
      "Epoch 1:  43%|████▎     | 258/604 [01:41<02:23,  2.41it/s, training loss=0.244]\u001b[A\n",
      "Epoch 1:  43%|████▎     | 258/604 [01:41<02:23,  2.41it/s, training loss=0.258]\u001b[A\n",
      "Epoch 1:  43%|████▎     | 259/604 [01:41<02:21,  2.45it/s, training loss=0.258]\u001b[A\n",
      "Epoch 1:  43%|████▎     | 259/604 [01:42<02:21,  2.45it/s, training loss=0.282]\u001b[A\n",
      "Epoch 1:  43%|████▎     | 260/604 [01:42<02:18,  2.48it/s, training loss=0.282]\u001b[A\n",
      "Epoch 1:  43%|████▎     | 260/604 [01:42<02:18,  2.48it/s, training loss=0.186]\u001b[A\n",
      "Epoch 1:  43%|████▎     | 261/604 [01:42<02:16,  2.52it/s, training loss=0.186]\u001b[A\n",
      "Epoch 1:  43%|████▎     | 261/604 [01:42<02:16,  2.52it/s, training loss=0.218]\u001b[A\n",
      "Epoch 1:  43%|████▎     | 262/604 [01:42<02:14,  2.54it/s, training loss=0.218]\u001b[A\n",
      "Epoch 1:  43%|████▎     | 262/604 [01:43<02:14,  2.54it/s, training loss=0.243]\u001b[A\n",
      "Epoch 1:  44%|████▎     | 263/604 [01:43<02:13,  2.56it/s, training loss=0.243]\u001b[A\n",
      "Epoch 1:  44%|████▎     | 263/604 [01:43<02:13,  2.56it/s, training loss=0.253]\u001b[A\n",
      "Epoch 1:  44%|████▎     | 264/604 [01:43<02:12,  2.57it/s, training loss=0.253]\u001b[A\n",
      "Epoch 1:  44%|████▎     | 264/604 [01:43<02:12,  2.57it/s, training loss=0.193]\u001b[A\n",
      "Epoch 1:  44%|████▍     | 265/604 [01:43<02:12,  2.56it/s, training loss=0.193]\u001b[A\n",
      "Epoch 1:  44%|████▍     | 265/604 [01:44<02:12,  2.56it/s, training loss=0.270]\u001b[A\n",
      "Epoch 1:  44%|████▍     | 266/604 [01:44<02:11,  2.57it/s, training loss=0.270]\u001b[A\n",
      "Epoch 1:  44%|████▍     | 266/604 [01:44<02:11,  2.57it/s, training loss=0.285]\u001b[A\n",
      "Epoch 1:  44%|████▍     | 267/604 [01:44<02:10,  2.58it/s, training loss=0.285]\u001b[A\n",
      "Epoch 1:  44%|████▍     | 267/604 [01:45<02:10,  2.58it/s, training loss=0.236]\u001b[A\n",
      "Epoch 1:  44%|████▍     | 268/604 [01:45<02:11,  2.56it/s, training loss=0.236]\u001b[A\n",
      "Epoch 1:  44%|████▍     | 268/604 [01:45<02:11,  2.56it/s, training loss=0.306]\u001b[A\n",
      "Epoch 1:  45%|████▍     | 269/604 [01:45<02:12,  2.53it/s, training loss=0.306]\u001b[A\n",
      "Epoch 1:  45%|████▍     | 269/604 [01:45<02:12,  2.53it/s, training loss=0.182]\u001b[A\n",
      "Epoch 1:  45%|████▍     | 270/604 [01:45<02:13,  2.50it/s, training loss=0.182]\u001b[A\n",
      "Epoch 1:  45%|████▍     | 270/604 [01:46<02:13,  2.50it/s, training loss=0.231]\u001b[A\n",
      "Epoch 1:  45%|████▍     | 271/604 [01:46<02:11,  2.52it/s, training loss=0.231]\u001b[A\n",
      "Epoch 1:  45%|████▍     | 271/604 [01:46<02:11,  2.52it/s, training loss=0.221]\u001b[A\n",
      "Epoch 1:  45%|████▌     | 272/604 [01:46<02:10,  2.54it/s, training loss=0.221]\u001b[A\n",
      "Epoch 1:  45%|████▌     | 272/604 [01:47<02:10,  2.54it/s, training loss=0.280]\u001b[A\n",
      "Epoch 1:  45%|████▌     | 273/604 [01:47<02:09,  2.55it/s, training loss=0.280]\u001b[A\n",
      "Epoch 1:  45%|████▌     | 273/604 [01:47<02:09,  2.55it/s, training loss=0.196]\u001b[A\n",
      "Epoch 1:  45%|████▌     | 274/604 [01:47<02:08,  2.56it/s, training loss=0.196]\u001b[A\n",
      "Epoch 1:  45%|████▌     | 274/604 [01:47<02:08,  2.56it/s, training loss=0.221]\u001b[A\n",
      "Epoch 1:  46%|████▌     | 275/604 [01:47<02:08,  2.57it/s, training loss=0.221]\u001b[A\n",
      "Epoch 1:  46%|████▌     | 275/604 [01:48<02:08,  2.57it/s, training loss=0.196]\u001b[A\n",
      "Epoch 1:  46%|████▌     | 276/604 [01:48<02:07,  2.58it/s, training loss=0.196]\u001b[A\n",
      "Epoch 1:  46%|████▌     | 276/604 [01:48<02:07,  2.58it/s, training loss=0.206]\u001b[A\n",
      "Epoch 1:  46%|████▌     | 277/604 [01:48<02:06,  2.58it/s, training loss=0.206]\u001b[A\n",
      "Epoch 1:  46%|████▌     | 277/604 [01:49<02:06,  2.58it/s, training loss=0.206]\u001b[A\n",
      "Epoch 1:  46%|████▌     | 278/604 [01:49<02:06,  2.58it/s, training loss=0.206]\u001b[A\n",
      "Epoch 1:  46%|████▌     | 278/604 [01:49<02:06,  2.58it/s, training loss=0.277]\u001b[A\n",
      "Epoch 1:  46%|████▌     | 279/604 [01:49<02:06,  2.58it/s, training loss=0.277]\u001b[A\n",
      "Epoch 1:  46%|████▌     | 279/604 [01:49<02:06,  2.58it/s, training loss=0.207]\u001b[A\n",
      "Epoch 1:  46%|████▋     | 280/604 [01:49<02:05,  2.58it/s, training loss=0.207]\u001b[A\n",
      "Epoch 1:  46%|████▋     | 280/604 [01:50<02:05,  2.58it/s, training loss=0.252]\u001b[A\n",
      "Epoch 1:  47%|████▋     | 281/604 [01:50<02:05,  2.58it/s, training loss=0.252]\u001b[A\n",
      "Epoch 1:  47%|████▋     | 281/604 [01:50<02:05,  2.58it/s, training loss=0.190]\u001b[A\n",
      "Epoch 1:  47%|████▋     | 282/604 [01:50<02:04,  2.59it/s, training loss=0.190]\u001b[A\n",
      "Epoch 1:  47%|████▋     | 282/604 [01:50<02:04,  2.59it/s, training loss=0.226]\u001b[A\n",
      "Epoch 1:  47%|████▋     | 283/604 [01:50<02:04,  2.58it/s, training loss=0.226]\u001b[A\n",
      "Epoch 1:  47%|████▋     | 283/604 [01:51<02:04,  2.58it/s, training loss=0.253]\u001b[A\n",
      "Epoch 1:  47%|████▋     | 284/604 [01:51<02:03,  2.58it/s, training loss=0.253]\u001b[A\n",
      "Epoch 1:  47%|████▋     | 284/604 [01:51<02:03,  2.58it/s, training loss=0.195]\u001b[A\n",
      "Epoch 1:  47%|████▋     | 285/604 [01:51<02:05,  2.55it/s, training loss=0.195]\u001b[A\n",
      "Epoch 1:  47%|████▋     | 285/604 [01:52<02:05,  2.55it/s, training loss=0.259]\u001b[A\n",
      "Epoch 1:  47%|████▋     | 286/604 [01:52<02:07,  2.49it/s, training loss=0.259]\u001b[A\n",
      "Epoch 1:  47%|████▋     | 286/604 [01:52<02:07,  2.49it/s, training loss=0.230]\u001b[A\n",
      "Epoch 1:  48%|████▊     | 287/604 [01:52<02:06,  2.51it/s, training loss=0.230]\u001b[A\n",
      "Epoch 1:  48%|████▊     | 287/604 [01:52<02:06,  2.51it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  48%|████▊     | 288/604 [01:52<02:04,  2.53it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  48%|████▊     | 288/604 [01:53<02:04,  2.53it/s, training loss=0.237]\u001b[A\n",
      "Epoch 1:  48%|████▊     | 289/604 [01:53<02:03,  2.55it/s, training loss=0.237]\u001b[A\n",
      "Epoch 1:  48%|████▊     | 289/604 [01:53<02:03,  2.55it/s, training loss=0.206]\u001b[A\n",
      "Epoch 1:  48%|████▊     | 290/604 [01:53<02:02,  2.56it/s, training loss=0.206]\u001b[A\n",
      "Epoch 1:  48%|████▊     | 290/604 [01:54<02:02,  2.56it/s, training loss=0.308]\u001b[A\n",
      "Epoch 1:  48%|████▊     | 291/604 [01:54<02:01,  2.57it/s, training loss=0.308]\u001b[A\n",
      "Epoch 1:  48%|████▊     | 291/604 [01:54<02:01,  2.57it/s, training loss=0.245]\u001b[A\n",
      "Epoch 1:  48%|████▊     | 292/604 [01:54<02:01,  2.58it/s, training loss=0.245]\u001b[A\n",
      "Epoch 1:  48%|████▊     | 292/604 [01:54<02:01,  2.58it/s, training loss=0.172]\u001b[A\n",
      "Epoch 1:  49%|████▊     | 293/604 [01:54<02:00,  2.58it/s, training loss=0.172]\u001b[A\n",
      "Epoch 1:  49%|████▊     | 293/604 [01:55<02:00,  2.58it/s, training loss=0.204]\u001b[A\n",
      "Epoch 1:  49%|████▊     | 294/604 [01:55<01:59,  2.59it/s, training loss=0.204]\u001b[A\n",
      "Epoch 1:  49%|████▊     | 294/604 [01:55<01:59,  2.59it/s, training loss=0.306]\u001b[A\n",
      "Epoch 1:  49%|████▉     | 295/604 [01:55<01:59,  2.59it/s, training loss=0.306]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  49%|████▉     | 295/604 [01:56<01:59,  2.59it/s, training loss=0.303]\u001b[A\n",
      "Epoch 1:  49%|████▉     | 296/604 [01:56<01:58,  2.59it/s, training loss=0.303]\u001b[A\n",
      "Epoch 1:  49%|████▉     | 296/604 [01:56<01:58,  2.59it/s, training loss=0.200]\u001b[A\n",
      "Epoch 1:  49%|████▉     | 297/604 [01:56<01:58,  2.59it/s, training loss=0.200]\u001b[A\n",
      "Epoch 1:  49%|████▉     | 297/604 [01:56<01:58,  2.59it/s, training loss=0.207]\u001b[A\n",
      "Epoch 1:  49%|████▉     | 298/604 [01:56<01:58,  2.59it/s, training loss=0.207]\u001b[A\n",
      "Epoch 1:  49%|████▉     | 298/604 [01:57<01:58,  2.59it/s, training loss=0.214]\u001b[A\n",
      "Epoch 1:  50%|████▉     | 299/604 [01:57<01:57,  2.59it/s, training loss=0.214]\u001b[A\n",
      "Epoch 1:  50%|████▉     | 299/604 [01:57<01:57,  2.59it/s, training loss=0.250]\u001b[A\n",
      "Epoch 1:  50%|████▉     | 300/604 [01:57<01:57,  2.59it/s, training loss=0.250]\u001b[A\n",
      "Epoch 1:  50%|████▉     | 300/604 [01:57<01:57,  2.59it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  50%|████▉     | 301/604 [01:58<01:56,  2.59it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  50%|████▉     | 301/604 [01:58<01:56,  2.59it/s, training loss=0.164]\u001b[A\n",
      "Epoch 1:  50%|█████     | 302/604 [01:58<01:56,  2.58it/s, training loss=0.164]\u001b[A\n",
      "Epoch 1:  50%|█████     | 302/604 [01:58<01:56,  2.58it/s, training loss=0.297]\u001b[A\n",
      "Epoch 1:  50%|█████     | 303/604 [01:58<01:56,  2.59it/s, training loss=0.297]\u001b[A\n",
      "Epoch 1:  50%|█████     | 303/604 [01:59<01:56,  2.59it/s, training loss=0.238]\u001b[A\n",
      "Epoch 1:  50%|█████     | 304/604 [01:59<01:55,  2.59it/s, training loss=0.238]\u001b[A\n",
      "Epoch 1:  50%|█████     | 304/604 [01:59<01:55,  2.59it/s, training loss=0.182]\u001b[A\n",
      "Epoch 1:  50%|█████     | 305/604 [01:59<01:55,  2.58it/s, training loss=0.182]\u001b[A\n",
      "Epoch 1:  50%|█████     | 305/604 [01:59<01:55,  2.58it/s, training loss=0.233]\u001b[A\n",
      "Epoch 1:  51%|█████     | 306/604 [01:59<01:55,  2.58it/s, training loss=0.233]\u001b[A\n",
      "Epoch 1:  51%|█████     | 306/604 [02:00<01:55,  2.58it/s, training loss=0.215]\u001b[A\n",
      "Epoch 1:  51%|█████     | 307/604 [02:00<01:54,  2.58it/s, training loss=0.215]\u001b[A\n",
      "Epoch 1:  51%|█████     | 307/604 [02:00<01:54,  2.58it/s, training loss=0.253]\u001b[A\n",
      "Epoch 1:  51%|█████     | 308/604 [02:00<01:54,  2.58it/s, training loss=0.253]\u001b[A\n",
      "Epoch 1:  51%|█████     | 308/604 [02:01<01:54,  2.58it/s, training loss=0.174]\u001b[A\n",
      "Epoch 1:  51%|█████     | 309/604 [02:01<01:54,  2.59it/s, training loss=0.174]\u001b[A\n",
      "Epoch 1:  51%|█████     | 309/604 [02:01<01:54,  2.59it/s, training loss=0.262]\u001b[A\n",
      "Epoch 1:  51%|█████▏    | 310/604 [02:01<01:54,  2.58it/s, training loss=0.262]\u001b[A\n",
      "Epoch 1:  51%|█████▏    | 310/604 [02:01<01:54,  2.58it/s, training loss=0.221]\u001b[A\n",
      "Epoch 1:  51%|█████▏    | 311/604 [02:01<01:53,  2.58it/s, training loss=0.221]\u001b[A\n",
      "Epoch 1:  51%|█████▏    | 311/604 [02:02<01:53,  2.58it/s, training loss=0.137]\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 312/604 [02:02<01:52,  2.58it/s, training loss=0.137]\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 312/604 [02:02<01:52,  2.58it/s, training loss=0.245]\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 313/604 [02:02<01:52,  2.59it/s, training loss=0.245]\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 313/604 [02:03<01:52,  2.59it/s, training loss=0.172]\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 314/604 [02:03<01:53,  2.57it/s, training loss=0.172]\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 314/604 [02:03<01:53,  2.57it/s, training loss=0.242]\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 315/604 [02:03<01:53,  2.55it/s, training loss=0.242]\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 315/604 [02:03<01:53,  2.55it/s, training loss=0.266]\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 316/604 [02:03<01:52,  2.55it/s, training loss=0.266]\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 316/604 [02:04<01:52,  2.55it/s, training loss=0.221]\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 317/604 [02:04<01:51,  2.56it/s, training loss=0.221]\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 317/604 [02:04<01:51,  2.56it/s, training loss=0.271]\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 318/604 [02:04<01:51,  2.57it/s, training loss=0.271]\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 318/604 [02:04<01:51,  2.57it/s, training loss=0.285]\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 319/604 [02:04<01:50,  2.58it/s, training loss=0.285]\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 319/604 [02:05<01:50,  2.58it/s, training loss=0.219]\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 320/604 [02:05<01:49,  2.58it/s, training loss=0.219]\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 320/604 [02:05<01:49,  2.58it/s, training loss=0.238]\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 321/604 [02:05<01:49,  2.59it/s, training loss=0.238]\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 321/604 [02:06<01:49,  2.59it/s, training loss=0.140]\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 322/604 [02:06<01:48,  2.59it/s, training loss=0.140]\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 322/604 [02:06<01:48,  2.59it/s, training loss=0.321]\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 323/604 [02:06<01:48,  2.59it/s, training loss=0.321]\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 323/604 [02:06<01:48,  2.59it/s, training loss=0.213]\u001b[A\n",
      "Epoch 1:  54%|█████▎    | 324/604 [02:06<01:48,  2.59it/s, training loss=0.213]\u001b[A\n",
      "Epoch 1:  54%|█████▎    | 324/604 [02:07<01:48,  2.59it/s, training loss=0.302]\u001b[A\n",
      "Epoch 1:  54%|█████▍    | 325/604 [02:07<01:47,  2.59it/s, training loss=0.302]\u001b[A\n",
      "Epoch 1:  54%|█████▍    | 325/604 [02:07<01:47,  2.59it/s, training loss=0.261]\u001b[A\n",
      "Epoch 1:  54%|█████▍    | 326/604 [02:07<01:47,  2.59it/s, training loss=0.261]\u001b[A\n",
      "Epoch 1:  54%|█████▍    | 326/604 [02:08<01:47,  2.59it/s, training loss=0.203]\u001b[A\n",
      "Epoch 1:  54%|█████▍    | 327/604 [02:08<01:46,  2.59it/s, training loss=0.203]\u001b[A\n",
      "Epoch 1:  54%|█████▍    | 327/604 [02:08<01:46,  2.59it/s, training loss=0.196]\u001b[A\n",
      "Epoch 1:  54%|█████▍    | 328/604 [02:08<01:46,  2.59it/s, training loss=0.196]\u001b[A\n",
      "Epoch 1:  54%|█████▍    | 328/604 [02:08<01:46,  2.59it/s, training loss=0.250]\u001b[A\n",
      "Epoch 1:  54%|█████▍    | 329/604 [02:08<01:46,  2.59it/s, training loss=0.250]\u001b[A\n",
      "Epoch 1:  54%|█████▍    | 329/604 [02:09<01:46,  2.59it/s, training loss=0.188]\u001b[A\n",
      "Epoch 1:  55%|█████▍    | 330/604 [02:09<01:45,  2.59it/s, training loss=0.188]\u001b[A\n",
      "Epoch 1:  55%|█████▍    | 330/604 [02:09<01:45,  2.59it/s, training loss=0.288]\u001b[A\n",
      "Epoch 1:  55%|█████▍    | 331/604 [02:09<01:45,  2.59it/s, training loss=0.288]\u001b[A\n",
      "Epoch 1:  55%|█████▍    | 331/604 [02:10<01:45,  2.59it/s, training loss=0.242]\u001b[A\n",
      "Epoch 1:  55%|█████▍    | 332/604 [02:10<01:44,  2.59it/s, training loss=0.242]\u001b[A\n",
      "Epoch 1:  55%|█████▍    | 332/604 [02:10<01:44,  2.59it/s, training loss=0.254]\u001b[A\n",
      "Epoch 1:  55%|█████▌    | 333/604 [02:10<01:44,  2.59it/s, training loss=0.254]\u001b[A\n",
      "Epoch 1:  55%|█████▌    | 333/604 [02:10<01:44,  2.59it/s, training loss=0.210]\u001b[A\n",
      "Epoch 1:  55%|█████▌    | 334/604 [02:10<01:44,  2.60it/s, training loss=0.210]\u001b[A\n",
      "Epoch 1:  55%|█████▌    | 334/604 [02:11<01:44,  2.60it/s, training loss=0.253]\u001b[A\n",
      "Epoch 1:  55%|█████▌    | 335/604 [02:11<01:43,  2.59it/s, training loss=0.253]\u001b[A\n",
      "Epoch 1:  55%|█████▌    | 335/604 [02:11<01:43,  2.59it/s, training loss=0.178]\u001b[A\n",
      "Epoch 1:  56%|█████▌    | 336/604 [02:11<01:43,  2.59it/s, training loss=0.178]\u001b[A\n",
      "Epoch 1:  56%|█████▌    | 336/604 [02:11<01:43,  2.59it/s, training loss=0.271]\u001b[A\n",
      "Epoch 1:  56%|█████▌    | 337/604 [02:11<01:43,  2.59it/s, training loss=0.271]\u001b[A\n",
      "Epoch 1:  56%|█████▌    | 337/604 [02:12<01:43,  2.59it/s, training loss=0.210]\u001b[A\n",
      "Epoch 1:  56%|█████▌    | 338/604 [02:12<01:42,  2.59it/s, training loss=0.210]\u001b[A\n",
      "Epoch 1:  56%|█████▌    | 338/604 [02:12<01:42,  2.59it/s, training loss=0.221]\u001b[A\n",
      "Epoch 1:  56%|█████▌    | 339/604 [02:12<01:42,  2.59it/s, training loss=0.221]\u001b[A\n",
      "Epoch 1:  56%|█████▌    | 339/604 [02:13<01:42,  2.59it/s, training loss=0.234]\u001b[A\n",
      "Epoch 1:  56%|█████▋    | 340/604 [02:13<01:41,  2.59it/s, training loss=0.234]\u001b[A\n",
      "Epoch 1:  56%|█████▋    | 340/604 [02:13<01:41,  2.59it/s, training loss=0.222]\u001b[A\n",
      "Epoch 1:  56%|█████▋    | 341/604 [02:13<01:41,  2.59it/s, training loss=0.222]\u001b[A\n",
      "Epoch 1:  56%|█████▋    | 341/604 [02:13<01:41,  2.59it/s, training loss=0.200]\u001b[A\n",
      "Epoch 1:  57%|█████▋    | 342/604 [02:13<01:43,  2.54it/s, training loss=0.200]\u001b[A\n",
      "Epoch 1:  57%|█████▋    | 342/604 [02:14<01:43,  2.54it/s, training loss=0.200]\u001b[A\n",
      "Epoch 1:  57%|█████▋    | 343/604 [02:14<01:43,  2.52it/s, training loss=0.200]\u001b[A\n",
      "Epoch 1:  57%|█████▋    | 343/604 [02:14<01:43,  2.52it/s, training loss=0.243]\u001b[A\n",
      "Epoch 1:  57%|█████▋    | 344/604 [02:14<01:42,  2.54it/s, training loss=0.243]\u001b[A\n",
      "Epoch 1:  57%|█████▋    | 344/604 [02:15<01:42,  2.54it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  57%|█████▋    | 345/604 [02:15<01:41,  2.56it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  57%|█████▋    | 345/604 [02:15<01:41,  2.56it/s, training loss=0.262]\u001b[A\n",
      "Epoch 1:  57%|█████▋    | 346/604 [02:15<01:40,  2.57it/s, training loss=0.262]\u001b[A\n",
      "Epoch 1:  57%|█████▋    | 346/604 [02:15<01:40,  2.57it/s, training loss=0.284]\u001b[A\n",
      "Epoch 1:  57%|█████▋    | 347/604 [02:15<01:41,  2.53it/s, training loss=0.284]\u001b[A\n",
      "Epoch 1:  57%|█████▋    | 347/604 [02:16<01:41,  2.53it/s, training loss=0.250]\u001b[A\n",
      "Epoch 1:  58%|█████▊    | 348/604 [02:16<01:40,  2.55it/s, training loss=0.250]\u001b[A\n",
      "Epoch 1:  58%|█████▊    | 348/604 [02:16<01:40,  2.55it/s, training loss=0.231]\u001b[A\n",
      "Epoch 1:  58%|█████▊    | 349/604 [02:16<01:39,  2.56it/s, training loss=0.231]\u001b[A\n",
      "Epoch 1:  58%|█████▊    | 349/604 [02:17<01:39,  2.56it/s, training loss=0.273]\u001b[A\n",
      "Epoch 1:  58%|█████▊    | 350/604 [02:17<01:39,  2.54it/s, training loss=0.273]\u001b[A\n",
      "Epoch 1:  58%|█████▊    | 350/604 [02:17<01:39,  2.54it/s, training loss=0.226]\u001b[A\n",
      "Epoch 1:  58%|█████▊    | 351/604 [02:17<01:38,  2.56it/s, training loss=0.226]\u001b[A\n",
      "Epoch 1:  58%|█████▊    | 351/604 [02:17<01:38,  2.56it/s, training loss=0.266]\u001b[A\n",
      "Epoch 1:  58%|█████▊    | 352/604 [02:17<01:38,  2.57it/s, training loss=0.266]\u001b[A\n",
      "Epoch 1:  58%|█████▊    | 352/604 [02:18<01:38,  2.57it/s, training loss=0.234]\u001b[A\n",
      "Epoch 1:  58%|█████▊    | 353/604 [02:18<01:37,  2.57it/s, training loss=0.234]\u001b[A\n",
      "Epoch 1:  58%|█████▊    | 353/604 [02:18<01:37,  2.57it/s, training loss=0.263]\u001b[A\n",
      "Epoch 1:  59%|█████▊    | 354/604 [02:18<01:36,  2.58it/s, training loss=0.263]\u001b[A\n",
      "Epoch 1:  59%|█████▊    | 354/604 [02:18<01:36,  2.58it/s, training loss=0.268]\u001b[A\n",
      "Epoch 1:  59%|█████▉    | 355/604 [02:18<01:36,  2.58it/s, training loss=0.268]\u001b[A\n",
      "Epoch 1:  59%|█████▉    | 355/604 [02:19<01:36,  2.58it/s, training loss=0.279]\u001b[A\n",
      "Epoch 1:  59%|█████▉    | 356/604 [02:19<01:36,  2.58it/s, training loss=0.279]\u001b[A\n",
      "Epoch 1:  59%|█████▉    | 356/604 [02:19<01:36,  2.58it/s, training loss=0.225]\u001b[A\n",
      "Epoch 1:  59%|█████▉    | 357/604 [02:19<01:35,  2.58it/s, training loss=0.225]\u001b[A\n",
      "Epoch 1:  59%|█████▉    | 357/604 [02:20<01:35,  2.58it/s, training loss=0.190]\u001b[A\n",
      "Epoch 1:  59%|█████▉    | 358/604 [02:20<01:35,  2.59it/s, training loss=0.190]\u001b[A\n",
      "Epoch 1:  59%|█████▉    | 358/604 [02:20<01:35,  2.59it/s, training loss=0.224]\u001b[A\n",
      "Epoch 1:  59%|█████▉    | 359/604 [02:20<01:34,  2.59it/s, training loss=0.224]\u001b[A\n",
      "Epoch 1:  59%|█████▉    | 359/604 [02:20<01:34,  2.59it/s, training loss=0.191]\u001b[A\n",
      "Epoch 1:  60%|█████▉    | 360/604 [02:20<01:34,  2.58it/s, training loss=0.191]\u001b[A\n",
      "Epoch 1:  60%|█████▉    | 360/604 [02:21<01:34,  2.58it/s, training loss=0.253]\u001b[A\n",
      "Epoch 1:  60%|█████▉    | 361/604 [02:21<01:34,  2.57it/s, training loss=0.253]\u001b[A\n",
      "Epoch 1:  60%|█████▉    | 361/604 [02:21<01:34,  2.57it/s, training loss=0.219]\u001b[A\n",
      "Epoch 1:  60%|█████▉    | 362/604 [02:21<01:33,  2.58it/s, training loss=0.219]\u001b[A\n",
      "Epoch 1:  60%|█████▉    | 362/604 [02:22<01:33,  2.58it/s, training loss=0.186]\u001b[A\n",
      "Epoch 1:  60%|██████    | 363/604 [02:22<01:33,  2.58it/s, training loss=0.186]\u001b[A\n",
      "Epoch 1:  60%|██████    | 363/604 [02:22<01:33,  2.58it/s, training loss=0.269]\u001b[A\n",
      "Epoch 1:  60%|██████    | 364/604 [02:22<01:32,  2.58it/s, training loss=0.269]\u001b[A\n",
      "Epoch 1:  60%|██████    | 364/604 [02:22<01:32,  2.58it/s, training loss=0.256]\u001b[A\n",
      "Epoch 1:  60%|██████    | 365/604 [02:22<01:32,  2.59it/s, training loss=0.256]\u001b[A\n",
      "Epoch 1:  60%|██████    | 365/604 [02:23<01:32,  2.59it/s, training loss=0.244]\u001b[A\n",
      "Epoch 1:  61%|██████    | 366/604 [02:23<01:32,  2.58it/s, training loss=0.244]\u001b[A\n",
      "Epoch 1:  61%|██████    | 366/604 [02:23<01:32,  2.58it/s, training loss=0.256]\u001b[A\n",
      "Epoch 1:  61%|██████    | 367/604 [02:23<01:31,  2.58it/s, training loss=0.256]\u001b[A\n",
      "Epoch 1:  61%|██████    | 367/604 [02:23<01:31,  2.58it/s, training loss=0.191]\u001b[A\n",
      "Epoch 1:  61%|██████    | 368/604 [02:23<01:31,  2.58it/s, training loss=0.191]\u001b[A\n",
      "Epoch 1:  61%|██████    | 368/604 [02:24<01:31,  2.58it/s, training loss=0.249]\u001b[A\n",
      "Epoch 1:  61%|██████    | 369/604 [02:24<01:31,  2.58it/s, training loss=0.249]\u001b[A\n",
      "Epoch 1:  61%|██████    | 369/604 [02:24<01:31,  2.58it/s, training loss=0.236]\u001b[A\n",
      "Epoch 1:  61%|██████▏   | 370/604 [02:24<01:31,  2.55it/s, training loss=0.236]\u001b[A\n",
      "Epoch 1:  61%|██████▏   | 370/604 [02:25<01:31,  2.55it/s, training loss=0.256]\u001b[A\n",
      "Epoch 1:  61%|██████▏   | 371/604 [02:25<01:33,  2.48it/s, training loss=0.256]\u001b[A\n",
      "Epoch 1:  61%|██████▏   | 371/604 [02:25<01:33,  2.48it/s, training loss=0.214]\u001b[A\n",
      "Epoch 1:  62%|██████▏   | 372/604 [02:25<01:32,  2.50it/s, training loss=0.214]\u001b[A\n",
      "Epoch 1:  62%|██████▏   | 372/604 [02:25<01:32,  2.50it/s, training loss=0.216]\u001b[A\n",
      "Epoch 1:  62%|██████▏   | 373/604 [02:25<01:31,  2.53it/s, training loss=0.216]\u001b[A\n",
      "Epoch 1:  62%|██████▏   | 373/604 [02:26<01:31,  2.53it/s, training loss=0.323]\u001b[A\n",
      "Epoch 1:  62%|██████▏   | 374/604 [02:26<01:30,  2.55it/s, training loss=0.323]\u001b[A\n",
      "Epoch 1:  62%|██████▏   | 374/604 [02:26<01:30,  2.55it/s, training loss=0.218]\u001b[A\n",
      "Epoch 1:  62%|██████▏   | 375/604 [02:26<01:29,  2.56it/s, training loss=0.218]\u001b[A\n",
      "Epoch 1:  62%|██████▏   | 375/604 [02:27<01:29,  2.56it/s, training loss=0.196]\u001b[A\n",
      "Epoch 1:  62%|██████▏   | 376/604 [02:27<01:28,  2.57it/s, training loss=0.196]\u001b[A\n",
      "Epoch 1:  62%|██████▏   | 376/604 [02:27<01:28,  2.57it/s, training loss=0.213]\u001b[A\n",
      "Epoch 1:  62%|██████▏   | 377/604 [02:27<01:28,  2.58it/s, training loss=0.213]\u001b[A\n",
      "Epoch 1:  62%|██████▏   | 377/604 [02:27<01:28,  2.58it/s, training loss=0.190]\u001b[A\n",
      "Epoch 1:  63%|██████▎   | 378/604 [02:27<01:27,  2.58it/s, training loss=0.190]\u001b[A\n",
      "Epoch 1:  63%|██████▎   | 378/604 [02:28<01:27,  2.58it/s, training loss=0.228]\u001b[A\n",
      "Epoch 1:  63%|██████▎   | 379/604 [02:28<01:26,  2.59it/s, training loss=0.228]\u001b[A\n",
      "Epoch 1:  63%|██████▎   | 379/604 [02:28<01:26,  2.59it/s, training loss=0.233]\u001b[A\n",
      "Epoch 1:  63%|██████▎   | 380/604 [02:28<01:26,  2.59it/s, training loss=0.233]\u001b[A\n",
      "Epoch 1:  63%|██████▎   | 380/604 [02:29<01:26,  2.59it/s, training loss=0.234]\u001b[A\n",
      "Epoch 1:  63%|██████▎   | 381/604 [02:29<01:26,  2.59it/s, training loss=0.234]\u001b[A\n",
      "Epoch 1:  63%|██████▎   | 381/604 [02:29<01:26,  2.59it/s, training loss=0.192]\u001b[A\n",
      "Epoch 1:  63%|██████▎   | 382/604 [02:29<01:25,  2.59it/s, training loss=0.192]\u001b[A\n",
      "Epoch 1:  63%|██████▎   | 382/604 [02:29<01:25,  2.59it/s, training loss=0.299]\u001b[A\n",
      "Epoch 1:  63%|██████▎   | 383/604 [02:29<01:25,  2.59it/s, training loss=0.299]\u001b[A\n",
      "Epoch 1:  63%|██████▎   | 383/604 [02:30<01:25,  2.59it/s, training loss=0.303]\u001b[A\n",
      "Epoch 1:  64%|██████▎   | 384/604 [02:30<01:25,  2.58it/s, training loss=0.303]\u001b[A\n",
      "Epoch 1:  64%|██████▎   | 384/604 [02:30<01:25,  2.58it/s, training loss=0.227]\u001b[A\n",
      "Epoch 1:  64%|██████▎   | 385/604 [02:30<01:24,  2.59it/s, training loss=0.227]\u001b[A\n",
      "Epoch 1:  64%|██████▎   | 385/604 [02:31<01:24,  2.59it/s, training loss=0.218]\u001b[A\n",
      "Epoch 1:  64%|██████▍   | 386/604 [02:31<01:24,  2.59it/s, training loss=0.218]\u001b[A\n",
      "Epoch 1:  64%|██████▍   | 386/604 [02:31<01:24,  2.59it/s, training loss=0.263]\u001b[A\n",
      "Epoch 1:  64%|██████▍   | 387/604 [02:31<01:23,  2.59it/s, training loss=0.263]\u001b[A\n",
      "Epoch 1:  64%|██████▍   | 387/604 [02:31<01:23,  2.59it/s, training loss=0.335]\u001b[A\n",
      "Epoch 1:  64%|██████▍   | 388/604 [02:31<01:23,  2.59it/s, training loss=0.335]\u001b[A\n",
      "Epoch 1:  64%|██████▍   | 388/604 [02:32<01:23,  2.59it/s, training loss=0.194]\u001b[A\n",
      "Epoch 1:  64%|██████▍   | 389/604 [02:32<01:22,  2.59it/s, training loss=0.194]\u001b[A\n",
      "Epoch 1:  64%|██████▍   | 389/604 [02:32<01:22,  2.59it/s, training loss=0.330]\u001b[A\n",
      "Epoch 1:  65%|██████▍   | 390/604 [02:32<01:22,  2.59it/s, training loss=0.330]\u001b[A\n",
      "Epoch 1:  65%|██████▍   | 390/604 [02:32<01:22,  2.59it/s, training loss=0.218]\u001b[A\n",
      "Epoch 1:  65%|██████▍   | 391/604 [02:32<01:22,  2.59it/s, training loss=0.218]\u001b[A\n",
      "Epoch 1:  65%|██████▍   | 391/604 [02:33<01:22,  2.59it/s, training loss=0.233]\u001b[A\n",
      "Epoch 1:  65%|██████▍   | 392/604 [02:33<01:22,  2.58it/s, training loss=0.233]\u001b[A\n",
      "Epoch 1:  65%|██████▍   | 392/604 [02:33<01:22,  2.58it/s, training loss=0.186]\u001b[A\n",
      "Epoch 1:  65%|██████▌   | 393/604 [02:33<01:21,  2.58it/s, training loss=0.186]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  65%|██████▌   | 393/604 [02:34<01:21,  2.58it/s, training loss=0.265]\u001b[A\n",
      "Epoch 1:  65%|██████▌   | 394/604 [02:34<01:21,  2.59it/s, training loss=0.265]\u001b[A\n",
      "Epoch 1:  65%|██████▌   | 394/604 [02:34<01:21,  2.59it/s, training loss=0.195]\u001b[A\n",
      "Epoch 1:  65%|██████▌   | 395/604 [02:34<01:20,  2.59it/s, training loss=0.195]\u001b[A\n",
      "Epoch 1:  65%|██████▌   | 395/604 [02:34<01:20,  2.59it/s, training loss=0.257]\u001b[A\n",
      "Epoch 1:  66%|██████▌   | 396/604 [02:34<01:20,  2.59it/s, training loss=0.257]\u001b[A\n",
      "Epoch 1:  66%|██████▌   | 396/604 [02:35<01:20,  2.59it/s, training loss=0.223]\u001b[A\n",
      "Epoch 1:  66%|██████▌   | 397/604 [02:35<01:19,  2.59it/s, training loss=0.223]\u001b[A\n",
      "Epoch 1:  66%|██████▌   | 397/604 [02:35<01:19,  2.59it/s, training loss=0.281]\u001b[A\n",
      "Epoch 1:  66%|██████▌   | 398/604 [02:35<01:19,  2.59it/s, training loss=0.281]\u001b[A\n",
      "Epoch 1:  66%|██████▌   | 398/604 [02:36<01:19,  2.59it/s, training loss=0.245]\u001b[A\n",
      "Epoch 1:  66%|██████▌   | 399/604 [02:36<01:19,  2.57it/s, training loss=0.245]\u001b[A\n",
      "Epoch 1:  66%|██████▌   | 399/604 [02:36<01:19,  2.57it/s, training loss=0.236]\u001b[A\n",
      "Epoch 1:  66%|██████▌   | 400/604 [02:36<01:19,  2.57it/s, training loss=0.236]\u001b[A\n",
      "Epoch 1:  66%|██████▌   | 400/604 [02:36<01:19,  2.57it/s, training loss=0.176]\u001b[A\n",
      "Epoch 1:  66%|██████▋   | 401/604 [02:36<01:18,  2.58it/s, training loss=0.176]\u001b[A\n",
      "Epoch 1:  66%|██████▋   | 401/604 [02:37<01:18,  2.58it/s, training loss=0.246]\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 402/604 [02:37<01:18,  2.58it/s, training loss=0.246]\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 402/604 [02:37<01:18,  2.58it/s, training loss=0.210]\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 403/604 [02:37<01:17,  2.59it/s, training loss=0.210]\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 403/604 [02:37<01:17,  2.59it/s, training loss=0.219]\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 404/604 [02:37<01:17,  2.59it/s, training loss=0.219]\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 404/604 [02:38<01:17,  2.59it/s, training loss=0.228]\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 405/604 [02:38<01:16,  2.59it/s, training loss=0.228]\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 405/604 [02:38<01:16,  2.59it/s, training loss=0.281]\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 406/604 [02:38<01:16,  2.59it/s, training loss=0.281]\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 406/604 [02:39<01:16,  2.59it/s, training loss=0.209]\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 407/604 [02:39<01:15,  2.59it/s, training loss=0.209]\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 407/604 [02:39<01:15,  2.59it/s, training loss=0.192]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 408/604 [02:39<01:15,  2.59it/s, training loss=0.192]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 408/604 [02:39<01:15,  2.59it/s, training loss=0.247]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 409/604 [02:39<01:17,  2.51it/s, training loss=0.247]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 409/604 [02:40<01:17,  2.51it/s, training loss=0.219]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 410/604 [02:40<01:18,  2.46it/s, training loss=0.219]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 410/604 [02:40<01:18,  2.46it/s, training loss=0.193]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 411/604 [02:40<01:17,  2.48it/s, training loss=0.193]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 411/604 [02:41<01:17,  2.48it/s, training loss=0.198]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 412/604 [02:41<01:16,  2.51it/s, training loss=0.198]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 412/604 [02:41<01:16,  2.51it/s, training loss=0.270]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 413/604 [02:41<01:16,  2.49it/s, training loss=0.270]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 413/604 [02:41<01:16,  2.49it/s, training loss=0.208]\u001b[A\n",
      "Epoch 1:  69%|██████▊   | 414/604 [02:41<01:16,  2.50it/s, training loss=0.208]\u001b[A\n",
      "Epoch 1:  69%|██████▊   | 414/604 [02:42<01:16,  2.50it/s, training loss=0.238]\u001b[A\n",
      "Epoch 1:  69%|██████▊   | 415/604 [02:42<01:14,  2.53it/s, training loss=0.238]\u001b[A\n",
      "Epoch 1:  69%|██████▊   | 415/604 [02:42<01:14,  2.53it/s, training loss=0.271]\u001b[A\n",
      "Epoch 1:  69%|██████▉   | 416/604 [02:42<01:13,  2.55it/s, training loss=0.271]\u001b[A\n",
      "Epoch 1:  69%|██████▉   | 416/604 [02:43<01:13,  2.55it/s, training loss=0.207]\u001b[A\n",
      "Epoch 1:  69%|██████▉   | 417/604 [02:43<01:13,  2.56it/s, training loss=0.207]\u001b[A\n",
      "Epoch 1:  69%|██████▉   | 417/604 [02:43<01:13,  2.56it/s, training loss=0.277]\u001b[A\n",
      "Epoch 1:  69%|██████▉   | 418/604 [02:43<01:12,  2.57it/s, training loss=0.277]\u001b[A\n",
      "Epoch 1:  69%|██████▉   | 418/604 [02:43<01:12,  2.57it/s, training loss=0.272]\u001b[A\n",
      "Epoch 1:  69%|██████▉   | 419/604 [02:43<01:11,  2.58it/s, training loss=0.272]\u001b[A\n",
      "Epoch 1:  69%|██████▉   | 419/604 [02:44<01:11,  2.58it/s, training loss=0.226]\u001b[A\n",
      "Epoch 1:  70%|██████▉   | 420/604 [02:44<01:11,  2.58it/s, training loss=0.226]\u001b[A\n",
      "Epoch 1:  70%|██████▉   | 420/604 [02:44<01:11,  2.58it/s, training loss=0.216]\u001b[A\n",
      "Epoch 1:  70%|██████▉   | 421/604 [02:44<01:10,  2.59it/s, training loss=0.216]\u001b[A\n",
      "Epoch 1:  70%|██████▉   | 421/604 [02:45<01:10,  2.59it/s, training loss=0.282]\u001b[A\n",
      "Epoch 1:  70%|██████▉   | 422/604 [02:45<01:10,  2.59it/s, training loss=0.282]\u001b[A\n",
      "Epoch 1:  70%|██████▉   | 422/604 [02:45<01:10,  2.59it/s, training loss=0.305]\u001b[A\n",
      "Epoch 1:  70%|███████   | 423/604 [02:45<01:09,  2.59it/s, training loss=0.305]\u001b[A\n",
      "Epoch 1:  70%|███████   | 423/604 [02:45<01:09,  2.59it/s, training loss=0.187]\u001b[A\n",
      "Epoch 1:  70%|███████   | 424/604 [02:45<01:11,  2.51it/s, training loss=0.187]\u001b[A\n",
      "Epoch 1:  70%|███████   | 424/604 [02:46<01:11,  2.51it/s, training loss=0.223]\u001b[A\n",
      "Epoch 1:  70%|███████   | 425/604 [02:46<01:10,  2.52it/s, training loss=0.223]\u001b[A\n",
      "Epoch 1:  70%|███████   | 425/604 [02:46<01:10,  2.52it/s, training loss=0.237]\u001b[A\n",
      "Epoch 1:  71%|███████   | 426/604 [02:46<01:10,  2.54it/s, training loss=0.237]\u001b[A\n",
      "Epoch 1:  71%|███████   | 426/604 [02:47<01:10,  2.54it/s, training loss=0.211]\u001b[A\n",
      "Epoch 1:  71%|███████   | 427/604 [02:47<01:12,  2.45it/s, training loss=0.211]\u001b[A\n",
      "Epoch 1:  71%|███████   | 427/604 [02:47<01:12,  2.45it/s, training loss=0.222]\u001b[A\n",
      "Epoch 1:  71%|███████   | 428/604 [02:47<01:11,  2.48it/s, training loss=0.222]\u001b[A\n",
      "Epoch 1:  71%|███████   | 428/604 [02:47<01:11,  2.48it/s, training loss=0.189]\u001b[A\n",
      "Epoch 1:  71%|███████   | 429/604 [02:47<01:10,  2.47it/s, training loss=0.189]\u001b[A\n",
      "Epoch 1:  71%|███████   | 429/604 [02:48<01:10,  2.47it/s, training loss=0.246]\u001b[A\n",
      "Epoch 1:  71%|███████   | 430/604 [02:48<01:09,  2.50it/s, training loss=0.246]\u001b[A\n",
      "Epoch 1:  71%|███████   | 430/604 [02:48<01:09,  2.50it/s, training loss=0.301]\u001b[A\n",
      "Epoch 1:  71%|███████▏  | 431/604 [02:48<01:09,  2.49it/s, training loss=0.301]\u001b[A\n",
      "Epoch 1:  71%|███████▏  | 431/604 [02:49<01:09,  2.49it/s, training loss=0.226]\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 432/604 [02:49<01:08,  2.51it/s, training loss=0.226]\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 432/604 [02:49<01:08,  2.51it/s, training loss=0.261]\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 433/604 [02:49<01:07,  2.54it/s, training loss=0.261]\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 433/604 [02:49<01:07,  2.54it/s, training loss=0.256]\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 434/604 [02:49<01:07,  2.53it/s, training loss=0.256]\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 434/604 [02:50<01:07,  2.53it/s, training loss=0.283]\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 435/604 [02:50<01:06,  2.54it/s, training loss=0.283]\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 435/604 [02:50<01:06,  2.54it/s, training loss=0.278]\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 436/604 [02:50<01:06,  2.54it/s, training loss=0.278]\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 436/604 [02:51<01:06,  2.54it/s, training loss=0.235]\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 437/604 [02:51<01:05,  2.55it/s, training loss=0.235]\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 437/604 [02:51<01:05,  2.55it/s, training loss=0.159]\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 438/604 [02:51<01:04,  2.57it/s, training loss=0.159]\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 438/604 [02:51<01:04,  2.57it/s, training loss=0.282]\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 439/604 [02:51<01:04,  2.57it/s, training loss=0.282]\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 439/604 [02:52<01:04,  2.57it/s, training loss=0.230]\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 440/604 [02:52<01:03,  2.58it/s, training loss=0.230]\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 440/604 [02:52<01:03,  2.58it/s, training loss=0.200]\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 441/604 [02:52<01:03,  2.58it/s, training loss=0.200]\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 441/604 [02:52<01:03,  2.58it/s, training loss=0.188]\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 442/604 [02:52<01:02,  2.58it/s, training loss=0.188]\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 442/604 [02:53<01:02,  2.58it/s, training loss=0.223]\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 443/604 [02:53<01:02,  2.58it/s, training loss=0.223]\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 443/604 [02:53<01:02,  2.58it/s, training loss=0.171]\u001b[A\n",
      "Epoch 1:  74%|███████▎  | 444/604 [02:53<01:02,  2.58it/s, training loss=0.171]\u001b[A\n",
      "Epoch 1:  74%|███████▎  | 444/604 [02:54<01:02,  2.58it/s, training loss=0.207]\u001b[A\n",
      "Epoch 1:  74%|███████▎  | 445/604 [02:54<01:01,  2.58it/s, training loss=0.207]\u001b[A\n",
      "Epoch 1:  74%|███████▎  | 445/604 [02:54<01:01,  2.58it/s, training loss=0.192]\u001b[A\n",
      "Epoch 1:  74%|███████▍  | 446/604 [02:54<01:01,  2.58it/s, training loss=0.192]\u001b[A\n",
      "Epoch 1:  74%|███████▍  | 446/604 [02:54<01:01,  2.58it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  74%|███████▍  | 447/604 [02:54<01:00,  2.59it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  74%|███████▍  | 447/604 [02:55<01:00,  2.59it/s, training loss=0.304]\u001b[A\n",
      "Epoch 1:  74%|███████▍  | 448/604 [02:55<01:00,  2.59it/s, training loss=0.304]\u001b[A\n",
      "Epoch 1:  74%|███████▍  | 448/604 [02:55<01:00,  2.59it/s, training loss=0.201]\u001b[A\n",
      "Epoch 1:  74%|███████▍  | 449/604 [02:55<00:59,  2.59it/s, training loss=0.201]\u001b[A\n",
      "Epoch 1:  74%|███████▍  | 449/604 [02:56<00:59,  2.59it/s, training loss=0.190]\u001b[A\n",
      "Epoch 1:  75%|███████▍  | 450/604 [02:56<00:59,  2.59it/s, training loss=0.190]\u001b[A\n",
      "Epoch 1:  75%|███████▍  | 450/604 [02:56<00:59,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 1:  75%|███████▍  | 451/604 [02:56<00:59,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 1:  75%|███████▍  | 451/604 [02:56<00:59,  2.59it/s, training loss=0.216]\u001b[A\n",
      "Epoch 1:  75%|███████▍  | 452/604 [02:56<00:58,  2.59it/s, training loss=0.216]\u001b[A\n",
      "Epoch 1:  75%|███████▍  | 452/604 [02:57<00:58,  2.59it/s, training loss=0.256]\u001b[A\n",
      "Epoch 1:  75%|███████▌  | 453/604 [02:57<00:59,  2.55it/s, training loss=0.256]\u001b[A\n",
      "Epoch 1:  75%|███████▌  | 453/604 [02:57<00:59,  2.55it/s, training loss=0.207]\u001b[A\n",
      "Epoch 1:  75%|███████▌  | 454/604 [02:57<00:58,  2.55it/s, training loss=0.207]\u001b[A\n",
      "Epoch 1:  75%|███████▌  | 454/604 [02:57<00:58,  2.55it/s, training loss=0.201]\u001b[A\n",
      "Epoch 1:  75%|███████▌  | 455/604 [02:57<00:58,  2.56it/s, training loss=0.201]\u001b[A\n",
      "Epoch 1:  75%|███████▌  | 455/604 [02:58<00:58,  2.56it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  75%|███████▌  | 456/604 [02:58<00:57,  2.57it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  75%|███████▌  | 456/604 [02:58<00:57,  2.57it/s, training loss=0.223]\u001b[A\n",
      "Epoch 1:  76%|███████▌  | 457/604 [02:58<00:57,  2.58it/s, training loss=0.223]\u001b[A\n",
      "Epoch 1:  76%|███████▌  | 457/604 [02:59<00:57,  2.58it/s, training loss=0.208]\u001b[A\n",
      "Epoch 1:  76%|███████▌  | 458/604 [02:59<00:56,  2.58it/s, training loss=0.208]\u001b[A\n",
      "Epoch 1:  76%|███████▌  | 458/604 [02:59<00:56,  2.58it/s, training loss=0.200]\u001b[A\n",
      "Epoch 1:  76%|███████▌  | 459/604 [02:59<00:56,  2.56it/s, training loss=0.200]\u001b[A\n",
      "Epoch 1:  76%|███████▌  | 459/604 [02:59<00:56,  2.56it/s, training loss=0.218]\u001b[A\n",
      "Epoch 1:  76%|███████▌  | 460/604 [02:59<00:56,  2.57it/s, training loss=0.218]\u001b[A\n",
      "Epoch 1:  76%|███████▌  | 460/604 [03:00<00:56,  2.57it/s, training loss=0.212]\u001b[A\n",
      "Epoch 1:  76%|███████▋  | 461/604 [03:00<00:55,  2.56it/s, training loss=0.212]\u001b[A\n",
      "Epoch 1:  76%|███████▋  | 461/604 [03:00<00:55,  2.56it/s, training loss=0.181]\u001b[A\n",
      "Epoch 1:  76%|███████▋  | 462/604 [03:00<00:55,  2.57it/s, training loss=0.181]\u001b[A\n",
      "Epoch 1:  76%|███████▋  | 462/604 [03:01<00:55,  2.57it/s, training loss=0.230]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 463/604 [03:01<00:54,  2.57it/s, training loss=0.230]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 463/604 [03:01<00:54,  2.57it/s, training loss=0.194]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 464/604 [03:01<00:54,  2.58it/s, training loss=0.194]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 464/604 [03:01<00:54,  2.58it/s, training loss=0.244]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 465/604 [03:01<00:53,  2.58it/s, training loss=0.244]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 465/604 [03:02<00:53,  2.58it/s, training loss=0.220]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 466/604 [03:02<00:53,  2.59it/s, training loss=0.220]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 466/604 [03:02<00:53,  2.59it/s, training loss=0.214]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 467/604 [03:02<00:52,  2.59it/s, training loss=0.214]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 467/604 [03:03<00:52,  2.59it/s, training loss=0.274]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 468/604 [03:03<00:52,  2.59it/s, training loss=0.274]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 468/604 [03:03<00:52,  2.59it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  78%|███████▊  | 469/604 [03:03<00:52,  2.59it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  78%|███████▊  | 469/604 [03:03<00:52,  2.59it/s, training loss=0.225]\u001b[A\n",
      "Epoch 1:  78%|███████▊  | 470/604 [03:03<00:51,  2.59it/s, training loss=0.225]\u001b[A\n",
      "Epoch 1:  78%|███████▊  | 470/604 [03:04<00:51,  2.59it/s, training loss=0.180]\u001b[A\n",
      "Epoch 1:  78%|███████▊  | 471/604 [03:04<00:51,  2.59it/s, training loss=0.180]\u001b[A\n",
      "Epoch 1:  78%|███████▊  | 471/604 [03:04<00:51,  2.59it/s, training loss=0.249]\u001b[A\n",
      "Epoch 1:  78%|███████▊  | 472/604 [03:04<00:50,  2.59it/s, training loss=0.249]\u001b[A\n",
      "Epoch 1:  78%|███████▊  | 472/604 [03:04<00:50,  2.59it/s, training loss=0.213]\u001b[A\n",
      "Epoch 1:  78%|███████▊  | 473/604 [03:04<00:50,  2.59it/s, training loss=0.213]\u001b[A\n",
      "Epoch 1:  78%|███████▊  | 473/604 [03:05<00:50,  2.59it/s, training loss=0.262]\u001b[A\n",
      "Epoch 1:  78%|███████▊  | 474/604 [03:05<00:50,  2.60it/s, training loss=0.262]\u001b[A\n",
      "Epoch 1:  78%|███████▊  | 474/604 [03:05<00:50,  2.60it/s, training loss=0.230]\u001b[A\n",
      "Epoch 1:  79%|███████▊  | 475/604 [03:05<00:49,  2.60it/s, training loss=0.230]\u001b[A\n",
      "Epoch 1:  79%|███████▊  | 475/604 [03:06<00:49,  2.60it/s, training loss=0.236]\u001b[A\n",
      "Epoch 1:  79%|███████▉  | 476/604 [03:06<00:49,  2.59it/s, training loss=0.236]\u001b[A\n",
      "Epoch 1:  79%|███████▉  | 476/604 [03:06<00:49,  2.59it/s, training loss=0.176]\u001b[A\n",
      "Epoch 1:  79%|███████▉  | 477/604 [03:06<00:49,  2.59it/s, training loss=0.176]\u001b[A\n",
      "Epoch 1:  79%|███████▉  | 477/604 [03:06<00:49,  2.59it/s, training loss=0.236]\u001b[A\n",
      "Epoch 1:  79%|███████▉  | 478/604 [03:06<00:48,  2.59it/s, training loss=0.236]\u001b[A\n",
      "Epoch 1:  79%|███████▉  | 478/604 [03:07<00:48,  2.59it/s, training loss=0.194]\u001b[A\n",
      "Epoch 1:  79%|███████▉  | 479/604 [03:07<00:48,  2.59it/s, training loss=0.194]\u001b[A\n",
      "Epoch 1:  79%|███████▉  | 479/604 [03:07<00:48,  2.59it/s, training loss=0.188]\u001b[A\n",
      "Epoch 1:  79%|███████▉  | 480/604 [03:07<00:48,  2.58it/s, training loss=0.188]\u001b[A\n",
      "Epoch 1:  79%|███████▉  | 480/604 [03:08<00:48,  2.58it/s, training loss=0.265]\u001b[A\n",
      "Epoch 1:  80%|███████▉  | 481/604 [03:08<00:48,  2.56it/s, training loss=0.265]\u001b[A\n",
      "Epoch 1:  80%|███████▉  | 481/604 [03:08<00:48,  2.56it/s, training loss=0.219]\u001b[A\n",
      "Epoch 1:  80%|███████▉  | 482/604 [03:08<00:47,  2.57it/s, training loss=0.219]\u001b[A\n",
      "Epoch 1:  80%|███████▉  | 482/604 [03:08<00:47,  2.57it/s, training loss=0.265]\u001b[A\n",
      "Epoch 1:  80%|███████▉  | 483/604 [03:08<00:47,  2.57it/s, training loss=0.265]\u001b[A\n",
      "Epoch 1:  80%|███████▉  | 483/604 [03:09<00:47,  2.57it/s, training loss=0.225]\u001b[A\n",
      "Epoch 1:  80%|████████  | 484/604 [03:09<00:46,  2.58it/s, training loss=0.225]\u001b[A\n",
      "Epoch 1:  80%|████████  | 484/604 [03:09<00:46,  2.58it/s, training loss=0.255]\u001b[A\n",
      "Epoch 1:  80%|████████  | 485/604 [03:09<00:46,  2.58it/s, training loss=0.255]\u001b[A\n",
      "Epoch 1:  80%|████████  | 485/604 [03:09<00:46,  2.58it/s, training loss=0.255]\u001b[A\n",
      "Epoch 1:  80%|████████  | 486/604 [03:09<00:45,  2.59it/s, training loss=0.255]\u001b[A\n",
      "Epoch 1:  80%|████████  | 486/604 [03:10<00:45,  2.59it/s, training loss=0.206]\u001b[A\n",
      "Epoch 1:  81%|████████  | 487/604 [03:10<00:45,  2.59it/s, training loss=0.206]\u001b[A\n",
      "Epoch 1:  81%|████████  | 487/604 [03:10<00:45,  2.59it/s, training loss=0.243]\u001b[A\n",
      "Epoch 1:  81%|████████  | 488/604 [03:10<00:44,  2.59it/s, training loss=0.243]\u001b[A\n",
      "Epoch 1:  81%|████████  | 488/604 [03:11<00:44,  2.59it/s, training loss=0.230]\u001b[A\n",
      "Epoch 1:  81%|████████  | 489/604 [03:11<00:44,  2.59it/s, training loss=0.230]\u001b[A\n",
      "Epoch 1:  81%|████████  | 489/604 [03:11<00:44,  2.59it/s, training loss=0.209]\u001b[A\n",
      "Epoch 1:  81%|████████  | 490/604 [03:11<00:44,  2.59it/s, training loss=0.209]\u001b[A\n",
      "Epoch 1:  81%|████████  | 490/604 [03:11<00:44,  2.59it/s, training loss=0.249]\u001b[A\n",
      "Epoch 1:  81%|████████▏ | 491/604 [03:11<00:43,  2.59it/s, training loss=0.249]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  81%|████████▏ | 491/604 [03:12<00:43,  2.59it/s, training loss=0.318]\u001b[A\n",
      "Epoch 1:  81%|████████▏ | 492/604 [03:12<00:43,  2.59it/s, training loss=0.318]\u001b[A\n",
      "Epoch 1:  81%|████████▏ | 492/604 [03:12<00:43,  2.59it/s, training loss=0.220]\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 493/604 [03:12<00:42,  2.59it/s, training loss=0.220]\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 493/604 [03:13<00:42,  2.59it/s, training loss=0.189]\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 494/604 [03:13<00:42,  2.59it/s, training loss=0.189]\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 494/604 [03:13<00:42,  2.59it/s, training loss=0.228]\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 495/604 [03:13<00:42,  2.59it/s, training loss=0.228]\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 495/604 [03:13<00:42,  2.59it/s, training loss=0.185]\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 496/604 [03:13<00:41,  2.59it/s, training loss=0.185]\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 496/604 [03:14<00:41,  2.59it/s, training loss=0.210]\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 497/604 [03:14<00:41,  2.59it/s, training loss=0.210]\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 497/604 [03:14<00:41,  2.59it/s, training loss=0.174]\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 498/604 [03:14<00:40,  2.60it/s, training loss=0.174]\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 498/604 [03:15<00:40,  2.60it/s, training loss=0.146]\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 499/604 [03:15<00:40,  2.60it/s, training loss=0.146]\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 499/604 [03:15<00:40,  2.60it/s, training loss=0.164]\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 500/604 [03:15<00:40,  2.59it/s, training loss=0.164]\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 500/604 [03:15<00:40,  2.59it/s, training loss=0.301]\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 501/604 [03:15<00:39,  2.60it/s, training loss=0.301]\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 501/604 [03:16<00:39,  2.60it/s, training loss=0.244]\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 502/604 [03:16<00:39,  2.59it/s, training loss=0.244]\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 502/604 [03:16<00:39,  2.59it/s, training loss=0.229]\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 503/604 [03:16<00:38,  2.59it/s, training loss=0.229]\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 503/604 [03:16<00:38,  2.59it/s, training loss=0.168]\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 504/604 [03:16<00:38,  2.59it/s, training loss=0.168]\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 504/604 [03:17<00:38,  2.59it/s, training loss=0.178]\u001b[A\n",
      "Epoch 1:  84%|████████▎ | 505/604 [03:17<00:38,  2.56it/s, training loss=0.178]\u001b[A\n",
      "Epoch 1:  84%|████████▎ | 505/604 [03:17<00:38,  2.56it/s, training loss=0.198]\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 506/604 [03:17<00:38,  2.57it/s, training loss=0.198]\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 506/604 [03:18<00:38,  2.57it/s, training loss=0.255]\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 507/604 [03:18<00:37,  2.57it/s, training loss=0.255]\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 507/604 [03:18<00:37,  2.57it/s, training loss=0.200]\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 508/604 [03:18<00:37,  2.57it/s, training loss=0.200]\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 508/604 [03:18<00:37,  2.57it/s, training loss=0.280]\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 509/604 [03:18<00:37,  2.53it/s, training loss=0.280]\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 509/604 [03:19<00:37,  2.53it/s, training loss=0.153]\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 510/604 [03:19<00:37,  2.49it/s, training loss=0.153]\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 510/604 [03:19<00:37,  2.49it/s, training loss=0.246]\u001b[A\n",
      "Epoch 1:  85%|████████▍ | 511/604 [03:19<00:36,  2.52it/s, training loss=0.246]\u001b[A\n",
      "Epoch 1:  85%|████████▍ | 511/604 [03:20<00:36,  2.52it/s, training loss=0.219]\u001b[A\n",
      "Epoch 1:  85%|████████▍ | 512/604 [03:20<00:36,  2.51it/s, training loss=0.219]\u001b[A\n",
      "Epoch 1:  85%|████████▍ | 512/604 [03:20<00:36,  2.51it/s, training loss=0.204]\u001b[A\n",
      "Epoch 1:  85%|████████▍ | 513/604 [03:20<00:35,  2.53it/s, training loss=0.204]\u001b[A\n",
      "Epoch 1:  85%|████████▍ | 513/604 [03:20<00:35,  2.53it/s, training loss=0.177]\u001b[A\n",
      "Epoch 1:  85%|████████▌ | 514/604 [03:20<00:35,  2.55it/s, training loss=0.177]\u001b[A\n",
      "Epoch 1:  85%|████████▌ | 514/604 [03:21<00:35,  2.55it/s, training loss=0.223]\u001b[A\n",
      "Epoch 1:  85%|████████▌ | 515/604 [03:21<00:34,  2.56it/s, training loss=0.223]\u001b[A\n",
      "Epoch 1:  85%|████████▌ | 515/604 [03:21<00:34,  2.56it/s, training loss=0.238]\u001b[A\n",
      "Epoch 1:  85%|████████▌ | 516/604 [03:21<00:34,  2.57it/s, training loss=0.238]\u001b[A\n",
      "Epoch 1:  85%|████████▌ | 516/604 [03:22<00:34,  2.57it/s, training loss=0.216]\u001b[A\n",
      "Epoch 1:  86%|████████▌ | 517/604 [03:22<00:33,  2.58it/s, training loss=0.216]\u001b[A\n",
      "Epoch 1:  86%|████████▌ | 517/604 [03:22<00:33,  2.58it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  86%|████████▌ | 518/604 [03:22<00:33,  2.58it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  86%|████████▌ | 518/604 [03:22<00:33,  2.58it/s, training loss=0.285]\u001b[A\n",
      "Epoch 1:  86%|████████▌ | 519/604 [03:22<00:32,  2.59it/s, training loss=0.285]\u001b[A\n",
      "Epoch 1:  86%|████████▌ | 519/604 [03:23<00:32,  2.59it/s, training loss=0.203]\u001b[A\n",
      "Epoch 1:  86%|████████▌ | 520/604 [03:23<00:32,  2.59it/s, training loss=0.203]\u001b[A\n",
      "Epoch 1:  86%|████████▌ | 520/604 [03:23<00:32,  2.59it/s, training loss=0.218]\u001b[A\n",
      "Epoch 1:  86%|████████▋ | 521/604 [03:23<00:32,  2.59it/s, training loss=0.218]\u001b[A\n",
      "Epoch 1:  86%|████████▋ | 521/604 [03:23<00:32,  2.59it/s, training loss=0.239]\u001b[A\n",
      "Epoch 1:  86%|████████▋ | 522/604 [03:23<00:31,  2.59it/s, training loss=0.239]\u001b[A\n",
      "Epoch 1:  86%|████████▋ | 522/604 [03:24<00:31,  2.59it/s, training loss=0.237]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 523/604 [03:24<00:31,  2.59it/s, training loss=0.237]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 523/604 [03:24<00:31,  2.59it/s, training loss=0.191]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 524/604 [03:24<00:30,  2.59it/s, training loss=0.191]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 524/604 [03:25<00:30,  2.59it/s, training loss=0.171]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 525/604 [03:25<00:30,  2.59it/s, training loss=0.171]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 525/604 [03:25<00:30,  2.59it/s, training loss=0.254]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 526/604 [03:25<00:30,  2.58it/s, training loss=0.254]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 526/604 [03:25<00:30,  2.58it/s, training loss=0.228]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 527/604 [03:25<00:29,  2.59it/s, training loss=0.228]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 527/604 [03:26<00:29,  2.59it/s, training loss=0.296]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 528/604 [03:26<00:29,  2.59it/s, training loss=0.296]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 528/604 [03:26<00:29,  2.59it/s, training loss=0.148]\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 529/604 [03:26<00:28,  2.59it/s, training loss=0.148]\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 529/604 [03:27<00:28,  2.59it/s, training loss=0.309]\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 530/604 [03:27<00:28,  2.59it/s, training loss=0.309]\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 530/604 [03:27<00:28,  2.59it/s, training loss=0.252]\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 531/604 [03:27<00:28,  2.59it/s, training loss=0.252]\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 531/604 [03:27<00:28,  2.59it/s, training loss=0.205]\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 532/604 [03:27<00:27,  2.59it/s, training loss=0.205]\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 532/604 [03:28<00:27,  2.59it/s, training loss=0.188]\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 533/604 [03:28<00:27,  2.59it/s, training loss=0.188]\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 533/604 [03:28<00:27,  2.59it/s, training loss=0.189]\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 534/604 [03:28<00:26,  2.59it/s, training loss=0.189]\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 534/604 [03:28<00:26,  2.59it/s, training loss=0.242]\u001b[A\n",
      "Epoch 1:  89%|████████▊ | 535/604 [03:28<00:26,  2.59it/s, training loss=0.242]\u001b[A\n",
      "Epoch 1:  89%|████████▊ | 535/604 [03:29<00:26,  2.59it/s, training loss=0.255]\u001b[A\n",
      "Epoch 1:  89%|████████▊ | 536/604 [03:29<00:26,  2.59it/s, training loss=0.255]\u001b[A\n",
      "Epoch 1:  89%|████████▊ | 536/604 [03:29<00:26,  2.59it/s, training loss=0.205]\u001b[A\n",
      "Epoch 1:  89%|████████▉ | 537/604 [03:29<00:25,  2.59it/s, training loss=0.205]\u001b[A\n",
      "Epoch 1:  89%|████████▉ | 537/604 [03:30<00:25,  2.59it/s, training loss=0.245]\u001b[A\n",
      "Epoch 1:  89%|████████▉ | 538/604 [03:30<00:25,  2.57it/s, training loss=0.245]\u001b[A\n",
      "Epoch 1:  89%|████████▉ | 538/604 [03:30<00:25,  2.57it/s, training loss=0.255]\u001b[A\n",
      "Epoch 1:  89%|████████▉ | 539/604 [03:30<00:25,  2.57it/s, training loss=0.255]\u001b[A\n",
      "Epoch 1:  89%|████████▉ | 539/604 [03:30<00:25,  2.57it/s, training loss=0.189]\u001b[A\n",
      "Epoch 1:  89%|████████▉ | 540/604 [03:30<00:24,  2.56it/s, training loss=0.189]\u001b[A\n",
      "Epoch 1:  89%|████████▉ | 540/604 [03:31<00:24,  2.56it/s, training loss=0.180]\u001b[A\n",
      "Epoch 1:  90%|████████▉ | 541/604 [03:31<00:24,  2.57it/s, training loss=0.180]\u001b[A\n",
      "Epoch 1:  90%|████████▉ | 541/604 [03:31<00:24,  2.57it/s, training loss=0.281]\u001b[A\n",
      "Epoch 1:  90%|████████▉ | 542/604 [03:31<00:24,  2.57it/s, training loss=0.281]\u001b[A\n",
      "Epoch 1:  90%|████████▉ | 542/604 [03:32<00:24,  2.57it/s, training loss=0.224]\u001b[A\n",
      "Epoch 1:  90%|████████▉ | 543/604 [03:32<00:23,  2.58it/s, training loss=0.224]\u001b[A\n",
      "Epoch 1:  90%|████████▉ | 543/604 [03:32<00:23,  2.58it/s, training loss=0.209]\u001b[A\n",
      "Epoch 1:  90%|█████████ | 544/604 [03:32<00:23,  2.58it/s, training loss=0.209]\u001b[A\n",
      "Epoch 1:  90%|█████████ | 544/604 [03:32<00:23,  2.58it/s, training loss=0.223]\u001b[A\n",
      "Epoch 1:  90%|█████████ | 545/604 [03:32<00:22,  2.57it/s, training loss=0.223]\u001b[A\n",
      "Epoch 1:  90%|█████████ | 545/604 [03:33<00:22,  2.57it/s, training loss=0.253]\u001b[A\n",
      "Epoch 1:  90%|█████████ | 546/604 [03:33<00:22,  2.57it/s, training loss=0.253]\u001b[A\n",
      "Epoch 1:  90%|█████████ | 546/604 [03:33<00:22,  2.57it/s, training loss=0.225]\u001b[A\n",
      "Epoch 1:  91%|█████████ | 547/604 [03:33<00:22,  2.58it/s, training loss=0.225]\u001b[A\n",
      "Epoch 1:  91%|█████████ | 547/604 [03:34<00:22,  2.58it/s, training loss=0.221]\u001b[A\n",
      "Epoch 1:  91%|█████████ | 548/604 [03:34<00:21,  2.58it/s, training loss=0.221]\u001b[A\n",
      "Epoch 1:  91%|█████████ | 548/604 [03:34<00:21,  2.58it/s, training loss=0.248]\u001b[A\n",
      "Epoch 1:  91%|█████████ | 549/604 [03:34<00:21,  2.59it/s, training loss=0.248]\u001b[A\n",
      "Epoch 1:  91%|█████████ | 549/604 [03:34<00:21,  2.59it/s, training loss=0.189]\u001b[A\n",
      "Epoch 1:  91%|█████████ | 550/604 [03:34<00:20,  2.59it/s, training loss=0.189]\u001b[A\n",
      "Epoch 1:  91%|█████████ | 550/604 [03:35<00:20,  2.59it/s, training loss=0.248]\u001b[A\n",
      "Epoch 1:  91%|█████████ | 551/604 [03:35<00:20,  2.59it/s, training loss=0.248]\u001b[A\n",
      "Epoch 1:  91%|█████████ | 551/604 [03:35<00:20,  2.59it/s, training loss=0.221]\u001b[A\n",
      "Epoch 1:  91%|█████████▏| 552/604 [03:35<00:20,  2.59it/s, training loss=0.221]\u001b[A\n",
      "Epoch 1:  91%|█████████▏| 552/604 [03:35<00:20,  2.59it/s, training loss=0.158]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 553/604 [03:35<00:19,  2.59it/s, training loss=0.158]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 553/604 [03:36<00:19,  2.59it/s, training loss=0.209]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 554/604 [03:36<00:19,  2.59it/s, training loss=0.209]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 554/604 [03:36<00:19,  2.59it/s, training loss=0.231]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 555/604 [03:36<00:18,  2.59it/s, training loss=0.231]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 555/604 [03:37<00:18,  2.59it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 556/604 [03:37<00:18,  2.59it/s, training loss=0.241]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 556/604 [03:37<00:18,  2.59it/s, training loss=0.215]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 557/604 [03:37<00:18,  2.59it/s, training loss=0.215]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 557/604 [03:37<00:18,  2.59it/s, training loss=0.220]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 558/604 [03:37<00:17,  2.59it/s, training loss=0.220]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 558/604 [03:38<00:17,  2.59it/s, training loss=0.274]\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 559/604 [03:38<00:17,  2.59it/s, training loss=0.274]\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 559/604 [03:38<00:17,  2.59it/s, training loss=0.187]\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 560/604 [03:38<00:16,  2.59it/s, training loss=0.187]\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 560/604 [03:39<00:16,  2.59it/s, training loss=0.175]\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 561/604 [03:39<00:16,  2.59it/s, training loss=0.175]\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 561/604 [03:39<00:16,  2.59it/s, training loss=0.235]\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 562/604 [03:39<00:16,  2.59it/s, training loss=0.235]\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 562/604 [03:39<00:16,  2.59it/s, training loss=0.212]\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 563/604 [03:39<00:17,  2.39it/s, training loss=0.212]\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 563/604 [03:40<00:17,  2.39it/s, training loss=0.196]\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 564/604 [03:40<00:16,  2.36it/s, training loss=0.196]\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 564/604 [03:40<00:16,  2.36it/s, training loss=0.200]\u001b[A\n",
      "Epoch 1:  94%|█████████▎| 565/604 [03:40<00:16,  2.42it/s, training loss=0.200]\u001b[A\n",
      "Epoch 1:  94%|█████████▎| 565/604 [03:41<00:16,  2.42it/s, training loss=0.158]\u001b[A\n",
      "Epoch 1:  94%|█████████▎| 566/604 [03:41<00:16,  2.32it/s, training loss=0.158]\u001b[A\n",
      "Epoch 1:  94%|█████████▎| 566/604 [03:41<00:16,  2.32it/s, training loss=0.282]\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 567/604 [03:41<00:15,  2.35it/s, training loss=0.282]\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 567/604 [03:42<00:15,  2.35it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 568/604 [03:42<00:14,  2.41it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 568/604 [03:42<00:14,  2.41it/s, training loss=0.185]\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 569/604 [03:42<00:14,  2.46it/s, training loss=0.185]\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 569/604 [03:42<00:14,  2.46it/s, training loss=0.263]\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 570/604 [03:42<00:13,  2.50it/s, training loss=0.263]\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 570/604 [03:43<00:13,  2.50it/s, training loss=0.184]\u001b[A\n",
      "Epoch 1:  95%|█████████▍| 571/604 [03:43<00:13,  2.52it/s, training loss=0.184]\u001b[A\n",
      "Epoch 1:  95%|█████████▍| 571/604 [03:43<00:13,  2.52it/s, training loss=0.218]\u001b[A\n",
      "Epoch 1:  95%|█████████▍| 572/604 [03:43<00:12,  2.54it/s, training loss=0.218]\u001b[A\n",
      "Epoch 1:  95%|█████████▍| 572/604 [03:43<00:12,  2.54it/s, training loss=0.172]\u001b[A\n",
      "Epoch 1:  95%|█████████▍| 573/604 [03:43<00:12,  2.55it/s, training loss=0.172]\u001b[A\n",
      "Epoch 1:  95%|█████████▍| 573/604 [03:44<00:12,  2.55it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  95%|█████████▌| 574/604 [03:44<00:11,  2.56it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  95%|█████████▌| 574/604 [03:44<00:11,  2.56it/s, training loss=0.286]\u001b[A\n",
      "Epoch 1:  95%|█████████▌| 575/604 [03:44<00:11,  2.57it/s, training loss=0.286]\u001b[A\n",
      "Epoch 1:  95%|█████████▌| 575/604 [03:45<00:11,  2.57it/s, training loss=0.257]\u001b[A\n",
      "Epoch 1:  95%|█████████▌| 576/604 [03:45<00:10,  2.57it/s, training loss=0.257]\u001b[A\n",
      "Epoch 1:  95%|█████████▌| 576/604 [03:45<00:10,  2.57it/s, training loss=0.264]\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 577/604 [03:45<00:10,  2.58it/s, training loss=0.264]\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 577/604 [03:45<00:10,  2.58it/s, training loss=0.217]\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 578/604 [03:45<00:10,  2.54it/s, training loss=0.217]\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 578/604 [03:46<00:10,  2.54it/s, training loss=0.286]\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 579/604 [03:46<00:09,  2.56it/s, training loss=0.286]\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 579/604 [03:46<00:09,  2.56it/s, training loss=0.209]\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 580/604 [03:46<00:09,  2.57it/s, training loss=0.209]\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 580/604 [03:47<00:09,  2.57it/s, training loss=0.207]\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 581/604 [03:47<00:08,  2.57it/s, training loss=0.207]\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 581/604 [03:47<00:08,  2.57it/s, training loss=0.211]\u001b[A\n",
      "Epoch 1:  96%|█████████▋| 582/604 [03:47<00:08,  2.58it/s, training loss=0.211]\u001b[A\n",
      "Epoch 1:  96%|█████████▋| 582/604 [03:47<00:08,  2.58it/s, training loss=0.308]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 583/604 [03:47<00:08,  2.58it/s, training loss=0.308]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 583/604 [03:48<00:08,  2.58it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 584/604 [03:48<00:07,  2.59it/s, training loss=0.232]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 584/604 [03:48<00:07,  2.59it/s, training loss=0.203]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 585/604 [03:48<00:07,  2.59it/s, training loss=0.203]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 585/604 [03:49<00:07,  2.59it/s, training loss=0.253]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 586/604 [03:49<00:06,  2.59it/s, training loss=0.253]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 586/604 [03:49<00:06,  2.59it/s, training loss=0.235]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 587/604 [03:49<00:06,  2.59it/s, training loss=0.235]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 587/604 [03:49<00:06,  2.59it/s, training loss=0.214]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 588/604 [03:49<00:06,  2.57it/s, training loss=0.214]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 588/604 [03:50<00:06,  2.57it/s, training loss=0.167]\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 589/604 [03:50<00:05,  2.56it/s, training loss=0.167]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  98%|█████████▊| 589/604 [03:50<00:05,  2.56it/s, training loss=0.209]\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 590/604 [03:50<00:05,  2.56it/s, training loss=0.209]\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 590/604 [03:50<00:05,  2.56it/s, training loss=0.215]\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 591/604 [03:50<00:05,  2.58it/s, training loss=0.215]\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 591/604 [03:51<00:05,  2.58it/s, training loss=0.251]\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 592/604 [03:51<00:04,  2.53it/s, training loss=0.251]\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 592/604 [03:51<00:04,  2.53it/s, training loss=0.182]\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 593/604 [03:51<00:04,  2.55it/s, training loss=0.182]\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 593/604 [03:52<00:04,  2.55it/s, training loss=0.243]\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 594/604 [03:52<00:03,  2.52it/s, training loss=0.243]\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 594/604 [03:52<00:03,  2.52it/s, training loss=0.188]\u001b[A\n",
      "Epoch 1:  99%|█████████▊| 595/604 [03:52<00:03,  2.54it/s, training loss=0.188]\u001b[A\n",
      "Epoch 1:  99%|█████████▊| 595/604 [03:52<00:03,  2.54it/s, training loss=0.191]\u001b[A\n",
      "Epoch 1:  99%|█████████▊| 596/604 [03:52<00:03,  2.54it/s, training loss=0.191]\u001b[A\n",
      "Epoch 1:  99%|█████████▊| 596/604 [03:53<00:03,  2.54it/s, training loss=0.198]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 597/604 [03:53<00:02,  2.54it/s, training loss=0.198]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 597/604 [03:53<00:02,  2.54it/s, training loss=0.224]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 598/604 [03:53<00:02,  2.55it/s, training loss=0.224]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 598/604 [03:54<00:02,  2.55it/s, training loss=0.195]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 599/604 [03:54<00:01,  2.57it/s, training loss=0.195]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 599/604 [03:54<00:01,  2.57it/s, training loss=0.179]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 600/604 [03:54<00:01,  2.57it/s, training loss=0.179]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 600/604 [03:54<00:01,  2.57it/s, training loss=0.176]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 601/604 [03:54<00:01,  2.57it/s, training loss=0.176]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 601/604 [03:55<00:01,  2.57it/s, training loss=0.198]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 602/604 [03:55<00:00,  2.58it/s, training loss=0.198]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 602/604 [03:55<00:00,  2.58it/s, training loss=0.272]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 603/604 [03:55<00:00,  2.58it/s, training loss=0.272]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 603/604 [03:55<00:00,  2.58it/s, training loss=0.194]\u001b[A\n",
      "Epoch 1: 100%|██████████| 604/604 [03:55<00:00,  2.78it/s, training loss=0.194]\u001b[A\n",
      "  0%|          | 0/3 [03:56<?, ?it/s]                                          \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch {epoch}\n",
      "Training loss: 0.7465102124885218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [04:04<08:08, 244.31s/it]\n",
      "Epoch 2:   0%|          | 0/604 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.6486472469918868\n",
      "F1 Score (weighted): 0.7143493560817514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:   0%|          | 0/604 [00:00<?, ?it/s, training loss=0.238]\u001b[A\n",
      "Epoch 2:   0%|          | 1/604 [00:00<03:54,  2.58it/s, training loss=0.238]\u001b[A\n",
      "Epoch 2:   0%|          | 1/604 [00:00<03:54,  2.58it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:   0%|          | 2/604 [00:00<03:53,  2.58it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:   0%|          | 2/604 [00:01<03:53,  2.58it/s, training loss=0.185]\u001b[A\n",
      "Epoch 2:   0%|          | 3/604 [00:01<03:52,  2.59it/s, training loss=0.185]\u001b[A\n",
      "Epoch 2:   0%|          | 3/604 [00:01<03:52,  2.59it/s, training loss=0.199]\u001b[A\n",
      "Epoch 2:   1%|          | 4/604 [00:01<03:51,  2.59it/s, training loss=0.199]\u001b[A\n",
      "Epoch 2:   1%|          | 4/604 [00:01<03:51,  2.59it/s, training loss=0.159]\u001b[A\n",
      "Epoch 2:   1%|          | 5/604 [00:01<03:51,  2.59it/s, training loss=0.159]\u001b[A\n",
      "Epoch 2:   1%|          | 5/604 [00:02<03:51,  2.59it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:   1%|          | 6/604 [00:02<03:51,  2.58it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:   1%|          | 6/604 [00:02<03:51,  2.58it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:   1%|          | 7/604 [00:02<03:58,  2.51it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:   1%|          | 7/604 [00:03<03:58,  2.51it/s, training loss=0.145]\u001b[A\n",
      "Epoch 2:   1%|▏         | 8/604 [00:03<03:56,  2.52it/s, training loss=0.145]\u001b[A\n",
      "Epoch 2:   1%|▏         | 8/604 [00:03<03:56,  2.52it/s, training loss=0.144]\u001b[A\n",
      "Epoch 2:   1%|▏         | 9/604 [00:03<03:53,  2.54it/s, training loss=0.144]\u001b[A\n",
      "Epoch 2:   1%|▏         | 9/604 [00:03<03:53,  2.54it/s, training loss=0.216]\u001b[A\n",
      "Epoch 2:   2%|▏         | 10/604 [00:03<03:52,  2.56it/s, training loss=0.216]\u001b[A\n",
      "Epoch 2:   2%|▏         | 10/604 [00:04<03:52,  2.56it/s, training loss=0.252]\u001b[A\n",
      "Epoch 2:   2%|▏         | 11/604 [00:04<03:51,  2.56it/s, training loss=0.252]\u001b[A\n",
      "Epoch 2:   2%|▏         | 11/604 [00:04<03:51,  2.56it/s, training loss=0.143]\u001b[A\n",
      "Epoch 2:   2%|▏         | 12/604 [00:04<03:50,  2.57it/s, training loss=0.143]\u001b[A\n",
      "Epoch 2:   2%|▏         | 12/604 [00:05<03:50,  2.57it/s, training loss=0.134]\u001b[A\n",
      "Epoch 2:   2%|▏         | 13/604 [00:05<03:49,  2.58it/s, training loss=0.134]\u001b[A\n",
      "Epoch 2:   2%|▏         | 13/604 [00:05<03:49,  2.58it/s, training loss=0.205]\u001b[A\n",
      "Epoch 2:   2%|▏         | 14/604 [00:05<03:48,  2.58it/s, training loss=0.205]\u001b[A\n",
      "Epoch 2:   2%|▏         | 14/604 [00:05<03:48,  2.58it/s, training loss=0.200]\u001b[A\n",
      "Epoch 2:   2%|▏         | 15/604 [00:05<03:48,  2.58it/s, training loss=0.200]\u001b[A\n",
      "Epoch 2:   2%|▏         | 15/604 [00:06<03:48,  2.58it/s, training loss=0.199]\u001b[A\n",
      "Epoch 2:   3%|▎         | 16/604 [00:06<03:47,  2.58it/s, training loss=0.199]\u001b[A\n",
      "Epoch 2:   3%|▎         | 16/604 [00:06<03:47,  2.58it/s, training loss=0.185]\u001b[A\n",
      "Epoch 2:   3%|▎         | 17/604 [00:06<03:47,  2.59it/s, training loss=0.185]\u001b[A\n",
      "Epoch 2:   3%|▎         | 17/604 [00:06<03:47,  2.59it/s, training loss=0.164]\u001b[A\n",
      "Epoch 2:   3%|▎         | 18/604 [00:07<03:46,  2.59it/s, training loss=0.164]\u001b[A\n",
      "Epoch 2:   3%|▎         | 18/604 [00:07<03:46,  2.59it/s, training loss=0.217]\u001b[A\n",
      "Epoch 2:   3%|▎         | 19/604 [00:07<03:45,  2.59it/s, training loss=0.217]\u001b[A\n",
      "Epoch 2:   3%|▎         | 19/604 [00:07<03:45,  2.59it/s, training loss=0.201]\u001b[A\n",
      "Epoch 2:   3%|▎         | 20/604 [00:07<03:46,  2.58it/s, training loss=0.201]\u001b[A\n",
      "Epoch 2:   3%|▎         | 20/604 [00:08<03:46,  2.58it/s, training loss=0.177]\u001b[A\n",
      "Epoch 2:   3%|▎         | 21/604 [00:08<03:45,  2.58it/s, training loss=0.177]\u001b[A\n",
      "Epoch 2:   3%|▎         | 21/604 [00:08<03:45,  2.58it/s, training loss=0.151]\u001b[A\n",
      "Epoch 2:   4%|▎         | 22/604 [00:08<03:46,  2.56it/s, training loss=0.151]\u001b[A\n",
      "Epoch 2:   4%|▎         | 22/604 [00:08<03:46,  2.56it/s, training loss=0.137]\u001b[A\n",
      "Epoch 2:   4%|▍         | 23/604 [00:08<03:46,  2.57it/s, training loss=0.137]\u001b[A\n",
      "Epoch 2:   4%|▍         | 23/604 [00:09<03:46,  2.57it/s, training loss=0.176]\u001b[A\n",
      "Epoch 2:   4%|▍         | 24/604 [00:09<03:45,  2.57it/s, training loss=0.176]\u001b[A\n",
      "Epoch 2:   4%|▍         | 24/604 [00:09<03:45,  2.57it/s, training loss=0.257]\u001b[A\n",
      "Epoch 2:   4%|▍         | 25/604 [00:09<03:47,  2.54it/s, training loss=0.257]\u001b[A\n",
      "Epoch 2:   4%|▍         | 25/604 [00:10<03:47,  2.54it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:   4%|▍         | 26/604 [00:10<03:49,  2.51it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:   4%|▍         | 26/604 [00:10<03:49,  2.51it/s, training loss=0.221]\u001b[A\n",
      "Epoch 2:   4%|▍         | 27/604 [00:10<03:48,  2.52it/s, training loss=0.221]\u001b[A\n",
      "Epoch 2:   4%|▍         | 27/604 [00:10<03:48,  2.52it/s, training loss=0.225]\u001b[A\n",
      "Epoch 2:   5%|▍         | 28/604 [00:10<03:48,  2.53it/s, training loss=0.225]\u001b[A\n",
      "Epoch 2:   5%|▍         | 28/604 [00:11<03:48,  2.53it/s, training loss=0.234]\u001b[A\n",
      "Epoch 2:   5%|▍         | 29/604 [00:11<03:46,  2.54it/s, training loss=0.234]\u001b[A\n",
      "Epoch 2:   5%|▍         | 29/604 [00:11<03:46,  2.54it/s, training loss=0.236]\u001b[A\n",
      "Epoch 2:   5%|▍         | 30/604 [00:11<03:44,  2.55it/s, training loss=0.236]\u001b[A\n",
      "Epoch 2:   5%|▍         | 30/604 [00:12<03:44,  2.55it/s, training loss=0.165]\u001b[A\n",
      "Epoch 2:   5%|▌         | 31/604 [00:12<03:43,  2.57it/s, training loss=0.165]\u001b[A\n",
      "Epoch 2:   5%|▌         | 31/604 [00:12<03:43,  2.57it/s, training loss=0.132]\u001b[A\n",
      "Epoch 2:   5%|▌         | 32/604 [00:12<03:42,  2.57it/s, training loss=0.132]\u001b[A\n",
      "Epoch 2:   5%|▌         | 32/604 [00:12<03:42,  2.57it/s, training loss=0.187]\u001b[A\n",
      "Epoch 2:   5%|▌         | 33/604 [00:12<03:41,  2.58it/s, training loss=0.187]\u001b[A\n",
      "Epoch 2:   5%|▌         | 33/604 [00:13<03:41,  2.58it/s, training loss=0.185]\u001b[A\n",
      "Epoch 2:   6%|▌         | 34/604 [00:13<03:40,  2.58it/s, training loss=0.185]\u001b[A\n",
      "Epoch 2:   6%|▌         | 34/604 [00:13<03:40,  2.58it/s, training loss=0.217]\u001b[A\n",
      "Epoch 2:   6%|▌         | 35/604 [00:13<03:39,  2.59it/s, training loss=0.217]\u001b[A\n",
      "Epoch 2:   6%|▌         | 35/604 [00:14<03:39,  2.59it/s, training loss=0.164]\u001b[A\n",
      "Epoch 2:   6%|▌         | 36/604 [00:14<03:39,  2.59it/s, training loss=0.164]\u001b[A\n",
      "Epoch 2:   6%|▌         | 36/604 [00:14<03:39,  2.59it/s, training loss=0.181]\u001b[A\n",
      "Epoch 2:   6%|▌         | 37/604 [00:14<03:38,  2.59it/s, training loss=0.181]\u001b[A\n",
      "Epoch 2:   6%|▌         | 37/604 [00:14<03:38,  2.59it/s, training loss=0.172]\u001b[A\n",
      "Epoch 2:   6%|▋         | 38/604 [00:14<03:38,  2.59it/s, training loss=0.172]\u001b[A\n",
      "Epoch 2:   6%|▋         | 38/604 [00:15<03:38,  2.59it/s, training loss=0.232]\u001b[A\n",
      "Epoch 2:   6%|▋         | 39/604 [00:15<03:37,  2.59it/s, training loss=0.232]\u001b[A\n",
      "Epoch 2:   6%|▋         | 39/604 [00:15<03:37,  2.59it/s, training loss=0.200]\u001b[A\n",
      "Epoch 2:   7%|▋         | 40/604 [00:15<03:37,  2.59it/s, training loss=0.200]\u001b[A\n",
      "Epoch 2:   7%|▋         | 40/604 [00:15<03:37,  2.59it/s, training loss=0.143]\u001b[A\n",
      "Epoch 2:   7%|▋         | 41/604 [00:15<03:38,  2.58it/s, training loss=0.143]\u001b[A\n",
      "Epoch 2:   7%|▋         | 41/604 [00:16<03:38,  2.58it/s, training loss=0.175]\u001b[A\n",
      "Epoch 2:   7%|▋         | 42/604 [00:16<03:37,  2.58it/s, training loss=0.175]\u001b[A\n",
      "Epoch 2:   7%|▋         | 42/604 [00:16<03:37,  2.58it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:   7%|▋         | 43/604 [00:16<03:37,  2.58it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:   7%|▋         | 43/604 [00:17<03:37,  2.58it/s, training loss=0.142]\u001b[A\n",
      "Epoch 2:   7%|▋         | 44/604 [00:17<03:36,  2.58it/s, training loss=0.142]\u001b[A\n",
      "Epoch 2:   7%|▋         | 44/604 [00:17<03:36,  2.58it/s, training loss=0.179]\u001b[A\n",
      "Epoch 2:   7%|▋         | 45/604 [00:17<03:36,  2.59it/s, training loss=0.179]\u001b[A\n",
      "Epoch 2:   7%|▋         | 45/604 [00:17<03:36,  2.59it/s, training loss=0.163]\u001b[A\n",
      "Epoch 2:   8%|▊         | 46/604 [00:17<03:35,  2.59it/s, training loss=0.163]\u001b[A\n",
      "Epoch 2:   8%|▊         | 46/604 [00:18<03:35,  2.59it/s, training loss=0.210]\u001b[A\n",
      "Epoch 2:   8%|▊         | 47/604 [00:18<03:34,  2.59it/s, training loss=0.210]\u001b[A\n",
      "Epoch 2:   8%|▊         | 47/604 [00:18<03:34,  2.59it/s, training loss=0.185]\u001b[A\n",
      "Epoch 2:   8%|▊         | 48/604 [00:18<03:34,  2.59it/s, training loss=0.185]\u001b[A\n",
      "Epoch 2:   8%|▊         | 48/604 [00:19<03:34,  2.59it/s, training loss=0.217]\u001b[A\n",
      "Epoch 2:   8%|▊         | 49/604 [00:19<03:37,  2.55it/s, training loss=0.217]\u001b[A\n",
      "Epoch 2:   8%|▊         | 49/604 [00:19<03:37,  2.55it/s, training loss=0.221]\u001b[A\n",
      "Epoch 2:   8%|▊         | 50/604 [00:19<03:36,  2.56it/s, training loss=0.221]\u001b[A\n",
      "Epoch 2:   8%|▊         | 50/604 [00:19<03:36,  2.56it/s, training loss=0.234]\u001b[A\n",
      "Epoch 2:   8%|▊         | 51/604 [00:19<03:35,  2.57it/s, training loss=0.234]\u001b[A\n",
      "Epoch 2:   8%|▊         | 51/604 [00:20<03:35,  2.57it/s, training loss=0.269]\u001b[A\n",
      "Epoch 2:   9%|▊         | 52/604 [00:20<03:34,  2.58it/s, training loss=0.269]\u001b[A\n",
      "Epoch 2:   9%|▊         | 52/604 [00:20<03:34,  2.58it/s, training loss=0.240]\u001b[A\n",
      "Epoch 2:   9%|▉         | 53/604 [00:20<03:33,  2.58it/s, training loss=0.240]\u001b[A\n",
      "Epoch 2:   9%|▉         | 53/604 [00:20<03:33,  2.58it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:   9%|▉         | 54/604 [00:21<03:34,  2.57it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:   9%|▉         | 54/604 [00:21<03:34,  2.57it/s, training loss=0.144]\u001b[A\n",
      "Epoch 2:   9%|▉         | 55/604 [00:21<03:33,  2.58it/s, training loss=0.144]\u001b[A\n",
      "Epoch 2:   9%|▉         | 55/604 [00:21<03:33,  2.58it/s, training loss=0.212]\u001b[A\n",
      "Epoch 2:   9%|▉         | 56/604 [00:21<03:32,  2.58it/s, training loss=0.212]\u001b[A\n",
      "Epoch 2:   9%|▉         | 56/604 [00:22<03:32,  2.58it/s, training loss=0.153]\u001b[A\n",
      "Epoch 2:   9%|▉         | 57/604 [00:22<03:31,  2.59it/s, training loss=0.153]\u001b[A\n",
      "Epoch 2:   9%|▉         | 57/604 [00:22<03:31,  2.59it/s, training loss=0.237]\u001b[A\n",
      "Epoch 2:  10%|▉         | 58/604 [00:22<03:30,  2.59it/s, training loss=0.237]\u001b[A\n",
      "Epoch 2:  10%|▉         | 58/604 [00:22<03:30,  2.59it/s, training loss=0.143]\u001b[A\n",
      "Epoch 2:  10%|▉         | 59/604 [00:22<03:30,  2.59it/s, training loss=0.143]\u001b[A\n",
      "Epoch 2:  10%|▉         | 59/604 [00:23<03:30,  2.59it/s, training loss=0.158]\u001b[A\n",
      "Epoch 2:  10%|▉         | 60/604 [00:23<03:29,  2.60it/s, training loss=0.158]\u001b[A\n",
      "Epoch 2:  10%|▉         | 60/604 [00:23<03:29,  2.60it/s, training loss=0.260]\u001b[A\n",
      "Epoch 2:  10%|█         | 61/604 [00:23<03:29,  2.60it/s, training loss=0.260]\u001b[A\n",
      "Epoch 2:  10%|█         | 61/604 [00:24<03:29,  2.60it/s, training loss=0.177]\u001b[A\n",
      "Epoch 2:  10%|█         | 62/604 [00:24<03:28,  2.60it/s, training loss=0.177]\u001b[A\n",
      "Epoch 2:  10%|█         | 62/604 [00:24<03:28,  2.60it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:  10%|█         | 63/604 [00:24<03:28,  2.60it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:  10%|█         | 63/604 [00:24<03:28,  2.60it/s, training loss=0.276]\u001b[A\n",
      "Epoch 2:  11%|█         | 64/604 [00:24<03:27,  2.60it/s, training loss=0.276]\u001b[A\n",
      "Epoch 2:  11%|█         | 64/604 [00:25<03:27,  2.60it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  11%|█         | 65/604 [00:25<03:27,  2.60it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  11%|█         | 65/604 [00:25<03:27,  2.60it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  11%|█         | 66/604 [00:25<03:26,  2.60it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  11%|█         | 66/604 [00:26<03:26,  2.60it/s, training loss=0.264]\u001b[A\n",
      "Epoch 2:  11%|█         | 67/604 [00:26<03:26,  2.60it/s, training loss=0.264]\u001b[A\n",
      "Epoch 2:  11%|█         | 67/604 [00:26<03:26,  2.60it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  11%|█▏        | 68/604 [00:26<03:26,  2.60it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  11%|█▏        | 68/604 [00:26<03:26,  2.60it/s, training loss=0.175]\u001b[A\n",
      "Epoch 2:  11%|█▏        | 69/604 [00:26<03:25,  2.60it/s, training loss=0.175]\u001b[A\n",
      "Epoch 2:  11%|█▏        | 69/604 [00:27<03:25,  2.60it/s, training loss=0.219]\u001b[A\n",
      "Epoch 2:  12%|█▏        | 70/604 [00:27<03:25,  2.60it/s, training loss=0.219]\u001b[A\n",
      "Epoch 2:  12%|█▏        | 70/604 [00:27<03:25,  2.60it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  12%|█▏        | 71/604 [00:27<03:24,  2.60it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  12%|█▏        | 71/604 [00:27<03:24,  2.60it/s, training loss=0.158]\u001b[A\n",
      "Epoch 2:  12%|█▏        | 72/604 [00:27<03:24,  2.60it/s, training loss=0.158]\u001b[A\n",
      "Epoch 2:  12%|█▏        | 72/604 [00:28<03:24,  2.60it/s, training loss=0.137]\u001b[A\n",
      "Epoch 2:  12%|█▏        | 73/604 [00:28<03:24,  2.60it/s, training loss=0.137]\u001b[A\n",
      "Epoch 2:  12%|█▏        | 73/604 [00:28<03:24,  2.60it/s, training loss=0.207]\u001b[A\n",
      "Epoch 2:  12%|█▏        | 74/604 [00:28<03:23,  2.60it/s, training loss=0.207]\u001b[A\n",
      "Epoch 2:  12%|█▏        | 74/604 [00:29<03:23,  2.60it/s, training loss=0.203]\u001b[A\n",
      "Epoch 2:  12%|█▏        | 75/604 [00:29<03:23,  2.60it/s, training loss=0.203]\u001b[A\n",
      "Epoch 2:  12%|█▏        | 75/604 [00:29<03:23,  2.60it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  13%|█▎        | 76/604 [00:29<03:23,  2.60it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  13%|█▎        | 76/604 [00:29<03:23,  2.60it/s, training loss=0.213]\u001b[A\n",
      "Epoch 2:  13%|█▎        | 77/604 [00:29<03:23,  2.59it/s, training loss=0.213]\u001b[A\n",
      "Epoch 2:  13%|█▎        | 77/604 [00:30<03:23,  2.59it/s, training loss=0.183]\u001b[A\n",
      "Epoch 2:  13%|█▎        | 78/604 [00:30<03:22,  2.59it/s, training loss=0.183]\u001b[A\n",
      "Epoch 2:  13%|█▎        | 78/604 [00:30<03:22,  2.59it/s, training loss=0.216]\u001b[A\n",
      "Epoch 2:  13%|█▎        | 79/604 [00:30<03:22,  2.59it/s, training loss=0.216]\u001b[A\n",
      "Epoch 2:  13%|█▎        | 79/604 [00:31<03:22,  2.59it/s, training loss=0.139]\u001b[A\n",
      "Epoch 2:  13%|█▎        | 80/604 [00:31<03:22,  2.59it/s, training loss=0.139]\u001b[A\n",
      "Epoch 2:  13%|█▎        | 80/604 [00:31<03:22,  2.59it/s, training loss=0.254]\u001b[A\n",
      "Epoch 2:  13%|█▎        | 81/604 [00:31<03:21,  2.59it/s, training loss=0.254]\u001b[A\n",
      "Epoch 2:  13%|█▎        | 81/604 [00:31<03:21,  2.59it/s, training loss=0.248]\u001b[A\n",
      "Epoch 2:  14%|█▎        | 82/604 [00:31<03:23,  2.56it/s, training loss=0.248]\u001b[A\n",
      "Epoch 2:  14%|█▎        | 82/604 [00:32<03:23,  2.56it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  14%|█▎        | 83/604 [00:32<03:23,  2.56it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  14%|█▎        | 83/604 [00:32<03:23,  2.56it/s, training loss=0.158]\u001b[A\n",
      "Epoch 2:  14%|█▍        | 84/604 [00:32<03:22,  2.57it/s, training loss=0.158]\u001b[A\n",
      "Epoch 2:  14%|█▍        | 84/604 [00:32<03:22,  2.57it/s, training loss=0.176]\u001b[A\n",
      "Epoch 2:  14%|█▍        | 85/604 [00:32<03:21,  2.58it/s, training loss=0.176]\u001b[A\n",
      "Epoch 2:  14%|█▍        | 85/604 [00:33<03:21,  2.58it/s, training loss=0.256]\u001b[A\n",
      "Epoch 2:  14%|█▍        | 86/604 [00:33<03:21,  2.57it/s, training loss=0.256]\u001b[A\n",
      "Epoch 2:  14%|█▍        | 86/604 [00:33<03:21,  2.57it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  14%|█▍        | 87/604 [00:33<03:20,  2.58it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  14%|█▍        | 87/604 [00:34<03:20,  2.58it/s, training loss=0.136]\u001b[A\n",
      "Epoch 2:  15%|█▍        | 88/604 [00:34<03:19,  2.58it/s, training loss=0.136]\u001b[A\n",
      "Epoch 2:  15%|█▍        | 88/604 [00:34<03:19,  2.58it/s, training loss=0.256]\u001b[A\n",
      "Epoch 2:  15%|█▍        | 89/604 [00:34<03:18,  2.59it/s, training loss=0.256]\u001b[A\n",
      "Epoch 2:  15%|█▍        | 89/604 [00:34<03:18,  2.59it/s, training loss=0.173]\u001b[A\n",
      "Epoch 2:  15%|█▍        | 90/604 [00:34<03:18,  2.59it/s, training loss=0.173]\u001b[A\n",
      "Epoch 2:  15%|█▍        | 90/604 [00:35<03:18,  2.59it/s, training loss=0.249]\u001b[A\n",
      "Epoch 2:  15%|█▌        | 91/604 [00:35<03:17,  2.60it/s, training loss=0.249]\u001b[A\n",
      "Epoch 2:  15%|█▌        | 91/604 [00:35<03:17,  2.60it/s, training loss=0.255]\u001b[A\n",
      "Epoch 2:  15%|█▌        | 92/604 [00:35<03:20,  2.56it/s, training loss=0.255]\u001b[A\n",
      "Epoch 2:  15%|█▌        | 92/604 [00:36<03:20,  2.56it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  15%|█▌        | 93/604 [00:36<03:22,  2.52it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  15%|█▌        | 93/604 [00:36<03:22,  2.52it/s, training loss=0.129]\u001b[A\n",
      "Epoch 2:  16%|█▌        | 94/604 [00:36<03:21,  2.53it/s, training loss=0.129]\u001b[A\n",
      "Epoch 2:  16%|█▌        | 94/604 [00:36<03:21,  2.53it/s, training loss=0.156]\u001b[A\n",
      "Epoch 2:  16%|█▌        | 95/604 [00:36<03:19,  2.55it/s, training loss=0.156]\u001b[A\n",
      "Epoch 2:  16%|█▌        | 95/604 [00:37<03:19,  2.55it/s, training loss=0.175]\u001b[A\n",
      "Epoch 2:  16%|█▌        | 96/604 [00:37<03:22,  2.51it/s, training loss=0.175]\u001b[A\n",
      "Epoch 2:  16%|█▌        | 96/604 [00:37<03:22,  2.51it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  16%|█▌        | 97/604 [00:37<03:21,  2.52it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  16%|█▌        | 97/604 [00:38<03:21,  2.52it/s, training loss=0.103]\u001b[A\n",
      "Epoch 2:  16%|█▌        | 98/604 [00:38<03:19,  2.54it/s, training loss=0.103]\u001b[A\n",
      "Epoch 2:  16%|█▌        | 98/604 [00:38<03:19,  2.54it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  16%|█▋        | 99/604 [00:38<03:17,  2.56it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  16%|█▋        | 99/604 [00:38<03:17,  2.56it/s, training loss=0.199]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  17%|█▋        | 100/604 [00:38<03:16,  2.57it/s, training loss=0.199]\u001b[A\n",
      "Epoch 2:  17%|█▋        | 100/604 [00:39<03:16,  2.57it/s, training loss=0.136]\u001b[A\n",
      "Epoch 2:  17%|█▋        | 101/604 [00:39<03:15,  2.58it/s, training loss=0.136]\u001b[A\n",
      "Epoch 2:  17%|█▋        | 101/604 [00:39<03:15,  2.58it/s, training loss=0.193]\u001b[A\n",
      "Epoch 2:  17%|█▋        | 102/604 [00:39<03:14,  2.58it/s, training loss=0.193]\u001b[A\n",
      "Epoch 2:  17%|█▋        | 102/604 [00:39<03:14,  2.58it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  17%|█▋        | 103/604 [00:39<03:13,  2.59it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  17%|█▋        | 103/604 [00:40<03:13,  2.59it/s, training loss=0.219]\u001b[A\n",
      "Epoch 2:  17%|█▋        | 104/604 [00:40<03:12,  2.59it/s, training loss=0.219]\u001b[A\n",
      "Epoch 2:  17%|█▋        | 104/604 [00:40<03:12,  2.59it/s, training loss=0.201]\u001b[A\n",
      "Epoch 2:  17%|█▋        | 105/604 [00:40<03:12,  2.59it/s, training loss=0.201]\u001b[A\n",
      "Epoch 2:  17%|█▋        | 105/604 [00:41<03:12,  2.59it/s, training loss=0.279]\u001b[A\n",
      "Epoch 2:  18%|█▊        | 106/604 [00:41<03:12,  2.59it/s, training loss=0.279]\u001b[A\n",
      "Epoch 2:  18%|█▊        | 106/604 [00:41<03:12,  2.59it/s, training loss=0.172]\u001b[A\n",
      "Epoch 2:  18%|█▊        | 107/604 [00:41<03:12,  2.58it/s, training loss=0.172]\u001b[A\n",
      "Epoch 2:  18%|█▊        | 107/604 [00:41<03:12,  2.58it/s, training loss=0.155]\u001b[A\n",
      "Epoch 2:  18%|█▊        | 108/604 [00:41<03:12,  2.58it/s, training loss=0.155]\u001b[A\n",
      "Epoch 2:  18%|█▊        | 108/604 [00:42<03:12,  2.58it/s, training loss=0.168]\u001b[A\n",
      "Epoch 2:  18%|█▊        | 109/604 [00:42<03:11,  2.58it/s, training loss=0.168]\u001b[A\n",
      "Epoch 2:  18%|█▊        | 109/604 [00:42<03:11,  2.58it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2:  18%|█▊        | 110/604 [00:42<03:16,  2.51it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2:  18%|█▊        | 110/604 [00:43<03:16,  2.51it/s, training loss=0.181]\u001b[A\n",
      "Epoch 2:  18%|█▊        | 111/604 [00:43<03:19,  2.47it/s, training loss=0.181]\u001b[A\n",
      "Epoch 2:  18%|█▊        | 111/604 [00:43<03:19,  2.47it/s, training loss=0.237]\u001b[A\n",
      "Epoch 2:  19%|█▊        | 112/604 [00:43<03:17,  2.50it/s, training loss=0.237]\u001b[A\n",
      "Epoch 2:  19%|█▊        | 112/604 [00:43<03:17,  2.50it/s, training loss=0.232]\u001b[A\n",
      "Epoch 2:  19%|█▊        | 113/604 [00:43<03:18,  2.47it/s, training loss=0.232]\u001b[A\n",
      "Epoch 2:  19%|█▊        | 113/604 [00:44<03:18,  2.47it/s, training loss=0.214]\u001b[A\n",
      "Epoch 2:  19%|█▉        | 114/604 [00:44<03:15,  2.50it/s, training loss=0.214]\u001b[A\n",
      "Epoch 2:  19%|█▉        | 114/604 [00:44<03:15,  2.50it/s, training loss=0.115]\u001b[A\n",
      "Epoch 2:  19%|█▉        | 115/604 [00:44<03:15,  2.50it/s, training loss=0.115]\u001b[A\n",
      "Epoch 2:  19%|█▉        | 115/604 [00:45<03:15,  2.50it/s, training loss=0.177]\u001b[A\n",
      "Epoch 2:  19%|█▉        | 116/604 [00:45<03:13,  2.52it/s, training loss=0.177]\u001b[A\n",
      "Epoch 2:  19%|█▉        | 116/604 [00:45<03:13,  2.52it/s, training loss=0.179]\u001b[A\n",
      "Epoch 2:  19%|█▉        | 117/604 [00:45<03:11,  2.54it/s, training loss=0.179]\u001b[A\n",
      "Epoch 2:  19%|█▉        | 117/604 [00:45<03:11,  2.54it/s, training loss=0.152]\u001b[A\n",
      "Epoch 2:  20%|█▉        | 118/604 [00:45<03:09,  2.56it/s, training loss=0.152]\u001b[A\n",
      "Epoch 2:  20%|█▉        | 118/604 [00:46<03:09,  2.56it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  20%|█▉        | 119/604 [00:46<03:08,  2.57it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  20%|█▉        | 119/604 [00:46<03:08,  2.57it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  20%|█▉        | 120/604 [00:46<03:07,  2.58it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  20%|█▉        | 120/604 [00:47<03:07,  2.58it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  20%|██        | 121/604 [00:47<03:07,  2.58it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  20%|██        | 121/604 [00:47<03:07,  2.58it/s, training loss=0.236]\u001b[A\n",
      "Epoch 2:  20%|██        | 122/604 [00:47<03:06,  2.59it/s, training loss=0.236]\u001b[A\n",
      "Epoch 2:  20%|██        | 122/604 [00:47<03:06,  2.59it/s, training loss=0.176]\u001b[A\n",
      "Epoch 2:  20%|██        | 123/604 [00:47<03:05,  2.59it/s, training loss=0.176]\u001b[A\n",
      "Epoch 2:  20%|██        | 123/604 [00:48<03:05,  2.59it/s, training loss=0.213]\u001b[A\n",
      "Epoch 2:  21%|██        | 124/604 [00:48<03:05,  2.59it/s, training loss=0.213]\u001b[A\n",
      "Epoch 2:  21%|██        | 124/604 [00:48<03:05,  2.59it/s, training loss=0.246]\u001b[A\n",
      "Epoch 2:  21%|██        | 125/604 [00:48<03:06,  2.57it/s, training loss=0.246]\u001b[A\n",
      "Epoch 2:  21%|██        | 125/604 [00:49<03:06,  2.57it/s, training loss=0.252]\u001b[A\n",
      "Epoch 2:  21%|██        | 126/604 [00:49<03:06,  2.56it/s, training loss=0.252]\u001b[A\n",
      "Epoch 2:  21%|██        | 126/604 [00:49<03:06,  2.56it/s, training loss=0.145]\u001b[A\n",
      "Epoch 2:  21%|██        | 127/604 [00:49<03:05,  2.57it/s, training loss=0.145]\u001b[A\n",
      "Epoch 2:  21%|██        | 127/604 [00:49<03:05,  2.57it/s, training loss=0.169]\u001b[A\n",
      "Epoch 2:  21%|██        | 128/604 [00:49<03:04,  2.58it/s, training loss=0.169]\u001b[A\n",
      "Epoch 2:  21%|██        | 128/604 [00:50<03:04,  2.58it/s, training loss=0.106]\u001b[A\n",
      "Epoch 2:  21%|██▏       | 129/604 [00:50<03:07,  2.53it/s, training loss=0.106]\u001b[A\n",
      "Epoch 2:  21%|██▏       | 129/604 [00:50<03:07,  2.53it/s, training loss=0.198]\u001b[A\n",
      "Epoch 2:  22%|██▏       | 130/604 [00:50<03:07,  2.53it/s, training loss=0.198]\u001b[A\n",
      "Epoch 2:  22%|██▏       | 130/604 [00:50<03:07,  2.53it/s, training loss=0.179]\u001b[A\n",
      "Epoch 2:  22%|██▏       | 131/604 [00:50<03:07,  2.52it/s, training loss=0.179]\u001b[A\n",
      "Epoch 2:  22%|██▏       | 131/604 [00:51<03:07,  2.52it/s, training loss=0.156]\u001b[A\n",
      "Epoch 2:  22%|██▏       | 132/604 [00:51<03:05,  2.54it/s, training loss=0.156]\u001b[A\n",
      "Epoch 2:  22%|██▏       | 132/604 [00:51<03:05,  2.54it/s, training loss=0.185]\u001b[A\n",
      "Epoch 2:  22%|██▏       | 133/604 [00:51<03:04,  2.56it/s, training loss=0.185]\u001b[A\n",
      "Epoch 2:  22%|██▏       | 133/604 [00:52<03:04,  2.56it/s, training loss=0.213]\u001b[A\n",
      "Epoch 2:  22%|██▏       | 134/604 [00:52<03:03,  2.56it/s, training loss=0.213]\u001b[A\n",
      "Epoch 2:  22%|██▏       | 134/604 [00:52<03:03,  2.56it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  22%|██▏       | 135/604 [00:52<03:02,  2.57it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  22%|██▏       | 135/604 [00:52<03:02,  2.57it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  23%|██▎       | 136/604 [00:52<03:02,  2.57it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  23%|██▎       | 136/604 [00:53<03:02,  2.57it/s, training loss=0.232]\u001b[A\n",
      "Epoch 2:  23%|██▎       | 137/604 [00:53<03:01,  2.58it/s, training loss=0.232]\u001b[A\n",
      "Epoch 2:  23%|██▎       | 137/604 [00:53<03:01,  2.58it/s, training loss=0.229]\u001b[A\n",
      "Epoch 2:  23%|██▎       | 138/604 [00:53<03:00,  2.59it/s, training loss=0.229]\u001b[A\n",
      "Epoch 2:  23%|██▎       | 138/604 [00:54<03:00,  2.59it/s, training loss=0.282]\u001b[A\n",
      "Epoch 2:  23%|██▎       | 139/604 [00:54<03:02,  2.55it/s, training loss=0.282]\u001b[A\n",
      "Epoch 2:  23%|██▎       | 139/604 [00:54<03:02,  2.55it/s, training loss=0.153]\u001b[A\n",
      "Epoch 2:  23%|██▎       | 140/604 [00:54<03:01,  2.56it/s, training loss=0.153]\u001b[A\n",
      "Epoch 2:  23%|██▎       | 140/604 [00:54<03:01,  2.56it/s, training loss=0.224]\u001b[A\n",
      "Epoch 2:  23%|██▎       | 141/604 [00:54<03:00,  2.56it/s, training loss=0.224]\u001b[A\n",
      "Epoch 2:  23%|██▎       | 141/604 [00:55<03:00,  2.56it/s, training loss=0.272]\u001b[A\n",
      "Epoch 2:  24%|██▎       | 142/604 [00:55<02:59,  2.57it/s, training loss=0.272]\u001b[A\n",
      "Epoch 2:  24%|██▎       | 142/604 [00:55<02:59,  2.57it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  24%|██▎       | 143/604 [00:55<02:58,  2.58it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  24%|██▎       | 143/604 [00:56<02:58,  2.58it/s, training loss=0.240]\u001b[A\n",
      "Epoch 2:  24%|██▍       | 144/604 [00:56<02:57,  2.59it/s, training loss=0.240]\u001b[A\n",
      "Epoch 2:  24%|██▍       | 144/604 [00:56<02:57,  2.59it/s, training loss=0.162]\u001b[A\n",
      "Epoch 2:  24%|██▍       | 145/604 [00:56<02:57,  2.58it/s, training loss=0.162]\u001b[A\n",
      "Epoch 2:  24%|██▍       | 145/604 [00:56<02:57,  2.58it/s, training loss=0.188]\u001b[A\n",
      "Epoch 2:  24%|██▍       | 146/604 [00:56<02:56,  2.59it/s, training loss=0.188]\u001b[A\n",
      "Epoch 2:  24%|██▍       | 146/604 [00:57<02:56,  2.59it/s, training loss=0.218]\u001b[A\n",
      "Epoch 2:  24%|██▍       | 147/604 [00:57<02:56,  2.59it/s, training loss=0.218]\u001b[A\n",
      "Epoch 2:  24%|██▍       | 147/604 [00:57<02:56,  2.59it/s, training loss=0.203]\u001b[A\n",
      "Epoch 2:  25%|██▍       | 148/604 [00:57<02:55,  2.59it/s, training loss=0.203]\u001b[A\n",
      "Epoch 2:  25%|██▍       | 148/604 [00:57<02:55,  2.59it/s, training loss=0.131]\u001b[A\n",
      "Epoch 2:  25%|██▍       | 149/604 [00:57<02:55,  2.60it/s, training loss=0.131]\u001b[A\n",
      "Epoch 2:  25%|██▍       | 149/604 [00:58<02:55,  2.60it/s, training loss=0.240]\u001b[A\n",
      "Epoch 2:  25%|██▍       | 150/604 [00:58<02:54,  2.60it/s, training loss=0.240]\u001b[A\n",
      "Epoch 2:  25%|██▍       | 150/604 [00:58<02:54,  2.60it/s, training loss=0.225]\u001b[A\n",
      "Epoch 2:  25%|██▌       | 151/604 [00:58<02:54,  2.60it/s, training loss=0.225]\u001b[A\n",
      "Epoch 2:  25%|██▌       | 151/604 [00:59<02:54,  2.60it/s, training loss=0.148]\u001b[A\n",
      "Epoch 2:  25%|██▌       | 152/604 [00:59<02:54,  2.58it/s, training loss=0.148]\u001b[A\n",
      "Epoch 2:  25%|██▌       | 152/604 [00:59<02:54,  2.58it/s, training loss=0.202]\u001b[A\n",
      "Epoch 2:  25%|██▌       | 153/604 [00:59<02:54,  2.59it/s, training loss=0.202]\u001b[A\n",
      "Epoch 2:  25%|██▌       | 153/604 [00:59<02:54,  2.59it/s, training loss=0.128]\u001b[A\n",
      "Epoch 2:  25%|██▌       | 154/604 [00:59<02:53,  2.59it/s, training loss=0.128]\u001b[A\n",
      "Epoch 2:  25%|██▌       | 154/604 [01:00<02:53,  2.59it/s, training loss=0.251]\u001b[A\n",
      "Epoch 2:  26%|██▌       | 155/604 [01:00<02:53,  2.59it/s, training loss=0.251]\u001b[A\n",
      "Epoch 2:  26%|██▌       | 155/604 [01:00<02:53,  2.59it/s, training loss=0.206]\u001b[A\n",
      "Epoch 2:  26%|██▌       | 156/604 [01:00<02:52,  2.59it/s, training loss=0.206]\u001b[A\n",
      "Epoch 2:  26%|██▌       | 156/604 [01:01<02:52,  2.59it/s, training loss=0.221]\u001b[A\n",
      "Epoch 2:  26%|██▌       | 157/604 [01:01<02:52,  2.60it/s, training loss=0.221]\u001b[A\n",
      "Epoch 2:  26%|██▌       | 157/604 [01:01<02:52,  2.60it/s, training loss=0.214]\u001b[A\n",
      "Epoch 2:  26%|██▌       | 158/604 [01:01<02:51,  2.60it/s, training loss=0.214]\u001b[A\n",
      "Epoch 2:  26%|██▌       | 158/604 [01:01<02:51,  2.60it/s, training loss=0.216]\u001b[A\n",
      "Epoch 2:  26%|██▋       | 159/604 [01:01<02:51,  2.60it/s, training loss=0.216]\u001b[A\n",
      "Epoch 2:  26%|██▋       | 159/604 [01:02<02:51,  2.60it/s, training loss=0.231]\u001b[A\n",
      "Epoch 2:  26%|██▋       | 160/604 [01:02<02:51,  2.59it/s, training loss=0.231]\u001b[A\n",
      "Epoch 2:  26%|██▋       | 160/604 [01:02<02:51,  2.59it/s, training loss=0.211]\u001b[A\n",
      "Epoch 2:  27%|██▋       | 161/604 [01:02<02:50,  2.59it/s, training loss=0.211]\u001b[A\n",
      "Epoch 2:  27%|██▋       | 161/604 [01:02<02:50,  2.59it/s, training loss=0.123]\u001b[A\n",
      "Epoch 2:  27%|██▋       | 162/604 [01:02<02:51,  2.58it/s, training loss=0.123]\u001b[A\n",
      "Epoch 2:  27%|██▋       | 162/604 [01:03<02:51,  2.58it/s, training loss=0.202]\u001b[A\n",
      "Epoch 2:  27%|██▋       | 163/604 [01:03<02:50,  2.58it/s, training loss=0.202]\u001b[A\n",
      "Epoch 2:  27%|██▋       | 163/604 [01:03<02:50,  2.58it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  27%|██▋       | 164/604 [01:03<02:49,  2.59it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  27%|██▋       | 164/604 [01:04<02:49,  2.59it/s, training loss=0.201]\u001b[A\n",
      "Epoch 2:  27%|██▋       | 165/604 [01:04<02:49,  2.59it/s, training loss=0.201]\u001b[A\n",
      "Epoch 2:  27%|██▋       | 165/604 [01:04<02:49,  2.59it/s, training loss=0.174]\u001b[A\n",
      "Epoch 2:  27%|██▋       | 166/604 [01:04<02:49,  2.59it/s, training loss=0.174]\u001b[A\n",
      "Epoch 2:  27%|██▋       | 166/604 [01:04<02:49,  2.59it/s, training loss=0.115]\u001b[A\n",
      "Epoch 2:  28%|██▊       | 167/604 [01:04<02:50,  2.57it/s, training loss=0.115]\u001b[A\n",
      "Epoch 2:  28%|██▊       | 167/604 [01:05<02:50,  2.57it/s, training loss=0.181]\u001b[A\n",
      "Epoch 2:  28%|██▊       | 168/604 [01:05<02:49,  2.57it/s, training loss=0.181]\u001b[A\n",
      "Epoch 2:  28%|██▊       | 168/604 [01:05<02:49,  2.57it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  28%|██▊       | 169/604 [01:05<02:48,  2.58it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  28%|██▊       | 169/604 [01:06<02:48,  2.58it/s, training loss=0.170]\u001b[A\n",
      "Epoch 2:  28%|██▊       | 170/604 [01:06<02:47,  2.58it/s, training loss=0.170]\u001b[A\n",
      "Epoch 2:  28%|██▊       | 170/604 [01:06<02:47,  2.58it/s, training loss=0.176]\u001b[A\n",
      "Epoch 2:  28%|██▊       | 171/604 [01:06<02:47,  2.59it/s, training loss=0.176]\u001b[A\n",
      "Epoch 2:  28%|██▊       | 171/604 [01:06<02:47,  2.59it/s, training loss=0.198]\u001b[A\n",
      "Epoch 2:  28%|██▊       | 172/604 [01:06<02:46,  2.59it/s, training loss=0.198]\u001b[A\n",
      "Epoch 2:  28%|██▊       | 172/604 [01:07<02:46,  2.59it/s, training loss=0.147]\u001b[A\n",
      "Epoch 2:  29%|██▊       | 173/604 [01:07<02:46,  2.59it/s, training loss=0.147]\u001b[A\n",
      "Epoch 2:  29%|██▊       | 173/604 [01:07<02:46,  2.59it/s, training loss=0.186]\u001b[A\n",
      "Epoch 2:  29%|██▉       | 174/604 [01:07<02:45,  2.59it/s, training loss=0.186]\u001b[A\n",
      "Epoch 2:  29%|██▉       | 174/604 [01:07<02:45,  2.59it/s, training loss=0.126]\u001b[A\n",
      "Epoch 2:  29%|██▉       | 175/604 [01:07<02:45,  2.60it/s, training loss=0.126]\u001b[A\n",
      "Epoch 2:  29%|██▉       | 175/604 [01:08<02:45,  2.60it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  29%|██▉       | 176/604 [01:08<02:44,  2.60it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  29%|██▉       | 176/604 [01:08<02:44,  2.60it/s, training loss=0.155]\u001b[A\n",
      "Epoch 2:  29%|██▉       | 177/604 [01:08<02:44,  2.60it/s, training loss=0.155]\u001b[A\n",
      "Epoch 2:  29%|██▉       | 177/604 [01:09<02:44,  2.60it/s, training loss=0.303]\u001b[A\n",
      "Epoch 2:  29%|██▉       | 178/604 [01:09<02:44,  2.59it/s, training loss=0.303]\u001b[A\n",
      "Epoch 2:  29%|██▉       | 178/604 [01:09<02:44,  2.59it/s, training loss=0.245]\u001b[A\n",
      "Epoch 2:  30%|██▉       | 179/604 [01:09<02:44,  2.59it/s, training loss=0.245]\u001b[A\n",
      "Epoch 2:  30%|██▉       | 179/604 [01:09<02:44,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 2:  30%|██▉       | 180/604 [01:09<02:43,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 2:  30%|██▉       | 180/604 [01:10<02:43,  2.59it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  30%|██▉       | 181/604 [01:10<02:42,  2.60it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  30%|██▉       | 181/604 [01:10<02:42,  2.60it/s, training loss=0.227]\u001b[A\n",
      "Epoch 2:  30%|███       | 182/604 [01:10<02:42,  2.60it/s, training loss=0.227]\u001b[A\n",
      "Epoch 2:  30%|███       | 182/604 [01:11<02:42,  2.60it/s, training loss=0.163]\u001b[A\n",
      "Epoch 2:  30%|███       | 183/604 [01:11<02:42,  2.60it/s, training loss=0.163]\u001b[A\n",
      "Epoch 2:  30%|███       | 183/604 [01:11<02:42,  2.60it/s, training loss=0.215]\u001b[A\n",
      "Epoch 2:  30%|███       | 184/604 [01:11<02:41,  2.60it/s, training loss=0.215]\u001b[A\n",
      "Epoch 2:  30%|███       | 184/604 [01:11<02:41,  2.60it/s, training loss=0.240]\u001b[A\n",
      "Epoch 2:  31%|███       | 185/604 [01:11<02:41,  2.60it/s, training loss=0.240]\u001b[A\n",
      "Epoch 2:  31%|███       | 185/604 [01:12<02:41,  2.60it/s, training loss=0.188]\u001b[A\n",
      "Epoch 2:  31%|███       | 186/604 [01:12<02:40,  2.60it/s, training loss=0.188]\u001b[A\n",
      "Epoch 2:  31%|███       | 186/604 [01:12<02:40,  2.60it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2:  31%|███       | 187/604 [01:12<02:40,  2.59it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2:  31%|███       | 187/604 [01:13<02:40,  2.59it/s, training loss=0.246]\u001b[A\n",
      "Epoch 2:  31%|███       | 188/604 [01:13<02:42,  2.57it/s, training loss=0.246]\u001b[A\n",
      "Epoch 2:  31%|███       | 188/604 [01:13<02:42,  2.57it/s, training loss=0.233]\u001b[A\n",
      "Epoch 2:  31%|███▏      | 189/604 [01:13<02:41,  2.57it/s, training loss=0.233]\u001b[A\n",
      "Epoch 2:  31%|███▏      | 189/604 [01:13<02:41,  2.57it/s, training loss=0.214]\u001b[A\n",
      "Epoch 2:  31%|███▏      | 190/604 [01:13<02:40,  2.58it/s, training loss=0.214]\u001b[A\n",
      "Epoch 2:  31%|███▏      | 190/604 [01:14<02:40,  2.58it/s, training loss=0.178]\u001b[A\n",
      "Epoch 2:  32%|███▏      | 191/604 [01:14<02:39,  2.59it/s, training loss=0.178]\u001b[A\n",
      "Epoch 2:  32%|███▏      | 191/604 [01:14<02:39,  2.59it/s, training loss=0.146]\u001b[A\n",
      "Epoch 2:  32%|███▏      | 192/604 [01:14<02:39,  2.59it/s, training loss=0.146]\u001b[A\n",
      "Epoch 2:  32%|███▏      | 192/604 [01:14<02:39,  2.59it/s, training loss=0.107]\u001b[A\n",
      "Epoch 2:  32%|███▏      | 193/604 [01:14<02:38,  2.59it/s, training loss=0.107]\u001b[A\n",
      "Epoch 2:  32%|███▏      | 193/604 [01:15<02:38,  2.59it/s, training loss=0.246]\u001b[A\n",
      "Epoch 2:  32%|███▏      | 194/604 [01:15<02:38,  2.59it/s, training loss=0.246]\u001b[A\n",
      "Epoch 2:  32%|███▏      | 194/604 [01:15<02:38,  2.59it/s, training loss=0.181]\u001b[A\n",
      "Epoch 2:  32%|███▏      | 195/604 [01:15<02:38,  2.58it/s, training loss=0.181]\u001b[A\n",
      "Epoch 2:  32%|███▏      | 195/604 [01:16<02:38,  2.58it/s, training loss=0.221]\u001b[A\n",
      "Epoch 2:  32%|███▏      | 196/604 [01:16<02:39,  2.56it/s, training loss=0.221]\u001b[A\n",
      "Epoch 2:  32%|███▏      | 196/604 [01:16<02:39,  2.56it/s, training loss=0.127]\u001b[A\n",
      "Epoch 2:  33%|███▎      | 197/604 [01:16<02:38,  2.57it/s, training loss=0.127]\u001b[A\n",
      "Epoch 2:  33%|███▎      | 197/604 [01:16<02:38,  2.57it/s, training loss=0.158]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  33%|███▎      | 198/604 [01:16<02:37,  2.57it/s, training loss=0.158]\u001b[A\n",
      "Epoch 2:  33%|███▎      | 198/604 [01:17<02:37,  2.57it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  33%|███▎      | 199/604 [01:17<02:37,  2.58it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  33%|███▎      | 199/604 [01:17<02:37,  2.58it/s, training loss=0.151]\u001b[A\n",
      "Epoch 2:  33%|███▎      | 200/604 [01:17<02:36,  2.58it/s, training loss=0.151]\u001b[A\n",
      "Epoch 2:  33%|███▎      | 200/604 [01:18<02:36,  2.58it/s, training loss=0.183]\u001b[A\n",
      "Epoch 2:  33%|███▎      | 201/604 [01:18<02:35,  2.59it/s, training loss=0.183]\u001b[A\n",
      "Epoch 2:  33%|███▎      | 201/604 [01:18<02:35,  2.59it/s, training loss=0.203]\u001b[A\n",
      "Epoch 2:  33%|███▎      | 202/604 [01:18<02:35,  2.59it/s, training loss=0.203]\u001b[A\n",
      "Epoch 2:  33%|███▎      | 202/604 [01:18<02:35,  2.59it/s, training loss=0.205]\u001b[A\n",
      "Epoch 2:  34%|███▎      | 203/604 [01:18<02:34,  2.59it/s, training loss=0.205]\u001b[A\n",
      "Epoch 2:  34%|███▎      | 203/604 [01:19<02:34,  2.59it/s, training loss=0.168]\u001b[A\n",
      "Epoch 2:  34%|███▍      | 204/604 [01:19<02:34,  2.59it/s, training loss=0.168]\u001b[A\n",
      "Epoch 2:  34%|███▍      | 204/604 [01:19<02:34,  2.59it/s, training loss=0.274]\u001b[A\n",
      "Epoch 2:  34%|███▍      | 205/604 [01:19<02:33,  2.59it/s, training loss=0.274]\u001b[A\n",
      "Epoch 2:  34%|███▍      | 205/604 [01:19<02:33,  2.59it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  34%|███▍      | 206/604 [01:19<02:33,  2.60it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  34%|███▍      | 206/604 [01:20<02:33,  2.60it/s, training loss=0.205]\u001b[A\n",
      "Epoch 2:  34%|███▍      | 207/604 [01:20<02:32,  2.60it/s, training loss=0.205]\u001b[A\n",
      "Epoch 2:  34%|███▍      | 207/604 [01:20<02:32,  2.60it/s, training loss=0.121]\u001b[A\n",
      "Epoch 2:  34%|███▍      | 208/604 [01:20<02:32,  2.60it/s, training loss=0.121]\u001b[A\n",
      "Epoch 2:  34%|███▍      | 208/604 [01:21<02:32,  2.60it/s, training loss=0.172]\u001b[A\n",
      "Epoch 2:  35%|███▍      | 209/604 [01:21<02:32,  2.59it/s, training loss=0.172]\u001b[A\n",
      "Epoch 2:  35%|███▍      | 209/604 [01:21<02:32,  2.59it/s, training loss=0.250]\u001b[A\n",
      "Epoch 2:  35%|███▍      | 210/604 [01:21<02:32,  2.58it/s, training loss=0.250]\u001b[A\n",
      "Epoch 2:  35%|███▍      | 210/604 [01:21<02:32,  2.58it/s, training loss=0.174]\u001b[A\n",
      "Epoch 2:  35%|███▍      | 211/604 [01:21<02:34,  2.55it/s, training loss=0.174]\u001b[A\n",
      "Epoch 2:  35%|███▍      | 211/604 [01:22<02:34,  2.55it/s, training loss=0.164]\u001b[A\n",
      "Epoch 2:  35%|███▌      | 212/604 [01:22<02:33,  2.55it/s, training loss=0.164]\u001b[A\n",
      "Epoch 2:  35%|███▌      | 212/604 [01:22<02:33,  2.55it/s, training loss=0.150]\u001b[A\n",
      "Epoch 2:  35%|███▌      | 213/604 [01:22<02:32,  2.56it/s, training loss=0.150]\u001b[A\n",
      "Epoch 2:  35%|███▌      | 213/604 [01:23<02:32,  2.56it/s, training loss=0.158]\u001b[A\n",
      "Epoch 2:  35%|███▌      | 214/604 [01:23<02:31,  2.57it/s, training loss=0.158]\u001b[A\n",
      "Epoch 2:  35%|███▌      | 214/604 [01:23<02:31,  2.57it/s, training loss=0.208]\u001b[A\n",
      "Epoch 2:  36%|███▌      | 215/604 [01:23<02:30,  2.58it/s, training loss=0.208]\u001b[A\n",
      "Epoch 2:  36%|███▌      | 215/604 [01:23<02:30,  2.58it/s, training loss=0.197]\u001b[A\n",
      "Epoch 2:  36%|███▌      | 216/604 [01:23<02:30,  2.58it/s, training loss=0.197]\u001b[A\n",
      "Epoch 2:  36%|███▌      | 216/604 [01:24<02:30,  2.58it/s, training loss=0.131]\u001b[A\n",
      "Epoch 2:  36%|███▌      | 217/604 [01:24<02:29,  2.59it/s, training loss=0.131]\u001b[A\n",
      "Epoch 2:  36%|███▌      | 217/604 [01:24<02:29,  2.59it/s, training loss=0.131]\u001b[A\n",
      "Epoch 2:  36%|███▌      | 218/604 [01:24<02:28,  2.59it/s, training loss=0.131]\u001b[A\n",
      "Epoch 2:  36%|███▌      | 218/604 [01:25<02:28,  2.59it/s, training loss=0.230]\u001b[A\n",
      "Epoch 2:  36%|███▋      | 219/604 [01:25<02:29,  2.58it/s, training loss=0.230]\u001b[A\n",
      "Epoch 2:  36%|███▋      | 219/604 [01:25<02:29,  2.58it/s, training loss=0.123]\u001b[A\n",
      "Epoch 2:  36%|███▋      | 220/604 [01:25<02:28,  2.58it/s, training loss=0.123]\u001b[A\n",
      "Epoch 2:  36%|███▋      | 220/604 [01:25<02:28,  2.58it/s, training loss=0.217]\u001b[A\n",
      "Epoch 2:  37%|███▋      | 221/604 [01:25<02:28,  2.59it/s, training loss=0.217]\u001b[A\n",
      "Epoch 2:  37%|███▋      | 221/604 [01:26<02:28,  2.59it/s, training loss=0.229]\u001b[A\n",
      "Epoch 2:  37%|███▋      | 222/604 [01:26<02:27,  2.59it/s, training loss=0.229]\u001b[A\n",
      "Epoch 2:  37%|███▋      | 222/604 [01:26<02:27,  2.59it/s, training loss=0.139]\u001b[A\n",
      "Epoch 2:  37%|███▋      | 223/604 [01:26<02:27,  2.59it/s, training loss=0.139]\u001b[A\n",
      "Epoch 2:  37%|███▋      | 223/604 [01:26<02:27,  2.59it/s, training loss=0.236]\u001b[A\n",
      "Epoch 2:  37%|███▋      | 224/604 [01:26<02:27,  2.57it/s, training loss=0.236]\u001b[A\n",
      "Epoch 2:  37%|███▋      | 224/604 [01:27<02:27,  2.57it/s, training loss=0.112]\u001b[A\n",
      "Epoch 2:  37%|███▋      | 225/604 [01:27<02:27,  2.58it/s, training loss=0.112]\u001b[A\n",
      "Epoch 2:  37%|███▋      | 225/604 [01:27<02:27,  2.58it/s, training loss=0.172]\u001b[A\n",
      "Epoch 2:  37%|███▋      | 226/604 [01:27<02:26,  2.58it/s, training loss=0.172]\u001b[A\n",
      "Epoch 2:  37%|███▋      | 226/604 [01:28<02:26,  2.58it/s, training loss=0.209]\u001b[A\n",
      "Epoch 2:  38%|███▊      | 227/604 [01:28<02:27,  2.56it/s, training loss=0.209]\u001b[A\n",
      "Epoch 2:  38%|███▊      | 227/604 [01:28<02:27,  2.56it/s, training loss=0.209]\u001b[A\n",
      "Epoch 2:  38%|███▊      | 228/604 [01:28<02:26,  2.57it/s, training loss=0.209]\u001b[A\n",
      "Epoch 2:  38%|███▊      | 228/604 [01:28<02:26,  2.57it/s, training loss=0.147]\u001b[A\n",
      "Epoch 2:  38%|███▊      | 229/604 [01:28<02:25,  2.58it/s, training loss=0.147]\u001b[A\n",
      "Epoch 2:  38%|███▊      | 229/604 [01:29<02:25,  2.58it/s, training loss=0.206]\u001b[A\n",
      "Epoch 2:  38%|███▊      | 230/604 [01:29<02:24,  2.58it/s, training loss=0.206]\u001b[A\n",
      "Epoch 2:  38%|███▊      | 230/604 [01:29<02:24,  2.58it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  38%|███▊      | 231/604 [01:29<02:24,  2.58it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  38%|███▊      | 231/604 [01:30<02:24,  2.58it/s, training loss=0.177]\u001b[A\n",
      "Epoch 2:  38%|███▊      | 232/604 [01:30<02:23,  2.59it/s, training loss=0.177]\u001b[A\n",
      "Epoch 2:  38%|███▊      | 232/604 [01:30<02:23,  2.59it/s, training loss=0.228]\u001b[A\n",
      "Epoch 2:  39%|███▊      | 233/604 [01:30<02:23,  2.59it/s, training loss=0.228]\u001b[A\n",
      "Epoch 2:  39%|███▊      | 233/604 [01:30<02:23,  2.59it/s, training loss=0.121]\u001b[A\n",
      "Epoch 2:  39%|███▊      | 234/604 [01:30<02:22,  2.59it/s, training loss=0.121]\u001b[A\n",
      "Epoch 2:  39%|███▊      | 234/604 [01:31<02:22,  2.59it/s, training loss=0.234]\u001b[A\n",
      "Epoch 2:  39%|███▉      | 235/604 [01:31<02:22,  2.59it/s, training loss=0.234]\u001b[A\n",
      "Epoch 2:  39%|███▉      | 235/604 [01:31<02:22,  2.59it/s, training loss=0.136]\u001b[A\n",
      "Epoch 2:  39%|███▉      | 236/604 [01:31<02:21,  2.59it/s, training loss=0.136]\u001b[A\n",
      "Epoch 2:  39%|███▉      | 236/604 [01:31<02:21,  2.59it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2:  39%|███▉      | 237/604 [01:31<02:21,  2.59it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2:  39%|███▉      | 237/604 [01:32<02:21,  2.59it/s, training loss=0.178]\u001b[A\n",
      "Epoch 2:  39%|███▉      | 238/604 [01:32<02:21,  2.59it/s, training loss=0.178]\u001b[A\n",
      "Epoch 2:  39%|███▉      | 238/604 [01:32<02:21,  2.59it/s, training loss=0.225]\u001b[A\n",
      "Epoch 2:  40%|███▉      | 239/604 [01:32<02:20,  2.60it/s, training loss=0.225]\u001b[A\n",
      "Epoch 2:  40%|███▉      | 239/604 [01:33<02:20,  2.60it/s, training loss=0.157]\u001b[A\n",
      "Epoch 2:  40%|███▉      | 240/604 [01:33<02:20,  2.58it/s, training loss=0.157]\u001b[A\n",
      "Epoch 2:  40%|███▉      | 240/604 [01:33<02:20,  2.58it/s, training loss=0.187]\u001b[A\n",
      "Epoch 2:  40%|███▉      | 241/604 [01:33<02:20,  2.59it/s, training loss=0.187]\u001b[A\n",
      "Epoch 2:  40%|███▉      | 241/604 [01:33<02:20,  2.59it/s, training loss=0.218]\u001b[A\n",
      "Epoch 2:  40%|████      | 242/604 [01:33<02:19,  2.59it/s, training loss=0.218]\u001b[A\n",
      "Epoch 2:  40%|████      | 242/604 [01:34<02:19,  2.59it/s, training loss=0.249]\u001b[A\n",
      "Epoch 2:  40%|████      | 243/604 [01:34<02:19,  2.59it/s, training loss=0.249]\u001b[A\n",
      "Epoch 2:  40%|████      | 243/604 [01:34<02:19,  2.59it/s, training loss=0.187]\u001b[A\n",
      "Epoch 2:  40%|████      | 244/604 [01:34<02:18,  2.60it/s, training loss=0.187]\u001b[A\n",
      "Epoch 2:  40%|████      | 244/604 [01:35<02:18,  2.60it/s, training loss=0.266]\u001b[A\n",
      "Epoch 2:  41%|████      | 245/604 [01:35<02:18,  2.60it/s, training loss=0.266]\u001b[A\n",
      "Epoch 2:  41%|████      | 245/604 [01:35<02:18,  2.60it/s, training loss=0.136]\u001b[A\n",
      "Epoch 2:  41%|████      | 246/604 [01:35<02:18,  2.58it/s, training loss=0.136]\u001b[A\n",
      "Epoch 2:  41%|████      | 246/604 [01:35<02:18,  2.58it/s, training loss=0.322]\u001b[A\n",
      "Epoch 2:  41%|████      | 247/604 [01:35<02:22,  2.50it/s, training loss=0.322]\u001b[A\n",
      "Epoch 2:  41%|████      | 247/604 [01:36<02:22,  2.50it/s, training loss=0.208]\u001b[A\n",
      "Epoch 2:  41%|████      | 248/604 [01:36<02:21,  2.51it/s, training loss=0.208]\u001b[A\n",
      "Epoch 2:  41%|████      | 248/604 [01:36<02:21,  2.51it/s, training loss=0.149]\u001b[A\n",
      "Epoch 2:  41%|████      | 249/604 [01:36<02:19,  2.54it/s, training loss=0.149]\u001b[A\n",
      "Epoch 2:  41%|████      | 249/604 [01:37<02:19,  2.54it/s, training loss=0.179]\u001b[A\n",
      "Epoch 2:  41%|████▏     | 250/604 [01:37<02:18,  2.55it/s, training loss=0.179]\u001b[A\n",
      "Epoch 2:  41%|████▏     | 250/604 [01:37<02:18,  2.55it/s, training loss=0.163]\u001b[A\n",
      "Epoch 2:  42%|████▏     | 251/604 [01:37<02:17,  2.57it/s, training loss=0.163]\u001b[A\n",
      "Epoch 2:  42%|████▏     | 251/604 [01:37<02:17,  2.57it/s, training loss=0.160]\u001b[A\n",
      "Epoch 2:  42%|████▏     | 252/604 [01:37<02:18,  2.53it/s, training loss=0.160]\u001b[A\n",
      "Epoch 2:  42%|████▏     | 252/604 [01:38<02:18,  2.53it/s, training loss=0.260]\u001b[A\n",
      "Epoch 2:  42%|████▏     | 253/604 [01:38<02:19,  2.51it/s, training loss=0.260]\u001b[A\n",
      "Epoch 2:  42%|████▏     | 253/604 [01:38<02:19,  2.51it/s, training loss=0.152]\u001b[A\n",
      "Epoch 2:  42%|████▏     | 254/604 [01:38<02:19,  2.52it/s, training loss=0.152]\u001b[A\n",
      "Epoch 2:  42%|████▏     | 254/604 [01:39<02:19,  2.52it/s, training loss=0.225]\u001b[A\n",
      "Epoch 2:  42%|████▏     | 255/604 [01:39<02:17,  2.53it/s, training loss=0.225]\u001b[A\n",
      "Epoch 2:  42%|████▏     | 255/604 [01:39<02:17,  2.53it/s, training loss=0.191]\u001b[A\n",
      "Epoch 2:  42%|████▏     | 256/604 [01:39<02:16,  2.55it/s, training loss=0.191]\u001b[A\n",
      "Epoch 2:  42%|████▏     | 256/604 [01:39<02:16,  2.55it/s, training loss=0.190]\u001b[A\n",
      "Epoch 2:  43%|████▎     | 257/604 [01:39<02:15,  2.57it/s, training loss=0.190]\u001b[A\n",
      "Epoch 2:  43%|████▎     | 257/604 [01:40<02:15,  2.57it/s, training loss=0.165]\u001b[A\n",
      "Epoch 2:  43%|████▎     | 258/604 [01:40<02:14,  2.57it/s, training loss=0.165]\u001b[A\n",
      "Epoch 2:  43%|████▎     | 258/604 [01:40<02:14,  2.57it/s, training loss=0.277]\u001b[A\n",
      "Epoch 2:  43%|████▎     | 259/604 [01:40<02:16,  2.53it/s, training loss=0.277]\u001b[A\n",
      "Epoch 2:  43%|████▎     | 259/604 [01:40<02:16,  2.53it/s, training loss=0.160]\u001b[A\n",
      "Epoch 2:  43%|████▎     | 260/604 [01:40<02:15,  2.54it/s, training loss=0.160]\u001b[A\n",
      "Epoch 2:  43%|████▎     | 260/604 [01:41<02:15,  2.54it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  43%|████▎     | 261/604 [01:41<02:14,  2.56it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  43%|████▎     | 261/604 [01:41<02:14,  2.56it/s, training loss=0.164]\u001b[A\n",
      "Epoch 2:  43%|████▎     | 262/604 [01:41<02:13,  2.56it/s, training loss=0.164]\u001b[A\n",
      "Epoch 2:  43%|████▎     | 262/604 [01:42<02:13,  2.56it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  44%|████▎     | 263/604 [01:42<02:12,  2.57it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  44%|████▎     | 263/604 [01:42<02:12,  2.57it/s, training loss=0.138]\u001b[A\n",
      "Epoch 2:  44%|████▎     | 264/604 [01:42<02:12,  2.57it/s, training loss=0.138]\u001b[A\n",
      "Epoch 2:  44%|████▎     | 264/604 [01:42<02:12,  2.57it/s, training loss=0.265]\u001b[A\n",
      "Epoch 2:  44%|████▍     | 265/604 [01:42<02:12,  2.56it/s, training loss=0.265]\u001b[A\n",
      "Epoch 2:  44%|████▍     | 265/604 [01:43<02:12,  2.56it/s, training loss=0.197]\u001b[A\n",
      "Epoch 2:  44%|████▍     | 266/604 [01:43<02:11,  2.56it/s, training loss=0.197]\u001b[A\n",
      "Epoch 2:  44%|████▍     | 266/604 [01:43<02:11,  2.56it/s, training loss=0.206]\u001b[A\n",
      "Epoch 2:  44%|████▍     | 267/604 [01:43<02:11,  2.57it/s, training loss=0.206]\u001b[A\n",
      "Epoch 2:  44%|████▍     | 267/604 [01:44<02:11,  2.57it/s, training loss=0.198]\u001b[A\n",
      "Epoch 2:  44%|████▍     | 268/604 [01:44<02:11,  2.56it/s, training loss=0.198]\u001b[A\n",
      "Epoch 2:  44%|████▍     | 268/604 [01:44<02:11,  2.56it/s, training loss=0.169]\u001b[A\n",
      "Epoch 2:  45%|████▍     | 269/604 [01:44<02:10,  2.57it/s, training loss=0.169]\u001b[A\n",
      "Epoch 2:  45%|████▍     | 269/604 [01:44<02:10,  2.57it/s, training loss=0.176]\u001b[A\n",
      "Epoch 2:  45%|████▍     | 270/604 [01:44<02:09,  2.57it/s, training loss=0.176]\u001b[A\n",
      "Epoch 2:  45%|████▍     | 270/604 [01:45<02:09,  2.57it/s, training loss=0.198]\u001b[A\n",
      "Epoch 2:  45%|████▍     | 271/604 [01:45<02:09,  2.57it/s, training loss=0.198]\u001b[A\n",
      "Epoch 2:  45%|████▍     | 271/604 [01:45<02:09,  2.57it/s, training loss=0.176]\u001b[A\n",
      "Epoch 2:  45%|████▌     | 272/604 [01:45<02:08,  2.58it/s, training loss=0.176]\u001b[A\n",
      "Epoch 2:  45%|████▌     | 272/604 [01:46<02:08,  2.58it/s, training loss=0.193]\u001b[A\n",
      "Epoch 2:  45%|████▌     | 273/604 [01:46<02:11,  2.51it/s, training loss=0.193]\u001b[A\n",
      "Epoch 2:  45%|████▌     | 273/604 [01:46<02:11,  2.51it/s, training loss=0.176]\u001b[A\n",
      "Epoch 2:  45%|████▌     | 274/604 [01:46<02:10,  2.52it/s, training loss=0.176]\u001b[A\n",
      "Epoch 2:  45%|████▌     | 274/604 [01:46<02:10,  2.52it/s, training loss=0.186]\u001b[A\n",
      "Epoch 2:  46%|████▌     | 275/604 [01:46<02:09,  2.54it/s, training loss=0.186]\u001b[A\n",
      "Epoch 2:  46%|████▌     | 275/604 [01:47<02:09,  2.54it/s, training loss=0.199]\u001b[A\n",
      "Epoch 2:  46%|████▌     | 276/604 [01:47<02:08,  2.55it/s, training loss=0.199]\u001b[A\n",
      "Epoch 2:  46%|████▌     | 276/604 [01:47<02:08,  2.55it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  46%|████▌     | 277/604 [01:47<02:07,  2.56it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  46%|████▌     | 277/604 [01:48<02:07,  2.56it/s, training loss=0.178]\u001b[A\n",
      "Epoch 2:  46%|████▌     | 278/604 [01:48<02:06,  2.57it/s, training loss=0.178]\u001b[A\n",
      "Epoch 2:  46%|████▌     | 278/604 [01:48<02:06,  2.57it/s, training loss=0.229]\u001b[A\n",
      "Epoch 2:  46%|████▌     | 279/604 [01:48<02:05,  2.58it/s, training loss=0.229]\u001b[A\n",
      "Epoch 2:  46%|████▌     | 279/604 [01:48<02:05,  2.58it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  46%|████▋     | 280/604 [01:48<02:06,  2.56it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  46%|████▋     | 280/604 [01:49<02:06,  2.56it/s, training loss=0.248]\u001b[A\n",
      "Epoch 2:  47%|████▋     | 281/604 [01:49<02:07,  2.54it/s, training loss=0.248]\u001b[A\n",
      "Epoch 2:  47%|████▋     | 281/604 [01:49<02:07,  2.54it/s, training loss=0.262]\u001b[A\n",
      "Epoch 2:  47%|████▋     | 282/604 [01:49<02:06,  2.55it/s, training loss=0.262]\u001b[A\n",
      "Epoch 2:  47%|████▋     | 282/604 [01:49<02:06,  2.55it/s, training loss=0.172]\u001b[A\n",
      "Epoch 2:  47%|████▋     | 283/604 [01:49<02:05,  2.56it/s, training loss=0.172]\u001b[A\n",
      "Epoch 2:  47%|████▋     | 283/604 [01:50<02:05,  2.56it/s, training loss=0.155]\u001b[A\n",
      "Epoch 2:  47%|████▋     | 284/604 [01:50<02:04,  2.57it/s, training loss=0.155]\u001b[A\n",
      "Epoch 2:  47%|████▋     | 284/604 [01:50<02:04,  2.57it/s, training loss=0.145]\u001b[A\n",
      "Epoch 2:  47%|████▋     | 285/604 [01:50<02:03,  2.58it/s, training loss=0.145]\u001b[A\n",
      "Epoch 2:  47%|████▋     | 285/604 [01:51<02:03,  2.58it/s, training loss=0.209]\u001b[A\n",
      "Epoch 2:  47%|████▋     | 286/604 [01:51<02:03,  2.58it/s, training loss=0.209]\u001b[A\n",
      "Epoch 2:  47%|████▋     | 286/604 [01:51<02:03,  2.58it/s, training loss=0.185]\u001b[A\n",
      "Epoch 2:  48%|████▊     | 287/604 [01:51<02:03,  2.57it/s, training loss=0.185]\u001b[A\n",
      "Epoch 2:  48%|████▊     | 287/604 [01:51<02:03,  2.57it/s, training loss=0.146]\u001b[A\n",
      "Epoch 2:  48%|████▊     | 288/604 [01:51<02:02,  2.57it/s, training loss=0.146]\u001b[A\n",
      "Epoch 2:  48%|████▊     | 288/604 [01:52<02:02,  2.57it/s, training loss=0.179]\u001b[A\n",
      "Epoch 2:  48%|████▊     | 289/604 [01:52<02:02,  2.58it/s, training loss=0.179]\u001b[A\n",
      "Epoch 2:  48%|████▊     | 289/604 [01:52<02:02,  2.58it/s, training loss=0.151]\u001b[A\n",
      "Epoch 2:  48%|████▊     | 290/604 [01:52<02:01,  2.58it/s, training loss=0.151]\u001b[A\n",
      "Epoch 2:  48%|████▊     | 290/604 [01:53<02:01,  2.58it/s, training loss=0.207]\u001b[A\n",
      "Epoch 2:  48%|████▊     | 291/604 [01:53<02:04,  2.51it/s, training loss=0.207]\u001b[A\n",
      "Epoch 2:  48%|████▊     | 291/604 [01:53<02:04,  2.51it/s, training loss=0.139]\u001b[A\n",
      "Epoch 2:  48%|████▊     | 292/604 [01:53<02:03,  2.53it/s, training loss=0.139]\u001b[A\n",
      "Epoch 2:  48%|████▊     | 292/604 [01:53<02:03,  2.53it/s, training loss=0.209]\u001b[A\n",
      "Epoch 2:  49%|████▊     | 293/604 [01:53<02:02,  2.55it/s, training loss=0.209]\u001b[A\n",
      "Epoch 2:  49%|████▊     | 293/604 [01:54<02:02,  2.55it/s, training loss=0.196]\u001b[A\n",
      "Epoch 2:  49%|████▊     | 294/604 [01:54<02:01,  2.56it/s, training loss=0.196]\u001b[A\n",
      "Epoch 2:  49%|████▊     | 294/604 [01:54<02:01,  2.56it/s, training loss=0.200]\u001b[A\n",
      "Epoch 2:  49%|████▉     | 295/604 [01:54<02:00,  2.57it/s, training loss=0.200]\u001b[A\n",
      "Epoch 2:  49%|████▉     | 295/604 [01:55<02:00,  2.57it/s, training loss=0.198]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  49%|████▉     | 296/604 [01:55<01:59,  2.57it/s, training loss=0.198]\u001b[A\n",
      "Epoch 2:  49%|████▉     | 296/604 [01:55<01:59,  2.57it/s, training loss=0.151]\u001b[A\n",
      "Epoch 2:  49%|████▉     | 297/604 [01:55<01:59,  2.58it/s, training loss=0.151]\u001b[A\n",
      "Epoch 2:  49%|████▉     | 297/604 [01:55<01:59,  2.58it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  49%|████▉     | 298/604 [01:55<01:58,  2.58it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  49%|████▉     | 298/604 [01:56<01:58,  2.58it/s, training loss=0.195]\u001b[A\n",
      "Epoch 2:  50%|████▉     | 299/604 [01:56<01:58,  2.58it/s, training loss=0.195]\u001b[A\n",
      "Epoch 2:  50%|████▉     | 299/604 [01:56<01:58,  2.58it/s, training loss=0.265]\u001b[A\n",
      "Epoch 2:  50%|████▉     | 300/604 [01:56<01:57,  2.58it/s, training loss=0.265]\u001b[A\n",
      "Epoch 2:  50%|████▉     | 300/604 [01:56<01:57,  2.58it/s, training loss=0.147]\u001b[A\n",
      "Epoch 2:  50%|████▉     | 301/604 [01:56<01:57,  2.59it/s, training loss=0.147]\u001b[A\n",
      "Epoch 2:  50%|████▉     | 301/604 [01:57<01:57,  2.59it/s, training loss=0.166]\u001b[A\n",
      "Epoch 2:  50%|█████     | 302/604 [01:57<01:56,  2.59it/s, training loss=0.166]\u001b[A\n",
      "Epoch 2:  50%|█████     | 302/604 [01:57<01:56,  2.59it/s, training loss=0.159]\u001b[A\n",
      "Epoch 2:  50%|█████     | 303/604 [01:57<01:56,  2.59it/s, training loss=0.159]\u001b[A\n",
      "Epoch 2:  50%|█████     | 303/604 [01:58<01:56,  2.59it/s, training loss=0.157]\u001b[A\n",
      "Epoch 2:  50%|█████     | 304/604 [01:58<01:56,  2.59it/s, training loss=0.157]\u001b[A\n",
      "Epoch 2:  50%|█████     | 304/604 [01:58<01:56,  2.59it/s, training loss=0.136]\u001b[A\n",
      "Epoch 2:  50%|█████     | 305/604 [01:58<01:55,  2.59it/s, training loss=0.136]\u001b[A\n",
      "Epoch 2:  50%|█████     | 305/604 [01:58<01:55,  2.59it/s, training loss=0.184]\u001b[A\n",
      "Epoch 2:  51%|█████     | 306/604 [01:58<01:55,  2.59it/s, training loss=0.184]\u001b[A\n",
      "Epoch 2:  51%|█████     | 306/604 [01:59<01:55,  2.59it/s, training loss=0.156]\u001b[A\n",
      "Epoch 2:  51%|█████     | 307/604 [01:59<01:54,  2.59it/s, training loss=0.156]\u001b[A\n",
      "Epoch 2:  51%|█████     | 307/604 [01:59<01:54,  2.59it/s, training loss=0.205]\u001b[A\n",
      "Epoch 2:  51%|█████     | 308/604 [01:59<01:54,  2.59it/s, training loss=0.205]\u001b[A\n",
      "Epoch 2:  51%|█████     | 308/604 [02:00<01:54,  2.59it/s, training loss=0.104]\u001b[A\n",
      "Epoch 2:  51%|█████     | 309/604 [02:00<01:54,  2.57it/s, training loss=0.104]\u001b[A\n",
      "Epoch 2:  51%|█████     | 309/604 [02:00<01:54,  2.57it/s, training loss=0.216]\u001b[A\n",
      "Epoch 2:  51%|█████▏    | 310/604 [02:00<01:54,  2.57it/s, training loss=0.216]\u001b[A\n",
      "Epoch 2:  51%|█████▏    | 310/604 [02:00<01:54,  2.57it/s, training loss=0.156]\u001b[A\n",
      "Epoch 2:  51%|█████▏    | 311/604 [02:00<01:53,  2.57it/s, training loss=0.156]\u001b[A\n",
      "Epoch 2:  51%|█████▏    | 311/604 [02:01<01:53,  2.57it/s, training loss=0.250]\u001b[A\n",
      "Epoch 2:  52%|█████▏    | 312/604 [02:01<01:53,  2.57it/s, training loss=0.250]\u001b[A\n",
      "Epoch 2:  52%|█████▏    | 312/604 [02:01<01:53,  2.57it/s, training loss=0.252]\u001b[A\n",
      "Epoch 2:  52%|█████▏    | 313/604 [02:01<01:53,  2.57it/s, training loss=0.252]\u001b[A\n",
      "Epoch 2:  52%|█████▏    | 313/604 [02:02<01:53,  2.57it/s, training loss=0.168]\u001b[A\n",
      "Epoch 2:  52%|█████▏    | 314/604 [02:02<01:52,  2.57it/s, training loss=0.168]\u001b[A\n",
      "Epoch 2:  52%|█████▏    | 314/604 [02:02<01:52,  2.57it/s, training loss=0.139]\u001b[A\n",
      "Epoch 2:  52%|█████▏    | 315/604 [02:02<01:52,  2.58it/s, training loss=0.139]\u001b[A\n",
      "Epoch 2:  52%|█████▏    | 315/604 [02:02<01:52,  2.58it/s, training loss=0.209]\u001b[A\n",
      "Epoch 2:  52%|█████▏    | 316/604 [02:02<01:51,  2.58it/s, training loss=0.209]\u001b[A\n",
      "Epoch 2:  52%|█████▏    | 316/604 [02:03<01:51,  2.58it/s, training loss=0.222]\u001b[A\n",
      "Epoch 2:  52%|█████▏    | 317/604 [02:03<01:50,  2.59it/s, training loss=0.222]\u001b[A\n",
      "Epoch 2:  52%|█████▏    | 317/604 [02:03<01:50,  2.59it/s, training loss=0.180]\u001b[A\n",
      "Epoch 2:  53%|█████▎    | 318/604 [02:03<01:50,  2.59it/s, training loss=0.180]\u001b[A\n",
      "Epoch 2:  53%|█████▎    | 318/604 [02:03<01:50,  2.59it/s, training loss=0.153]\u001b[A\n",
      "Epoch 2:  53%|█████▎    | 319/604 [02:03<01:49,  2.59it/s, training loss=0.153]\u001b[A\n",
      "Epoch 2:  53%|█████▎    | 319/604 [02:04<01:49,  2.59it/s, training loss=0.134]\u001b[A\n",
      "Epoch 2:  53%|█████▎    | 320/604 [02:04<01:49,  2.59it/s, training loss=0.134]\u001b[A\n",
      "Epoch 2:  53%|█████▎    | 320/604 [02:04<01:49,  2.59it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  53%|█████▎    | 321/604 [02:04<01:49,  2.59it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  53%|█████▎    | 321/604 [02:05<01:49,  2.59it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:  53%|█████▎    | 322/604 [02:05<01:48,  2.59it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:  53%|█████▎    | 322/604 [02:05<01:48,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 2:  53%|█████▎    | 323/604 [02:05<01:48,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 2:  53%|█████▎    | 323/604 [02:05<01:48,  2.59it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  54%|█████▎    | 324/604 [02:05<01:47,  2.59it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  54%|█████▎    | 324/604 [02:06<01:47,  2.59it/s, training loss=0.177]\u001b[A\n",
      "Epoch 2:  54%|█████▍    | 325/604 [02:06<01:48,  2.58it/s, training loss=0.177]\u001b[A\n",
      "Epoch 2:  54%|█████▍    | 325/604 [02:06<01:48,  2.58it/s, training loss=0.214]\u001b[A\n",
      "Epoch 2:  54%|█████▍    | 326/604 [02:06<01:47,  2.58it/s, training loss=0.214]\u001b[A\n",
      "Epoch 2:  54%|█████▍    | 326/604 [02:07<01:47,  2.58it/s, training loss=0.177]\u001b[A\n",
      "Epoch 2:  54%|█████▍    | 327/604 [02:07<01:47,  2.59it/s, training loss=0.177]\u001b[A\n",
      "Epoch 2:  54%|█████▍    | 327/604 [02:07<01:47,  2.59it/s, training loss=0.140]\u001b[A\n",
      "Epoch 2:  54%|█████▍    | 328/604 [02:07<01:46,  2.59it/s, training loss=0.140]\u001b[A\n",
      "Epoch 2:  54%|█████▍    | 328/604 [02:07<01:46,  2.59it/s, training loss=0.219]\u001b[A\n",
      "Epoch 2:  54%|█████▍    | 329/604 [02:07<01:46,  2.59it/s, training loss=0.219]\u001b[A\n",
      "Epoch 2:  54%|█████▍    | 329/604 [02:08<01:46,  2.59it/s, training loss=0.154]\u001b[A\n",
      "Epoch 2:  55%|█████▍    | 330/604 [02:08<01:45,  2.59it/s, training loss=0.154]\u001b[A\n",
      "Epoch 2:  55%|█████▍    | 330/604 [02:08<01:45,  2.59it/s, training loss=0.230]\u001b[A\n",
      "Epoch 2:  55%|█████▍    | 331/604 [02:08<01:45,  2.59it/s, training loss=0.230]\u001b[A\n",
      "Epoch 2:  55%|█████▍    | 331/604 [02:08<01:45,  2.59it/s, training loss=0.175]\u001b[A\n",
      "Epoch 2:  55%|█████▍    | 332/604 [02:08<01:45,  2.57it/s, training loss=0.175]\u001b[A\n",
      "Epoch 2:  55%|█████▍    | 332/604 [02:09<01:45,  2.57it/s, training loss=0.161]\u001b[A\n",
      "Epoch 2:  55%|█████▌    | 333/604 [02:09<01:45,  2.58it/s, training loss=0.161]\u001b[A\n",
      "Epoch 2:  55%|█████▌    | 333/604 [02:09<01:45,  2.58it/s, training loss=0.243]\u001b[A\n",
      "Epoch 2:  55%|█████▌    | 334/604 [02:09<01:44,  2.58it/s, training loss=0.243]\u001b[A\n",
      "Epoch 2:  55%|█████▌    | 334/604 [02:10<01:44,  2.58it/s, training loss=0.113]\u001b[A\n",
      "Epoch 2:  55%|█████▌    | 335/604 [02:10<01:43,  2.59it/s, training loss=0.113]\u001b[A\n",
      "Epoch 2:  55%|█████▌    | 335/604 [02:10<01:43,  2.59it/s, training loss=0.191]\u001b[A\n",
      "Epoch 2:  56%|█████▌    | 336/604 [02:10<01:43,  2.59it/s, training loss=0.191]\u001b[A\n",
      "Epoch 2:  56%|█████▌    | 336/604 [02:10<01:43,  2.59it/s, training loss=0.267]\u001b[A\n",
      "Epoch 2:  56%|█████▌    | 337/604 [02:10<01:44,  2.56it/s, training loss=0.267]\u001b[A\n",
      "Epoch 2:  56%|█████▌    | 337/604 [02:11<01:44,  2.56it/s, training loss=0.195]\u001b[A\n",
      "Epoch 2:  56%|█████▌    | 338/604 [02:11<01:45,  2.53it/s, training loss=0.195]\u001b[A\n",
      "Epoch 2:  56%|█████▌    | 338/604 [02:11<01:45,  2.53it/s, training loss=0.183]\u001b[A\n",
      "Epoch 2:  56%|█████▌    | 339/604 [02:11<01:44,  2.53it/s, training loss=0.183]\u001b[A\n",
      "Epoch 2:  56%|█████▌    | 339/604 [02:12<01:44,  2.53it/s, training loss=0.270]\u001b[A\n",
      "Epoch 2:  56%|█████▋    | 340/604 [02:12<01:43,  2.55it/s, training loss=0.270]\u001b[A\n",
      "Epoch 2:  56%|█████▋    | 340/604 [02:12<01:43,  2.55it/s, training loss=0.165]\u001b[A\n",
      "Epoch 2:  56%|█████▋    | 341/604 [02:12<01:42,  2.56it/s, training loss=0.165]\u001b[A\n",
      "Epoch 2:  56%|█████▋    | 341/604 [02:12<01:42,  2.56it/s, training loss=0.244]\u001b[A\n",
      "Epoch 2:  57%|█████▋    | 342/604 [02:12<01:41,  2.57it/s, training loss=0.244]\u001b[A\n",
      "Epoch 2:  57%|█████▋    | 342/604 [02:13<01:41,  2.57it/s, training loss=0.138]\u001b[A\n",
      "Epoch 2:  57%|█████▋    | 343/604 [02:13<01:41,  2.58it/s, training loss=0.138]\u001b[A\n",
      "Epoch 2:  57%|█████▋    | 343/604 [02:13<01:41,  2.58it/s, training loss=0.184]\u001b[A\n",
      "Epoch 2:  57%|█████▋    | 344/604 [02:13<01:40,  2.58it/s, training loss=0.184]\u001b[A\n",
      "Epoch 2:  57%|█████▋    | 344/604 [02:14<01:40,  2.58it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:  57%|█████▋    | 345/604 [02:14<01:40,  2.59it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:  57%|█████▋    | 345/604 [02:14<01:40,  2.59it/s, training loss=0.200]\u001b[A\n",
      "Epoch 2:  57%|█████▋    | 346/604 [02:14<01:39,  2.59it/s, training loss=0.200]\u001b[A\n",
      "Epoch 2:  57%|█████▋    | 346/604 [02:14<01:39,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 2:  57%|█████▋    | 347/604 [02:14<01:39,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 2:  57%|█████▋    | 347/604 [02:15<01:39,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 2:  58%|█████▊    | 348/604 [02:15<01:38,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 2:  58%|█████▊    | 348/604 [02:15<01:38,  2.59it/s, training loss=0.198]\u001b[A\n",
      "Epoch 2:  58%|█████▊    | 349/604 [02:15<01:38,  2.59it/s, training loss=0.198]\u001b[A\n",
      "Epoch 2:  58%|█████▊    | 349/604 [02:15<01:38,  2.59it/s, training loss=0.212]\u001b[A\n",
      "Epoch 2:  58%|█████▊    | 350/604 [02:15<01:37,  2.60it/s, training loss=0.212]\u001b[A\n",
      "Epoch 2:  58%|█████▊    | 350/604 [02:16<01:37,  2.60it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:  58%|█████▊    | 351/604 [02:16<01:37,  2.60it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:  58%|█████▊    | 351/604 [02:16<01:37,  2.60it/s, training loss=0.208]\u001b[A\n",
      "Epoch 2:  58%|█████▊    | 352/604 [02:16<01:37,  2.60it/s, training loss=0.208]\u001b[A\n",
      "Epoch 2:  58%|█████▊    | 352/604 [02:17<01:37,  2.60it/s, training loss=0.251]\u001b[A\n",
      "Epoch 2:  58%|█████▊    | 353/604 [02:17<01:36,  2.59it/s, training loss=0.251]\u001b[A\n",
      "Epoch 2:  58%|█████▊    | 353/604 [02:17<01:36,  2.59it/s, training loss=0.225]\u001b[A\n",
      "Epoch 2:  59%|█████▊    | 354/604 [02:17<01:36,  2.60it/s, training loss=0.225]\u001b[A\n",
      "Epoch 2:  59%|█████▊    | 354/604 [02:17<01:36,  2.60it/s, training loss=0.238]\u001b[A\n",
      "Epoch 2:  59%|█████▉    | 355/604 [02:17<01:35,  2.60it/s, training loss=0.238]\u001b[A\n",
      "Epoch 2:  59%|█████▉    | 355/604 [02:18<01:35,  2.60it/s, training loss=0.128]\u001b[A\n",
      "Epoch 2:  59%|█████▉    | 356/604 [02:18<01:35,  2.60it/s, training loss=0.128]\u001b[A\n",
      "Epoch 2:  59%|█████▉    | 356/604 [02:18<01:35,  2.60it/s, training loss=0.166]\u001b[A\n",
      "Epoch 2:  59%|█████▉    | 357/604 [02:18<01:35,  2.60it/s, training loss=0.166]\u001b[A\n",
      "Epoch 2:  59%|█████▉    | 357/604 [02:19<01:35,  2.60it/s, training loss=0.196]\u001b[A\n",
      "Epoch 2:  59%|█████▉    | 358/604 [02:19<01:34,  2.59it/s, training loss=0.196]\u001b[A\n",
      "Epoch 2:  59%|█████▉    | 358/604 [02:19<01:34,  2.59it/s, training loss=0.163]\u001b[A\n",
      "Epoch 2:  59%|█████▉    | 359/604 [02:19<01:34,  2.59it/s, training loss=0.163]\u001b[A\n",
      "Epoch 2:  59%|█████▉    | 359/604 [02:19<01:34,  2.59it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  60%|█████▉    | 360/604 [02:19<01:34,  2.59it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  60%|█████▉    | 360/604 [02:20<01:34,  2.59it/s, training loss=0.191]\u001b[A\n",
      "Epoch 2:  60%|█████▉    | 361/604 [02:20<01:33,  2.59it/s, training loss=0.191]\u001b[A\n",
      "Epoch 2:  60%|█████▉    | 361/604 [02:20<01:33,  2.59it/s, training loss=0.241]\u001b[A\n",
      "Epoch 2:  60%|█████▉    | 362/604 [02:20<01:33,  2.60it/s, training loss=0.241]\u001b[A\n",
      "Epoch 2:  60%|█████▉    | 362/604 [02:20<01:33,  2.60it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  60%|██████    | 363/604 [02:20<01:33,  2.58it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  60%|██████    | 363/604 [02:21<01:33,  2.58it/s, training loss=0.193]\u001b[A\n",
      "Epoch 2:  60%|██████    | 364/604 [02:21<01:32,  2.58it/s, training loss=0.193]\u001b[A\n",
      "Epoch 2:  60%|██████    | 364/604 [02:21<01:32,  2.58it/s, training loss=0.159]\u001b[A\n",
      "Epoch 2:  60%|██████    | 365/604 [02:21<01:32,  2.58it/s, training loss=0.159]\u001b[A\n",
      "Epoch 2:  60%|██████    | 365/604 [02:22<01:32,  2.58it/s, training loss=0.224]\u001b[A\n",
      "Epoch 2:  61%|██████    | 366/604 [02:22<01:33,  2.55it/s, training loss=0.224]\u001b[A\n",
      "Epoch 2:  61%|██████    | 366/604 [02:22<01:33,  2.55it/s, training loss=0.165]\u001b[A\n",
      "Epoch 2:  61%|██████    | 367/604 [02:22<01:32,  2.56it/s, training loss=0.165]\u001b[A\n",
      "Epoch 2:  61%|██████    | 367/604 [02:22<01:32,  2.56it/s, training loss=0.220]\u001b[A\n",
      "Epoch 2:  61%|██████    | 368/604 [02:22<01:32,  2.55it/s, training loss=0.220]\u001b[A\n",
      "Epoch 2:  61%|██████    | 368/604 [02:23<01:32,  2.55it/s, training loss=0.228]\u001b[A\n",
      "Epoch 2:  61%|██████    | 369/604 [02:23<01:31,  2.56it/s, training loss=0.228]\u001b[A\n",
      "Epoch 2:  61%|██████    | 369/604 [02:23<01:31,  2.56it/s, training loss=0.170]\u001b[A\n",
      "Epoch 2:  61%|██████▏   | 370/604 [02:23<01:31,  2.57it/s, training loss=0.170]\u001b[A\n",
      "Epoch 2:  61%|██████▏   | 370/604 [02:24<01:31,  2.57it/s, training loss=0.160]\u001b[A\n",
      "Epoch 2:  61%|██████▏   | 371/604 [02:24<01:30,  2.58it/s, training loss=0.160]\u001b[A\n",
      "Epoch 2:  61%|██████▏   | 371/604 [02:24<01:30,  2.58it/s, training loss=0.201]\u001b[A\n",
      "Epoch 2:  62%|██████▏   | 372/604 [02:24<01:31,  2.53it/s, training loss=0.201]\u001b[A\n",
      "Epoch 2:  62%|██████▏   | 372/604 [02:24<01:31,  2.53it/s, training loss=0.302]\u001b[A\n",
      "Epoch 2:  62%|██████▏   | 373/604 [02:24<01:30,  2.54it/s, training loss=0.302]\u001b[A\n",
      "Epoch 2:  62%|██████▏   | 373/604 [02:25<01:30,  2.54it/s, training loss=0.257]\u001b[A\n",
      "Epoch 2:  62%|██████▏   | 374/604 [02:25<01:30,  2.56it/s, training loss=0.257]\u001b[A\n",
      "Epoch 2:  62%|██████▏   | 374/604 [02:25<01:30,  2.56it/s, training loss=0.269]\u001b[A\n",
      "Epoch 2:  62%|██████▏   | 375/604 [02:25<01:29,  2.56it/s, training loss=0.269]\u001b[A\n",
      "Epoch 2:  62%|██████▏   | 375/604 [02:26<01:29,  2.56it/s, training loss=0.157]\u001b[A\n",
      "Epoch 2:  62%|██████▏   | 376/604 [02:26<01:28,  2.57it/s, training loss=0.157]\u001b[A\n",
      "Epoch 2:  62%|██████▏   | 376/604 [02:26<01:28,  2.57it/s, training loss=0.119]\u001b[A\n",
      "Epoch 2:  62%|██████▏   | 377/604 [02:26<01:28,  2.58it/s, training loss=0.119]\u001b[A\n",
      "Epoch 2:  62%|██████▏   | 377/604 [02:26<01:28,  2.58it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:  63%|██████▎   | 378/604 [02:26<01:27,  2.58it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:  63%|██████▎   | 378/604 [02:27<01:27,  2.58it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:  63%|██████▎   | 379/604 [02:27<01:27,  2.58it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:  63%|██████▎   | 379/604 [02:27<01:27,  2.58it/s, training loss=0.166]\u001b[A\n",
      "Epoch 2:  63%|██████▎   | 380/604 [02:27<01:26,  2.59it/s, training loss=0.166]\u001b[A\n",
      "Epoch 2:  63%|██████▎   | 380/604 [02:27<01:26,  2.59it/s, training loss=0.130]\u001b[A\n",
      "Epoch 2:  63%|██████▎   | 381/604 [02:27<01:26,  2.59it/s, training loss=0.130]\u001b[A\n",
      "Epoch 2:  63%|██████▎   | 381/604 [02:28<01:26,  2.59it/s, training loss=0.165]\u001b[A\n",
      "Epoch 2:  63%|██████▎   | 382/604 [02:28<01:25,  2.59it/s, training loss=0.165]\u001b[A\n",
      "Epoch 2:  63%|██████▎   | 382/604 [02:28<01:25,  2.59it/s, training loss=0.216]\u001b[A\n",
      "Epoch 2:  63%|██████▎   | 383/604 [02:28<01:25,  2.59it/s, training loss=0.216]\u001b[A\n",
      "Epoch 2:  63%|██████▎   | 383/604 [02:29<01:25,  2.59it/s, training loss=0.199]\u001b[A\n",
      "Epoch 2:  64%|██████▎   | 384/604 [02:29<01:25,  2.58it/s, training loss=0.199]\u001b[A\n",
      "Epoch 2:  64%|██████▎   | 384/604 [02:29<01:25,  2.58it/s, training loss=0.188]\u001b[A\n",
      "Epoch 2:  64%|██████▎   | 385/604 [02:29<01:24,  2.59it/s, training loss=0.188]\u001b[A\n",
      "Epoch 2:  64%|██████▎   | 385/604 [02:29<01:24,  2.59it/s, training loss=0.091]\u001b[A\n",
      "Epoch 2:  64%|██████▍   | 386/604 [02:29<01:24,  2.59it/s, training loss=0.091]\u001b[A\n",
      "Epoch 2:  64%|██████▍   | 386/604 [02:30<01:24,  2.59it/s, training loss=0.199]\u001b[A\n",
      "Epoch 2:  64%|██████▍   | 387/604 [02:30<01:23,  2.59it/s, training loss=0.199]\u001b[A\n",
      "Epoch 2:  64%|██████▍   | 387/604 [02:30<01:23,  2.59it/s, training loss=0.256]\u001b[A\n",
      "Epoch 2:  64%|██████▍   | 388/604 [02:30<01:23,  2.59it/s, training loss=0.256]\u001b[A\n",
      "Epoch 2:  64%|██████▍   | 388/604 [02:31<01:23,  2.59it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  64%|██████▍   | 389/604 [02:31<01:23,  2.58it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  64%|██████▍   | 389/604 [02:31<01:23,  2.58it/s, training loss=0.206]\u001b[A\n",
      "Epoch 2:  65%|██████▍   | 390/604 [02:31<01:22,  2.59it/s, training loss=0.206]\u001b[A\n",
      "Epoch 2:  65%|██████▍   | 390/604 [02:31<01:22,  2.59it/s, training loss=0.136]\u001b[A\n",
      "Epoch 2:  65%|██████▍   | 391/604 [02:31<01:22,  2.59it/s, training loss=0.136]\u001b[A\n",
      "Epoch 2:  65%|██████▍   | 391/604 [02:32<01:22,  2.59it/s, training loss=0.236]\u001b[A\n",
      "Epoch 2:  65%|██████▍   | 392/604 [02:32<01:21,  2.59it/s, training loss=0.236]\u001b[A\n",
      "Epoch 2:  65%|██████▍   | 392/604 [02:32<01:21,  2.59it/s, training loss=0.295]\u001b[A\n",
      "Epoch 2:  65%|██████▌   | 393/604 [02:32<01:21,  2.59it/s, training loss=0.295]\u001b[A\n",
      "Epoch 2:  65%|██████▌   | 393/604 [02:33<01:21,  2.59it/s, training loss=0.286]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  65%|██████▌   | 394/604 [02:33<01:22,  2.54it/s, training loss=0.286]\u001b[A\n",
      "Epoch 2:  65%|██████▌   | 394/604 [02:33<01:22,  2.54it/s, training loss=0.236]\u001b[A\n",
      "Epoch 2:  65%|██████▌   | 395/604 [02:33<01:21,  2.55it/s, training loss=0.236]\u001b[A\n",
      "Epoch 2:  65%|██████▌   | 395/604 [02:33<01:21,  2.55it/s, training loss=0.186]\u001b[A\n",
      "Epoch 2:  66%|██████▌   | 396/604 [02:33<01:21,  2.56it/s, training loss=0.186]\u001b[A\n",
      "Epoch 2:  66%|██████▌   | 396/604 [02:34<01:21,  2.56it/s, training loss=0.234]\u001b[A\n",
      "Epoch 2:  66%|██████▌   | 397/604 [02:34<01:20,  2.56it/s, training loss=0.234]\u001b[A\n",
      "Epoch 2:  66%|██████▌   | 397/604 [02:34<01:20,  2.56it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  66%|██████▌   | 398/604 [02:34<01:20,  2.57it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  66%|██████▌   | 398/604 [02:34<01:20,  2.57it/s, training loss=0.243]\u001b[A\n",
      "Epoch 2:  66%|██████▌   | 399/604 [02:34<01:19,  2.58it/s, training loss=0.243]\u001b[A\n",
      "Epoch 2:  66%|██████▌   | 399/604 [02:35<01:19,  2.58it/s, training loss=0.132]\u001b[A\n",
      "Epoch 2:  66%|██████▌   | 400/604 [02:35<01:19,  2.58it/s, training loss=0.132]\u001b[A\n",
      "Epoch 2:  66%|██████▌   | 400/604 [02:35<01:19,  2.58it/s, training loss=0.285]\u001b[A\n",
      "Epoch 2:  66%|██████▋   | 401/604 [02:35<01:20,  2.54it/s, training loss=0.285]\u001b[A\n",
      "Epoch 2:  66%|██████▋   | 401/604 [02:36<01:20,  2.54it/s, training loss=0.149]\u001b[A\n",
      "Epoch 2:  67%|██████▋   | 402/604 [02:36<01:21,  2.48it/s, training loss=0.149]\u001b[A\n",
      "Epoch 2:  67%|██████▋   | 402/604 [02:36<01:21,  2.48it/s, training loss=0.193]\u001b[A\n",
      "Epoch 2:  67%|██████▋   | 403/604 [02:36<01:20,  2.50it/s, training loss=0.193]\u001b[A\n",
      "Epoch 2:  67%|██████▋   | 403/604 [02:36<01:20,  2.50it/s, training loss=0.185]\u001b[A\n",
      "Epoch 2:  67%|██████▋   | 404/604 [02:36<01:19,  2.52it/s, training loss=0.185]\u001b[A\n",
      "Epoch 2:  67%|██████▋   | 404/604 [02:37<01:19,  2.52it/s, training loss=0.176]\u001b[A\n",
      "Epoch 2:  67%|██████▋   | 405/604 [02:37<01:18,  2.54it/s, training loss=0.176]\u001b[A\n",
      "Epoch 2:  67%|██████▋   | 405/604 [02:37<01:18,  2.54it/s, training loss=0.237]\u001b[A\n",
      "Epoch 2:  67%|██████▋   | 406/604 [02:37<01:17,  2.56it/s, training loss=0.237]\u001b[A\n",
      "Epoch 2:  67%|██████▋   | 406/604 [02:38<01:17,  2.56it/s, training loss=0.244]\u001b[A\n",
      "Epoch 2:  67%|██████▋   | 407/604 [02:38<01:16,  2.56it/s, training loss=0.244]\u001b[A\n",
      "Epoch 2:  67%|██████▋   | 407/604 [02:38<01:16,  2.56it/s, training loss=0.209]\u001b[A\n",
      "Epoch 2:  68%|██████▊   | 408/604 [02:38<01:16,  2.57it/s, training loss=0.209]\u001b[A\n",
      "Epoch 2:  68%|██████▊   | 408/604 [02:38<01:16,  2.57it/s, training loss=0.201]\u001b[A\n",
      "Epoch 2:  68%|██████▊   | 409/604 [02:38<01:15,  2.57it/s, training loss=0.201]\u001b[A\n",
      "Epoch 2:  68%|██████▊   | 409/604 [02:39<01:15,  2.57it/s, training loss=0.226]\u001b[A\n",
      "Epoch 2:  68%|██████▊   | 410/604 [02:39<01:15,  2.58it/s, training loss=0.226]\u001b[A\n",
      "Epoch 2:  68%|██████▊   | 410/604 [02:39<01:15,  2.58it/s, training loss=0.244]\u001b[A\n",
      "Epoch 2:  68%|██████▊   | 411/604 [02:39<01:14,  2.58it/s, training loss=0.244]\u001b[A\n",
      "Epoch 2:  68%|██████▊   | 411/604 [02:40<01:14,  2.58it/s, training loss=0.150]\u001b[A\n",
      "Epoch 2:  68%|██████▊   | 412/604 [02:40<01:14,  2.59it/s, training loss=0.150]\u001b[A\n",
      "Epoch 2:  68%|██████▊   | 412/604 [02:40<01:14,  2.59it/s, training loss=0.243]\u001b[A\n",
      "Epoch 2:  68%|██████▊   | 413/604 [02:40<01:13,  2.59it/s, training loss=0.243]\u001b[A\n",
      "Epoch 2:  68%|██████▊   | 413/604 [02:40<01:13,  2.59it/s, training loss=0.142]\u001b[A\n",
      "Epoch 2:  69%|██████▊   | 414/604 [02:40<01:13,  2.59it/s, training loss=0.142]\u001b[A\n",
      "Epoch 2:  69%|██████▊   | 414/604 [02:41<01:13,  2.59it/s, training loss=0.135]\u001b[A\n",
      "Epoch 2:  69%|██████▊   | 415/604 [02:41<01:14,  2.54it/s, training loss=0.135]\u001b[A\n",
      "Epoch 2:  69%|██████▊   | 415/604 [02:41<01:14,  2.54it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:  69%|██████▉   | 416/604 [02:41<01:14,  2.54it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:  69%|██████▉   | 416/604 [02:42<01:14,  2.54it/s, training loss=0.201]\u001b[A\n",
      "Epoch 2:  69%|██████▉   | 417/604 [02:42<01:13,  2.53it/s, training loss=0.201]\u001b[A\n",
      "Epoch 2:  69%|██████▉   | 417/604 [02:42<01:13,  2.53it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  69%|██████▉   | 418/604 [02:42<01:12,  2.55it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  69%|██████▉   | 418/604 [02:42<01:12,  2.55it/s, training loss=0.243]\u001b[A\n",
      "Epoch 2:  69%|██████▉   | 419/604 [02:42<01:12,  2.56it/s, training loss=0.243]\u001b[A\n",
      "Epoch 2:  69%|██████▉   | 419/604 [02:43<01:12,  2.56it/s, training loss=0.166]\u001b[A\n",
      "Epoch 2:  70%|██████▉   | 420/604 [02:43<01:11,  2.57it/s, training loss=0.166]\u001b[A\n",
      "Epoch 2:  70%|██████▉   | 420/604 [02:43<01:11,  2.57it/s, training loss=0.196]\u001b[A\n",
      "Epoch 2:  70%|██████▉   | 421/604 [02:43<01:11,  2.57it/s, training loss=0.196]\u001b[A\n",
      "Epoch 2:  70%|██████▉   | 421/604 [02:43<01:11,  2.57it/s, training loss=0.148]\u001b[A\n",
      "Epoch 2:  70%|██████▉   | 422/604 [02:43<01:12,  2.52it/s, training loss=0.148]\u001b[A\n",
      "Epoch 2:  70%|██████▉   | 422/604 [02:44<01:12,  2.52it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2:  70%|███████   | 423/604 [02:44<01:12,  2.49it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2:  70%|███████   | 423/604 [02:44<01:12,  2.49it/s, training loss=0.209]\u001b[A\n",
      "Epoch 2:  70%|███████   | 424/604 [02:44<01:11,  2.51it/s, training loss=0.209]\u001b[A\n",
      "Epoch 2:  70%|███████   | 424/604 [02:45<01:11,  2.51it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  70%|███████   | 425/604 [02:45<01:10,  2.54it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  70%|███████   | 425/604 [02:45<01:10,  2.54it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2:  71%|███████   | 426/604 [02:45<01:09,  2.55it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2:  71%|███████   | 426/604 [02:45<01:09,  2.55it/s, training loss=0.152]\u001b[A\n",
      "Epoch 2:  71%|███████   | 427/604 [02:45<01:08,  2.57it/s, training loss=0.152]\u001b[A\n",
      "Epoch 2:  71%|███████   | 427/604 [02:46<01:08,  2.57it/s, training loss=0.157]\u001b[A\n",
      "Epoch 2:  71%|███████   | 428/604 [02:46<01:08,  2.57it/s, training loss=0.157]\u001b[A\n",
      "Epoch 2:  71%|███████   | 428/604 [02:46<01:08,  2.57it/s, training loss=0.161]\u001b[A\n",
      "Epoch 2:  71%|███████   | 429/604 [02:46<01:08,  2.57it/s, training loss=0.161]\u001b[A\n",
      "Epoch 2:  71%|███████   | 429/604 [02:47<01:08,  2.57it/s, training loss=0.213]\u001b[A\n",
      "Epoch 2:  71%|███████   | 430/604 [02:47<01:07,  2.58it/s, training loss=0.213]\u001b[A\n",
      "Epoch 2:  71%|███████   | 430/604 [02:47<01:07,  2.58it/s, training loss=0.174]\u001b[A\n",
      "Epoch 2:  71%|███████▏  | 431/604 [02:47<01:07,  2.58it/s, training loss=0.174]\u001b[A\n",
      "Epoch 2:  71%|███████▏  | 431/604 [02:47<01:07,  2.58it/s, training loss=0.186]\u001b[A\n",
      "Epoch 2:  72%|███████▏  | 432/604 [02:47<01:06,  2.58it/s, training loss=0.186]\u001b[A\n",
      "Epoch 2:  72%|███████▏  | 432/604 [02:48<01:06,  2.58it/s, training loss=0.196]\u001b[A\n",
      "Epoch 2:  72%|███████▏  | 433/604 [02:48<01:06,  2.57it/s, training loss=0.196]\u001b[A\n",
      "Epoch 2:  72%|███████▏  | 433/604 [02:48<01:06,  2.57it/s, training loss=0.143]\u001b[A\n",
      "Epoch 2:  72%|███████▏  | 434/604 [02:48<01:05,  2.58it/s, training loss=0.143]\u001b[A\n",
      "Epoch 2:  72%|███████▏  | 434/604 [02:49<01:05,  2.58it/s, training loss=0.158]\u001b[A\n",
      "Epoch 2:  72%|███████▏  | 435/604 [02:49<01:05,  2.58it/s, training loss=0.158]\u001b[A\n",
      "Epoch 2:  72%|███████▏  | 435/604 [02:49<01:05,  2.58it/s, training loss=0.199]\u001b[A\n",
      "Epoch 2:  72%|███████▏  | 436/604 [02:49<01:04,  2.59it/s, training loss=0.199]\u001b[A\n",
      "Epoch 2:  72%|███████▏  | 436/604 [02:49<01:04,  2.59it/s, training loss=0.108]\u001b[A\n",
      "Epoch 2:  72%|███████▏  | 437/604 [02:49<01:04,  2.59it/s, training loss=0.108]\u001b[A\n",
      "Epoch 2:  72%|███████▏  | 437/604 [02:50<01:04,  2.59it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2:  73%|███████▎  | 438/604 [02:50<01:04,  2.58it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2:  73%|███████▎  | 438/604 [02:50<01:04,  2.58it/s, training loss=0.144]\u001b[A\n",
      "Epoch 2:  73%|███████▎  | 439/604 [02:50<01:03,  2.58it/s, training loss=0.144]\u001b[A\n",
      "Epoch 2:  73%|███████▎  | 439/604 [02:50<01:03,  2.58it/s, training loss=0.203]\u001b[A\n",
      "Epoch 2:  73%|███████▎  | 440/604 [02:50<01:03,  2.59it/s, training loss=0.203]\u001b[A\n",
      "Epoch 2:  73%|███████▎  | 440/604 [02:51<01:03,  2.59it/s, training loss=0.197]\u001b[A\n",
      "Epoch 2:  73%|███████▎  | 441/604 [02:51<01:02,  2.59it/s, training loss=0.197]\u001b[A\n",
      "Epoch 2:  73%|███████▎  | 441/604 [02:51<01:02,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 2:  73%|███████▎  | 442/604 [02:51<01:02,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 2:  73%|███████▎  | 442/604 [02:52<01:02,  2.59it/s, training loss=0.207]\u001b[A\n",
      "Epoch 2:  73%|███████▎  | 443/604 [02:52<01:02,  2.59it/s, training loss=0.207]\u001b[A\n",
      "Epoch 2:  73%|███████▎  | 443/604 [02:52<01:02,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 2:  74%|███████▎  | 444/604 [02:52<01:01,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 2:  74%|███████▎  | 444/604 [02:52<01:01,  2.59it/s, training loss=0.196]\u001b[A\n",
      "Epoch 2:  74%|███████▎  | 445/604 [02:52<01:01,  2.58it/s, training loss=0.196]\u001b[A\n",
      "Epoch 2:  74%|███████▎  | 445/604 [02:53<01:01,  2.58it/s, training loss=0.146]\u001b[A\n",
      "Epoch 2:  74%|███████▍  | 446/604 [02:53<01:01,  2.58it/s, training loss=0.146]\u001b[A\n",
      "Epoch 2:  74%|███████▍  | 446/604 [02:53<01:01,  2.58it/s, training loss=0.201]\u001b[A\n",
      "Epoch 2:  74%|███████▍  | 447/604 [02:53<01:01,  2.57it/s, training loss=0.201]\u001b[A\n",
      "Epoch 2:  74%|███████▍  | 447/604 [02:54<01:01,  2.57it/s, training loss=0.233]\u001b[A\n",
      "Epoch 2:  74%|███████▍  | 448/604 [02:54<01:00,  2.58it/s, training loss=0.233]\u001b[A\n",
      "Epoch 2:  74%|███████▍  | 448/604 [02:54<01:00,  2.58it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:  74%|███████▍  | 449/604 [02:54<01:00,  2.54it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:  74%|███████▍  | 449/604 [02:54<01:00,  2.54it/s, training loss=0.202]\u001b[A\n",
      "Epoch 2:  75%|███████▍  | 450/604 [02:54<01:00,  2.54it/s, training loss=0.202]\u001b[A\n",
      "Epoch 2:  75%|███████▍  | 450/604 [02:55<01:00,  2.54it/s, training loss=0.309]\u001b[A\n",
      "Epoch 2:  75%|███████▍  | 451/604 [02:55<01:00,  2.53it/s, training loss=0.309]\u001b[A\n",
      "Epoch 2:  75%|███████▍  | 451/604 [02:55<01:00,  2.53it/s, training loss=0.147]\u001b[A\n",
      "Epoch 2:  75%|███████▍  | 452/604 [02:55<00:59,  2.55it/s, training loss=0.147]\u001b[A\n",
      "Epoch 2:  75%|███████▍  | 452/604 [02:56<00:59,  2.55it/s, training loss=0.109]\u001b[A\n",
      "Epoch 2:  75%|███████▌  | 453/604 [02:56<00:59,  2.52it/s, training loss=0.109]\u001b[A\n",
      "Epoch 2:  75%|███████▌  | 453/604 [02:56<00:59,  2.52it/s, training loss=0.148]\u001b[A\n",
      "Epoch 2:  75%|███████▌  | 454/604 [02:56<00:59,  2.52it/s, training loss=0.148]\u001b[A\n",
      "Epoch 2:  75%|███████▌  | 454/604 [02:56<00:59,  2.52it/s, training loss=0.300]\u001b[A\n",
      "Epoch 2:  75%|███████▌  | 455/604 [02:56<00:58,  2.54it/s, training loss=0.300]\u001b[A\n",
      "Epoch 2:  75%|███████▌  | 455/604 [02:57<00:58,  2.54it/s, training loss=0.228]\u001b[A\n",
      "Epoch 2:  75%|███████▌  | 456/604 [02:57<00:57,  2.56it/s, training loss=0.228]\u001b[A\n",
      "Epoch 2:  75%|███████▌  | 456/604 [02:57<00:57,  2.56it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  76%|███████▌  | 457/604 [02:57<00:57,  2.57it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  76%|███████▌  | 457/604 [02:58<00:57,  2.57it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  76%|███████▌  | 458/604 [02:58<00:56,  2.57it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  76%|███████▌  | 458/604 [02:58<00:56,  2.57it/s, training loss=0.207]\u001b[A\n",
      "Epoch 2:  76%|███████▌  | 459/604 [02:58<00:56,  2.58it/s, training loss=0.207]\u001b[A\n",
      "Epoch 2:  76%|███████▌  | 459/604 [02:58<00:56,  2.58it/s, training loss=0.174]\u001b[A\n",
      "Epoch 2:  76%|███████▌  | 460/604 [02:58<00:55,  2.58it/s, training loss=0.174]\u001b[A\n",
      "Epoch 2:  76%|███████▌  | 460/604 [02:59<00:55,  2.58it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2:  76%|███████▋  | 461/604 [02:59<00:55,  2.59it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2:  76%|███████▋  | 461/604 [02:59<00:55,  2.59it/s, training loss=0.184]\u001b[A\n",
      "Epoch 2:  76%|███████▋  | 462/604 [02:59<00:54,  2.59it/s, training loss=0.184]\u001b[A\n",
      "Epoch 2:  76%|███████▋  | 462/604 [02:59<00:54,  2.59it/s, training loss=0.188]\u001b[A\n",
      "Epoch 2:  77%|███████▋  | 463/604 [02:59<00:54,  2.59it/s, training loss=0.188]\u001b[A\n",
      "Epoch 2:  77%|███████▋  | 463/604 [03:00<00:54,  2.59it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  77%|███████▋  | 464/604 [03:00<00:54,  2.59it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  77%|███████▋  | 464/604 [03:00<00:54,  2.59it/s, training loss=0.160]\u001b[A\n",
      "Epoch 2:  77%|███████▋  | 465/604 [03:00<00:53,  2.59it/s, training loss=0.160]\u001b[A\n",
      "Epoch 2:  77%|███████▋  | 465/604 [03:01<00:53,  2.59it/s, training loss=0.175]\u001b[A\n",
      "Epoch 2:  77%|███████▋  | 466/604 [03:01<00:53,  2.59it/s, training loss=0.175]\u001b[A\n",
      "Epoch 2:  77%|███████▋  | 466/604 [03:01<00:53,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 2:  77%|███████▋  | 467/604 [03:01<00:53,  2.58it/s, training loss=0.161]\u001b[A\n",
      "Epoch 2:  77%|███████▋  | 467/604 [03:01<00:53,  2.58it/s, training loss=0.208]\u001b[A\n",
      "Epoch 2:  77%|███████▋  | 468/604 [03:01<00:52,  2.57it/s, training loss=0.208]\u001b[A\n",
      "Epoch 2:  77%|███████▋  | 468/604 [03:02<00:52,  2.57it/s, training loss=0.175]\u001b[A\n",
      "Epoch 2:  78%|███████▊  | 469/604 [03:02<00:52,  2.58it/s, training loss=0.175]\u001b[A\n",
      "Epoch 2:  78%|███████▊  | 469/604 [03:02<00:52,  2.58it/s, training loss=0.210]\u001b[A\n",
      "Epoch 2:  78%|███████▊  | 470/604 [03:02<00:51,  2.58it/s, training loss=0.210]\u001b[A\n",
      "Epoch 2:  78%|███████▊  | 470/604 [03:03<00:51,  2.58it/s, training loss=0.178]\u001b[A\n",
      "Epoch 2:  78%|███████▊  | 471/604 [03:03<00:51,  2.59it/s, training loss=0.178]\u001b[A\n",
      "Epoch 2:  78%|███████▊  | 471/604 [03:03<00:51,  2.59it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  78%|███████▊  | 472/604 [03:03<00:51,  2.59it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  78%|███████▊  | 472/604 [03:03<00:51,  2.59it/s, training loss=0.135]\u001b[A\n",
      "Epoch 2:  78%|███████▊  | 473/604 [03:03<00:50,  2.59it/s, training loss=0.135]\u001b[A\n",
      "Epoch 2:  78%|███████▊  | 473/604 [03:04<00:50,  2.59it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:  78%|███████▊  | 474/604 [03:04<00:50,  2.59it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:  78%|███████▊  | 474/604 [03:04<00:50,  2.59it/s, training loss=0.198]\u001b[A\n",
      "Epoch 2:  79%|███████▊  | 475/604 [03:04<00:49,  2.59it/s, training loss=0.198]\u001b[A\n",
      "Epoch 2:  79%|███████▊  | 475/604 [03:04<00:49,  2.59it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  79%|███████▉  | 476/604 [03:04<00:49,  2.58it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  79%|███████▉  | 476/604 [03:05<00:49,  2.58it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2:  79%|███████▉  | 477/604 [03:05<00:49,  2.59it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2:  79%|███████▉  | 477/604 [03:05<00:49,  2.59it/s, training loss=0.205]\u001b[A\n",
      "Epoch 2:  79%|███████▉  | 478/604 [03:05<00:48,  2.59it/s, training loss=0.205]\u001b[A\n",
      "Epoch 2:  79%|███████▉  | 478/604 [03:06<00:48,  2.59it/s, training loss=0.128]\u001b[A\n",
      "Epoch 2:  79%|███████▉  | 479/604 [03:06<00:49,  2.54it/s, training loss=0.128]\u001b[A\n",
      "Epoch 2:  79%|███████▉  | 479/604 [03:06<00:49,  2.54it/s, training loss=0.199]\u001b[A\n",
      "Epoch 2:  79%|███████▉  | 480/604 [03:06<00:48,  2.54it/s, training loss=0.199]\u001b[A\n",
      "Epoch 2:  79%|███████▉  | 480/604 [03:06<00:48,  2.54it/s, training loss=0.229]\u001b[A\n",
      "Epoch 2:  80%|███████▉  | 481/604 [03:06<00:48,  2.56it/s, training loss=0.229]\u001b[A\n",
      "Epoch 2:  80%|███████▉  | 481/604 [03:07<00:48,  2.56it/s, training loss=0.321]\u001b[A\n",
      "Epoch 2:  80%|███████▉  | 482/604 [03:07<00:47,  2.57it/s, training loss=0.321]\u001b[A\n",
      "Epoch 2:  80%|███████▉  | 482/604 [03:07<00:47,  2.57it/s, training loss=0.211]\u001b[A\n",
      "Epoch 2:  80%|███████▉  | 483/604 [03:07<00:46,  2.58it/s, training loss=0.211]\u001b[A\n",
      "Epoch 2:  80%|███████▉  | 483/604 [03:08<00:46,  2.58it/s, training loss=0.243]\u001b[A\n",
      "Epoch 2:  80%|████████  | 484/604 [03:08<00:46,  2.58it/s, training loss=0.243]\u001b[A\n",
      "Epoch 2:  80%|████████  | 484/604 [03:08<00:46,  2.58it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  80%|████████  | 485/604 [03:08<00:46,  2.58it/s, training loss=0.189]\u001b[A\n",
      "Epoch 2:  80%|████████  | 485/604 [03:08<00:46,  2.58it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  80%|████████  | 486/604 [03:08<00:45,  2.59it/s, training loss=0.194]\u001b[A\n",
      "Epoch 2:  80%|████████  | 486/604 [03:09<00:45,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 2:  81%|████████  | 487/604 [03:09<00:45,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 2:  81%|████████  | 487/604 [03:09<00:45,  2.59it/s, training loss=0.115]\u001b[A\n",
      "Epoch 2:  81%|████████  | 488/604 [03:09<00:44,  2.59it/s, training loss=0.115]\u001b[A\n",
      "Epoch 2:  81%|████████  | 488/604 [03:10<00:44,  2.59it/s, training loss=0.175]\u001b[A\n",
      "Epoch 2:  81%|████████  | 489/604 [03:10<00:44,  2.59it/s, training loss=0.175]\u001b[A\n",
      "Epoch 2:  81%|████████  | 489/604 [03:10<00:44,  2.59it/s, training loss=0.321]\u001b[A\n",
      "Epoch 2:  81%|████████  | 490/604 [03:10<00:44,  2.59it/s, training loss=0.321]\u001b[A\n",
      "Epoch 2:  81%|████████  | 490/604 [03:10<00:44,  2.59it/s, training loss=0.179]\u001b[A\n",
      "Epoch 2:  81%|████████▏ | 491/604 [03:10<00:43,  2.59it/s, training loss=0.179]\u001b[A\n",
      "Epoch 2:  81%|████████▏ | 491/604 [03:11<00:43,  2.59it/s, training loss=0.173]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  81%|████████▏ | 492/604 [03:11<00:43,  2.59it/s, training loss=0.173]\u001b[A\n",
      "Epoch 2:  81%|████████▏ | 492/604 [03:11<00:43,  2.59it/s, training loss=0.221]\u001b[A\n",
      "Epoch 2:  82%|████████▏ | 493/604 [03:11<00:42,  2.59it/s, training loss=0.221]\u001b[A\n",
      "Epoch 2:  82%|████████▏ | 493/604 [03:11<00:42,  2.59it/s, training loss=0.264]\u001b[A\n",
      "Epoch 2:  82%|████████▏ | 494/604 [03:11<00:42,  2.59it/s, training loss=0.264]\u001b[A\n",
      "Epoch 2:  82%|████████▏ | 494/604 [03:12<00:42,  2.59it/s, training loss=0.139]\u001b[A\n",
      "Epoch 2:  82%|████████▏ | 495/604 [03:12<00:42,  2.59it/s, training loss=0.139]\u001b[A\n",
      "Epoch 2:  82%|████████▏ | 495/604 [03:12<00:42,  2.59it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  82%|████████▏ | 496/604 [03:12<00:41,  2.59it/s, training loss=0.204]\u001b[A\n",
      "Epoch 2:  82%|████████▏ | 496/604 [03:13<00:41,  2.59it/s, training loss=0.133]\u001b[A\n",
      "Epoch 2:  82%|████████▏ | 497/604 [03:13<00:41,  2.59it/s, training loss=0.133]\u001b[A\n",
      "Epoch 2:  82%|████████▏ | 497/604 [03:13<00:41,  2.59it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  82%|████████▏ | 498/604 [03:13<00:40,  2.59it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  82%|████████▏ | 498/604 [03:13<00:40,  2.59it/s, training loss=0.222]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 499/604 [03:13<00:40,  2.59it/s, training loss=0.222]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 499/604 [03:14<00:40,  2.59it/s, training loss=0.234]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 500/604 [03:14<00:40,  2.60it/s, training loss=0.234]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 500/604 [03:14<00:40,  2.60it/s, training loss=0.137]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 501/604 [03:14<00:39,  2.60it/s, training loss=0.137]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 501/604 [03:15<00:39,  2.60it/s, training loss=0.275]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 502/604 [03:15<00:39,  2.58it/s, training loss=0.275]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 502/604 [03:15<00:39,  2.58it/s, training loss=0.198]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 503/604 [03:15<00:39,  2.59it/s, training loss=0.198]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 503/604 [03:15<00:39,  2.59it/s, training loss=0.213]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 504/604 [03:15<00:38,  2.59it/s, training loss=0.213]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 504/604 [03:16<00:38,  2.59it/s, training loss=0.184]\u001b[A\n",
      "Epoch 2:  84%|████████▎ | 505/604 [03:16<00:38,  2.58it/s, training loss=0.184]\u001b[A\n",
      "Epoch 2:  84%|████████▎ | 505/604 [03:16<00:38,  2.58it/s, training loss=0.161]\u001b[A\n",
      "Epoch 2:  84%|████████▍ | 506/604 [03:16<00:38,  2.58it/s, training loss=0.161]\u001b[A\n",
      "Epoch 2:  84%|████████▍ | 506/604 [03:16<00:38,  2.58it/s, training loss=0.277]\u001b[A\n",
      "Epoch 2:  84%|████████▍ | 507/604 [03:16<00:38,  2.54it/s, training loss=0.277]\u001b[A\n",
      "Epoch 2:  84%|████████▍ | 507/604 [03:17<00:38,  2.54it/s, training loss=0.157]\u001b[A\n",
      "Epoch 2:  84%|████████▍ | 508/604 [03:17<00:38,  2.49it/s, training loss=0.157]\u001b[A\n",
      "Epoch 2:  84%|████████▍ | 508/604 [03:17<00:38,  2.49it/s, training loss=0.195]\u001b[A\n",
      "Epoch 2:  84%|████████▍ | 509/604 [03:17<00:37,  2.52it/s, training loss=0.195]\u001b[A\n",
      "Epoch 2:  84%|████████▍ | 509/604 [03:18<00:37,  2.52it/s, training loss=0.218]\u001b[A\n",
      "Epoch 2:  84%|████████▍ | 510/604 [03:18<00:37,  2.54it/s, training loss=0.218]\u001b[A\n",
      "Epoch 2:  84%|████████▍ | 510/604 [03:18<00:37,  2.54it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  85%|████████▍ | 511/604 [03:18<00:36,  2.56it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  85%|████████▍ | 511/604 [03:18<00:36,  2.56it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:  85%|████████▍ | 512/604 [03:18<00:35,  2.56it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:  85%|████████▍ | 512/604 [03:19<00:35,  2.56it/s, training loss=0.153]\u001b[A\n",
      "Epoch 2:  85%|████████▍ | 513/604 [03:19<00:35,  2.57it/s, training loss=0.153]\u001b[A\n",
      "Epoch 2:  85%|████████▍ | 513/604 [03:19<00:35,  2.57it/s, training loss=0.195]\u001b[A\n",
      "Epoch 2:  85%|████████▌ | 514/604 [03:19<00:34,  2.58it/s, training loss=0.195]\u001b[A\n",
      "Epoch 2:  85%|████████▌ | 514/604 [03:20<00:34,  2.58it/s, training loss=0.196]\u001b[A\n",
      "Epoch 2:  85%|████████▌ | 515/604 [03:20<00:34,  2.57it/s, training loss=0.196]\u001b[A\n",
      "Epoch 2:  85%|████████▌ | 515/604 [03:20<00:34,  2.57it/s, training loss=0.150]\u001b[A\n",
      "Epoch 2:  85%|████████▌ | 516/604 [03:20<00:34,  2.57it/s, training loss=0.150]\u001b[A\n",
      "Epoch 2:  85%|████████▌ | 516/604 [03:20<00:34,  2.57it/s, training loss=0.227]\u001b[A\n",
      "Epoch 2:  86%|████████▌ | 517/604 [03:20<00:33,  2.56it/s, training loss=0.227]\u001b[A\n",
      "Epoch 2:  86%|████████▌ | 517/604 [03:21<00:33,  2.56it/s, training loss=0.206]\u001b[A\n",
      "Epoch 2:  86%|████████▌ | 518/604 [03:21<00:33,  2.56it/s, training loss=0.206]\u001b[A\n",
      "Epoch 2:  86%|████████▌ | 518/604 [03:21<00:33,  2.56it/s, training loss=0.178]\u001b[A\n",
      "Epoch 2:  86%|████████▌ | 519/604 [03:21<00:33,  2.57it/s, training loss=0.178]\u001b[A\n",
      "Epoch 2:  86%|████████▌ | 519/604 [03:22<00:33,  2.57it/s, training loss=0.164]\u001b[A\n",
      "Epoch 2:  86%|████████▌ | 520/604 [03:22<00:32,  2.58it/s, training loss=0.164]\u001b[A\n",
      "Epoch 2:  86%|████████▌ | 520/604 [03:22<00:32,  2.58it/s, training loss=0.175]\u001b[A\n",
      "Epoch 2:  86%|████████▋ | 521/604 [03:22<00:32,  2.59it/s, training loss=0.175]\u001b[A\n",
      "Epoch 2:  86%|████████▋ | 521/604 [03:22<00:32,  2.59it/s, training loss=0.203]\u001b[A\n",
      "Epoch 2:  86%|████████▋ | 522/604 [03:22<00:31,  2.59it/s, training loss=0.203]\u001b[A\n",
      "Epoch 2:  86%|████████▋ | 522/604 [03:23<00:31,  2.59it/s, training loss=0.149]\u001b[A\n",
      "Epoch 2:  87%|████████▋ | 523/604 [03:23<00:31,  2.59it/s, training loss=0.149]\u001b[A\n",
      "Epoch 2:  87%|████████▋ | 523/604 [03:23<00:31,  2.59it/s, training loss=0.134]\u001b[A\n",
      "Epoch 2:  87%|████████▋ | 524/604 [03:23<00:30,  2.59it/s, training loss=0.134]\u001b[A\n",
      "Epoch 2:  87%|████████▋ | 524/604 [03:23<00:30,  2.59it/s, training loss=0.222]\u001b[A\n",
      "Epoch 2:  87%|████████▋ | 525/604 [03:23<00:30,  2.59it/s, training loss=0.222]\u001b[A\n",
      "Epoch 2:  87%|████████▋ | 525/604 [03:24<00:30,  2.59it/s, training loss=0.121]\u001b[A\n",
      "Epoch 2:  87%|████████▋ | 526/604 [03:24<00:30,  2.59it/s, training loss=0.121]\u001b[A\n",
      "Epoch 2:  87%|████████▋ | 526/604 [03:24<00:30,  2.59it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:  87%|████████▋ | 527/604 [03:24<00:29,  2.59it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:  87%|████████▋ | 527/604 [03:25<00:29,  2.59it/s, training loss=0.134]\u001b[A\n",
      "Epoch 2:  87%|████████▋ | 528/604 [03:25<00:29,  2.59it/s, training loss=0.134]\u001b[A\n",
      "Epoch 2:  87%|████████▋ | 528/604 [03:25<00:29,  2.59it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  88%|████████▊ | 529/604 [03:25<00:28,  2.59it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2:  88%|████████▊ | 529/604 [03:25<00:28,  2.59it/s, training loss=0.173]\u001b[A\n",
      "Epoch 2:  88%|████████▊ | 530/604 [03:25<00:28,  2.59it/s, training loss=0.173]\u001b[A\n",
      "Epoch 2:  88%|████████▊ | 530/604 [03:26<00:28,  2.59it/s, training loss=0.191]\u001b[A\n",
      "Epoch 2:  88%|████████▊ | 531/604 [03:26<00:28,  2.59it/s, training loss=0.191]\u001b[A\n",
      "Epoch 2:  88%|████████▊ | 531/604 [03:26<00:28,  2.59it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:  88%|████████▊ | 532/604 [03:26<00:27,  2.59it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:  88%|████████▊ | 532/604 [03:27<00:27,  2.59it/s, training loss=0.145]\u001b[A\n",
      "Epoch 2:  88%|████████▊ | 533/604 [03:27<00:27,  2.59it/s, training loss=0.145]\u001b[A\n",
      "Epoch 2:  88%|████████▊ | 533/604 [03:27<00:27,  2.59it/s, training loss=0.244]\u001b[A\n",
      "Epoch 2:  88%|████████▊ | 534/604 [03:27<00:27,  2.59it/s, training loss=0.244]\u001b[A\n",
      "Epoch 2:  88%|████████▊ | 534/604 [03:27<00:27,  2.59it/s, training loss=0.250]\u001b[A\n",
      "Epoch 2:  89%|████████▊ | 535/604 [03:27<00:27,  2.55it/s, training loss=0.250]\u001b[A\n",
      "Epoch 2:  89%|████████▊ | 535/604 [03:28<00:27,  2.55it/s, training loss=0.217]\u001b[A\n",
      "Epoch 2:  89%|████████▊ | 536/604 [03:28<00:26,  2.52it/s, training loss=0.217]\u001b[A\n",
      "Epoch 2:  89%|████████▊ | 536/604 [03:28<00:26,  2.52it/s, training loss=0.183]\u001b[A\n",
      "Epoch 2:  89%|████████▉ | 537/604 [03:28<00:26,  2.53it/s, training loss=0.183]\u001b[A\n",
      "Epoch 2:  89%|████████▉ | 537/604 [03:29<00:26,  2.53it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:  89%|████████▉ | 538/604 [03:29<00:25,  2.55it/s, training loss=0.192]\u001b[A\n",
      "Epoch 2:  89%|████████▉ | 538/604 [03:29<00:25,  2.55it/s, training loss=0.168]\u001b[A\n",
      "Epoch 2:  89%|████████▉ | 539/604 [03:29<00:25,  2.56it/s, training loss=0.168]\u001b[A\n",
      "Epoch 2:  89%|████████▉ | 539/604 [03:29<00:25,  2.56it/s, training loss=0.148]\u001b[A\n",
      "Epoch 2:  89%|████████▉ | 540/604 [03:29<00:24,  2.57it/s, training loss=0.148]\u001b[A\n",
      "Epoch 2:  89%|████████▉ | 540/604 [03:30<00:24,  2.57it/s, training loss=0.156]\u001b[A\n",
      "Epoch 2:  90%|████████▉ | 541/604 [03:30<00:24,  2.58it/s, training loss=0.156]\u001b[A\n",
      "Epoch 2:  90%|████████▉ | 541/604 [03:30<00:24,  2.58it/s, training loss=0.218]\u001b[A\n",
      "Epoch 2:  90%|████████▉ | 542/604 [03:30<00:24,  2.58it/s, training loss=0.218]\u001b[A\n",
      "Epoch 2:  90%|████████▉ | 542/604 [03:30<00:24,  2.58it/s, training loss=0.195]\u001b[A\n",
      "Epoch 2:  90%|████████▉ | 543/604 [03:30<00:23,  2.58it/s, training loss=0.195]\u001b[A\n",
      "Epoch 2:  90%|████████▉ | 543/604 [03:31<00:23,  2.58it/s, training loss=0.262]\u001b[A\n",
      "Epoch 2:  90%|█████████ | 544/604 [03:31<00:23,  2.58it/s, training loss=0.262]\u001b[A\n",
      "Epoch 2:  90%|█████████ | 544/604 [03:31<00:23,  2.58it/s, training loss=0.169]\u001b[A\n",
      "Epoch 2:  90%|█████████ | 545/604 [03:31<00:22,  2.58it/s, training loss=0.169]\u001b[A\n",
      "Epoch 2:  90%|█████████ | 545/604 [03:32<00:22,  2.58it/s, training loss=0.159]\u001b[A\n",
      "Epoch 2:  90%|█████████ | 546/604 [03:32<00:22,  2.56it/s, training loss=0.159]\u001b[A\n",
      "Epoch 2:  90%|█████████ | 546/604 [03:32<00:22,  2.56it/s, training loss=0.264]\u001b[A\n",
      "Epoch 2:  91%|█████████ | 547/604 [03:32<00:22,  2.57it/s, training loss=0.264]\u001b[A\n",
      "Epoch 2:  91%|█████████ | 547/604 [03:32<00:22,  2.57it/s, training loss=0.208]\u001b[A\n",
      "Epoch 2:  91%|█████████ | 548/604 [03:32<00:21,  2.58it/s, training loss=0.208]\u001b[A\n",
      "Epoch 2:  91%|█████████ | 548/604 [03:33<00:21,  2.58it/s, training loss=0.220]\u001b[A\n",
      "Epoch 2:  91%|█████████ | 549/604 [03:33<00:21,  2.58it/s, training loss=0.220]\u001b[A\n",
      "Epoch 2:  91%|█████████ | 549/604 [03:33<00:21,  2.58it/s, training loss=0.131]\u001b[A\n",
      "Epoch 2:  91%|█████████ | 550/604 [03:33<00:20,  2.59it/s, training loss=0.131]\u001b[A\n",
      "Epoch 2:  91%|█████████ | 550/604 [03:34<00:20,  2.59it/s, training loss=0.230]\u001b[A\n",
      "Epoch 2:  91%|█████████ | 551/604 [03:34<00:20,  2.59it/s, training loss=0.230]\u001b[A\n",
      "Epoch 2:  91%|█████████ | 551/604 [03:34<00:20,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 2:  91%|█████████▏| 552/604 [03:34<00:20,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 2:  91%|█████████▏| 552/604 [03:34<00:20,  2.59it/s, training loss=0.141]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 553/604 [03:34<00:19,  2.59it/s, training loss=0.141]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 553/604 [03:35<00:19,  2.59it/s, training loss=0.178]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 554/604 [03:35<00:19,  2.59it/s, training loss=0.178]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 554/604 [03:35<00:19,  2.59it/s, training loss=0.202]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 555/604 [03:35<00:19,  2.52it/s, training loss=0.202]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 555/604 [03:36<00:19,  2.52it/s, training loss=0.214]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 556/604 [03:36<00:19,  2.46it/s, training loss=0.214]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 556/604 [03:36<00:19,  2.46it/s, training loss=0.200]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 557/604 [03:36<00:19,  2.44it/s, training loss=0.200]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 557/604 [03:36<00:19,  2.44it/s, training loss=0.197]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 558/604 [03:36<00:18,  2.47it/s, training loss=0.197]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 558/604 [03:37<00:18,  2.47it/s, training loss=0.216]\u001b[A\n",
      "Epoch 2:  93%|█████████▎| 559/604 [03:37<00:17,  2.51it/s, training loss=0.216]\u001b[A\n",
      "Epoch 2:  93%|█████████▎| 559/604 [03:37<00:17,  2.51it/s, training loss=0.235]\u001b[A\n",
      "Epoch 2:  93%|█████████▎| 560/604 [03:37<00:17,  2.53it/s, training loss=0.235]\u001b[A\n",
      "Epoch 2:  93%|█████████▎| 560/604 [03:38<00:17,  2.53it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:  93%|█████████▎| 561/604 [03:38<00:16,  2.55it/s, training loss=0.171]\u001b[A\n",
      "Epoch 2:  93%|█████████▎| 561/604 [03:38<00:16,  2.55it/s, training loss=0.217]\u001b[A\n",
      "Epoch 2:  93%|█████████▎| 562/604 [03:38<00:16,  2.56it/s, training loss=0.217]\u001b[A\n",
      "Epoch 2:  93%|█████████▎| 562/604 [03:38<00:16,  2.56it/s, training loss=0.142]\u001b[A\n",
      "Epoch 2:  93%|█████████▎| 563/604 [03:38<00:15,  2.57it/s, training loss=0.142]\u001b[A\n",
      "Epoch 2:  93%|█████████▎| 563/604 [03:39<00:15,  2.57it/s, training loss=0.140]\u001b[A\n",
      "Epoch 2:  93%|█████████▎| 564/604 [03:39<00:15,  2.54it/s, training loss=0.140]\u001b[A\n",
      "Epoch 2:  93%|█████████▎| 564/604 [03:39<00:15,  2.54it/s, training loss=0.214]\u001b[A\n",
      "Epoch 2:  94%|█████████▎| 565/604 [03:39<00:15,  2.53it/s, training loss=0.214]\u001b[A\n",
      "Epoch 2:  94%|█████████▎| 565/604 [03:40<00:15,  2.53it/s, training loss=0.283]\u001b[A\n",
      "Epoch 2:  94%|█████████▎| 566/604 [03:40<00:14,  2.55it/s, training loss=0.283]\u001b[A\n",
      "Epoch 2:  94%|█████████▎| 566/604 [03:40<00:14,  2.55it/s, training loss=0.231]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 567/604 [03:40<00:14,  2.56it/s, training loss=0.231]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 567/604 [03:40<00:14,  2.56it/s, training loss=0.211]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 568/604 [03:40<00:13,  2.57it/s, training loss=0.211]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 568/604 [03:41<00:13,  2.57it/s, training loss=0.165]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 569/604 [03:41<00:13,  2.57it/s, training loss=0.165]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 569/604 [03:41<00:13,  2.57it/s, training loss=0.190]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 570/604 [03:41<00:13,  2.53it/s, training loss=0.190]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 570/604 [03:41<00:13,  2.53it/s, training loss=0.241]\u001b[A\n",
      "Epoch 2:  95%|█████████▍| 571/604 [03:41<00:12,  2.54it/s, training loss=0.241]\u001b[A\n",
      "Epoch 2:  95%|█████████▍| 571/604 [03:42<00:12,  2.54it/s, training loss=0.195]\u001b[A\n",
      "Epoch 2:  95%|█████████▍| 572/604 [03:42<00:12,  2.56it/s, training loss=0.195]\u001b[A\n",
      "Epoch 2:  95%|█████████▍| 572/604 [03:42<00:12,  2.56it/s, training loss=0.101]\u001b[A\n",
      "Epoch 2:  95%|█████████▍| 573/604 [03:42<00:12,  2.57it/s, training loss=0.101]\u001b[A\n",
      "Epoch 2:  95%|█████████▍| 573/604 [03:43<00:12,  2.57it/s, training loss=0.142]\u001b[A\n",
      "Epoch 2:  95%|█████████▌| 574/604 [03:43<00:11,  2.58it/s, training loss=0.142]\u001b[A\n",
      "Epoch 2:  95%|█████████▌| 574/604 [03:43<00:11,  2.58it/s, training loss=0.206]\u001b[A\n",
      "Epoch 2:  95%|█████████▌| 575/604 [03:43<00:11,  2.58it/s, training loss=0.206]\u001b[A\n",
      "Epoch 2:  95%|█████████▌| 575/604 [03:43<00:11,  2.58it/s, training loss=0.142]\u001b[A\n",
      "Epoch 2:  95%|█████████▌| 576/604 [03:43<00:10,  2.58it/s, training loss=0.142]\u001b[A\n",
      "Epoch 2:  95%|█████████▌| 576/604 [03:44<00:10,  2.58it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  96%|█████████▌| 577/604 [03:44<00:10,  2.58it/s, training loss=0.223]\u001b[A\n",
      "Epoch 2:  96%|█████████▌| 577/604 [03:44<00:10,  2.58it/s, training loss=0.358]\u001b[A\n",
      "Epoch 2:  96%|█████████▌| 578/604 [03:44<00:10,  2.59it/s, training loss=0.358]\u001b[A\n",
      "Epoch 2:  96%|█████████▌| 578/604 [03:45<00:10,  2.59it/s, training loss=0.160]\u001b[A\n",
      "Epoch 2:  96%|█████████▌| 579/604 [03:45<00:09,  2.59it/s, training loss=0.160]\u001b[A\n",
      "Epoch 2:  96%|█████████▌| 579/604 [03:45<00:09,  2.59it/s, training loss=0.220]\u001b[A\n",
      "Epoch 2:  96%|█████████▌| 580/604 [03:45<00:09,  2.59it/s, training loss=0.220]\u001b[A\n",
      "Epoch 2:  96%|█████████▌| 580/604 [03:45<00:09,  2.59it/s, training loss=0.237]\u001b[A\n",
      "Epoch 2:  96%|█████████▌| 581/604 [03:45<00:08,  2.59it/s, training loss=0.237]\u001b[A\n",
      "Epoch 2:  96%|█████████▌| 581/604 [03:46<00:08,  2.59it/s, training loss=0.235]\u001b[A\n",
      "Epoch 2:  96%|█████████▋| 582/604 [03:46<00:08,  2.58it/s, training loss=0.235]\u001b[A\n",
      "Epoch 2:  96%|█████████▋| 582/604 [03:46<00:08,  2.58it/s, training loss=0.185]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 583/604 [03:46<00:08,  2.56it/s, training loss=0.185]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 583/604 [03:47<00:08,  2.56it/s, training loss=0.196]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 584/604 [03:47<00:07,  2.57it/s, training loss=0.196]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 584/604 [03:47<00:07,  2.57it/s, training loss=0.143]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 585/604 [03:47<00:07,  2.58it/s, training loss=0.143]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 585/604 [03:47<00:07,  2.58it/s, training loss=0.170]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 586/604 [03:47<00:06,  2.58it/s, training loss=0.170]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 586/604 [03:48<00:06,  2.58it/s, training loss=0.184]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 587/604 [03:48<00:06,  2.59it/s, training loss=0.184]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 587/604 [03:48<00:06,  2.59it/s, training loss=0.157]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 588/604 [03:48<00:06,  2.57it/s, training loss=0.157]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 588/604 [03:48<00:06,  2.57it/s, training loss=0.107]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 589/604 [03:48<00:05,  2.55it/s, training loss=0.107]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 589/604 [03:49<00:05,  2.55it/s, training loss=0.232]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  98%|█████████▊| 590/604 [03:49<00:05,  2.56it/s, training loss=0.232]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 590/604 [03:49<00:05,  2.56it/s, training loss=0.147]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 591/604 [03:49<00:05,  2.57it/s, training loss=0.147]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 591/604 [03:50<00:05,  2.57it/s, training loss=0.229]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 592/604 [03:50<00:04,  2.55it/s, training loss=0.229]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 592/604 [03:50<00:04,  2.55it/s, training loss=0.132]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 593/604 [03:50<00:04,  2.55it/s, training loss=0.132]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 593/604 [03:50<00:04,  2.55it/s, training loss=0.153]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 594/604 [03:50<00:03,  2.56it/s, training loss=0.153]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 594/604 [03:51<00:03,  2.56it/s, training loss=0.188]\u001b[A\n",
      "Epoch 2:  99%|█████████▊| 595/604 [03:51<00:03,  2.57it/s, training loss=0.188]\u001b[A\n",
      "Epoch 2:  99%|█████████▊| 595/604 [03:51<00:03,  2.57it/s, training loss=0.217]\u001b[A\n",
      "Epoch 2:  99%|█████████▊| 596/604 [03:51<00:03,  2.58it/s, training loss=0.217]\u001b[A\n",
      "Epoch 2:  99%|█████████▊| 596/604 [03:52<00:03,  2.58it/s, training loss=0.240]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 597/604 [03:52<00:02,  2.58it/s, training loss=0.240]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 597/604 [03:52<00:02,  2.58it/s, training loss=0.307]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 598/604 [03:52<00:02,  2.58it/s, training loss=0.307]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 598/604 [03:52<00:02,  2.58it/s, training loss=0.132]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 599/604 [03:52<00:01,  2.59it/s, training loss=0.132]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 599/604 [03:53<00:01,  2.59it/s, training loss=0.168]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 600/604 [03:53<00:01,  2.59it/s, training loss=0.168]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 600/604 [03:53<00:01,  2.59it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 601/604 [03:53<00:01,  2.59it/s, training loss=0.182]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 601/604 [03:54<00:01,  2.59it/s, training loss=0.254]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 602/604 [03:54<00:00,  2.59it/s, training loss=0.254]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 602/604 [03:54<00:00,  2.59it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 603/604 [03:54<00:00,  2.59it/s, training loss=0.167]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 603/604 [03:54<00:00,  2.59it/s, training loss=0.157]\u001b[A\n",
      "Epoch 2: 100%|██████████| 604/604 [03:54<00:00,  2.78it/s, training loss=0.157]\u001b[A\n",
      " 33%|███▎      | 1/3 [07:59<08:08, 244.31s/it]                                 \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch {epoch}\n",
      "Training loss: 0.5749824955467356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [08:07<04:03, 243.93s/it]\n",
      "Epoch 3:   0%|          | 0/604 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.6591562006403419\n",
      "F1 Score (weighted): 0.708864509696215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:   0%|          | 0/604 [00:00<?, ?it/s, training loss=0.116]\u001b[A\n",
      "Epoch 3:   0%|          | 1/604 [00:00<03:52,  2.60it/s, training loss=0.116]\u001b[A\n",
      "Epoch 3:   0%|          | 1/604 [00:00<03:52,  2.60it/s, training loss=0.197]\u001b[A\n",
      "Epoch 3:   0%|          | 2/604 [00:00<03:51,  2.60it/s, training loss=0.197]\u001b[A\n",
      "Epoch 3:   0%|          | 2/604 [00:01<03:51,  2.60it/s, training loss=0.185]\u001b[A\n",
      "Epoch 3:   0%|          | 3/604 [00:01<03:51,  2.60it/s, training loss=0.185]\u001b[A\n",
      "Epoch 3:   0%|          | 3/604 [00:01<03:51,  2.60it/s, training loss=0.191]\u001b[A\n",
      "Epoch 3:   1%|          | 4/604 [00:01<03:51,  2.60it/s, training loss=0.191]\u001b[A\n",
      "Epoch 3:   1%|          | 4/604 [00:01<03:51,  2.60it/s, training loss=0.109]\u001b[A\n",
      "Epoch 3:   1%|          | 5/604 [00:01<03:50,  2.60it/s, training loss=0.109]\u001b[A\n",
      "Epoch 3:   1%|          | 5/604 [00:02<03:50,  2.60it/s, training loss=0.175]\u001b[A\n",
      "Epoch 3:   1%|          | 6/604 [00:02<03:50,  2.60it/s, training loss=0.175]\u001b[A\n",
      "Epoch 3:   1%|          | 6/604 [00:02<03:50,  2.60it/s, training loss=0.157]\u001b[A\n",
      "Epoch 3:   1%|          | 7/604 [00:02<03:50,  2.59it/s, training loss=0.157]\u001b[A\n",
      "Epoch 3:   1%|          | 7/604 [00:03<03:50,  2.59it/s, training loss=0.151]\u001b[A\n",
      "Epoch 3:   1%|▏         | 8/604 [00:03<03:51,  2.58it/s, training loss=0.151]\u001b[A\n",
      "Epoch 3:   1%|▏         | 8/604 [00:03<03:51,  2.58it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:   1%|▏         | 9/604 [00:03<03:51,  2.57it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:   1%|▏         | 9/604 [00:03<03:51,  2.57it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:   2%|▏         | 10/604 [00:03<03:50,  2.58it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:   2%|▏         | 10/604 [00:04<03:50,  2.58it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:   2%|▏         | 11/604 [00:04<03:49,  2.58it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:   2%|▏         | 11/604 [00:04<03:49,  2.58it/s, training loss=0.189]\u001b[A\n",
      "Epoch 3:   2%|▏         | 12/604 [00:04<03:49,  2.58it/s, training loss=0.189]\u001b[A\n",
      "Epoch 3:   2%|▏         | 12/604 [00:05<03:49,  2.58it/s, training loss=0.124]\u001b[A\n",
      "Epoch 3:   2%|▏         | 13/604 [00:05<03:48,  2.58it/s, training loss=0.124]\u001b[A\n",
      "Epoch 3:   2%|▏         | 13/604 [00:05<03:48,  2.58it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:   2%|▏         | 14/604 [00:05<03:48,  2.59it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:   2%|▏         | 14/604 [00:05<03:48,  2.59it/s, training loss=0.158]\u001b[A\n",
      "Epoch 3:   2%|▏         | 15/604 [00:05<03:47,  2.59it/s, training loss=0.158]\u001b[A\n",
      "Epoch 3:   2%|▏         | 15/604 [00:06<03:47,  2.59it/s, training loss=0.088]\u001b[A\n",
      "Epoch 3:   3%|▎         | 16/604 [00:06<03:47,  2.59it/s, training loss=0.088]\u001b[A\n",
      "Epoch 3:   3%|▎         | 16/604 [00:06<03:47,  2.59it/s, training loss=0.192]\u001b[A\n",
      "Epoch 3:   3%|▎         | 17/604 [00:06<03:46,  2.59it/s, training loss=0.192]\u001b[A\n",
      "Epoch 3:   3%|▎         | 17/604 [00:06<03:46,  2.59it/s, training loss=0.214]\u001b[A\n",
      "Epoch 3:   3%|▎         | 18/604 [00:06<03:46,  2.59it/s, training loss=0.214]\u001b[A\n",
      "Epoch 3:   3%|▎         | 18/604 [00:07<03:46,  2.59it/s, training loss=0.196]\u001b[A\n",
      "Epoch 3:   3%|▎         | 19/604 [00:07<03:45,  2.59it/s, training loss=0.196]\u001b[A\n",
      "Epoch 3:   3%|▎         | 19/604 [00:07<03:45,  2.59it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:   3%|▎         | 20/604 [00:07<03:45,  2.59it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:   3%|▎         | 20/604 [00:08<03:45,  2.59it/s, training loss=0.183]\u001b[A\n",
      "Epoch 3:   3%|▎         | 21/604 [00:08<03:45,  2.58it/s, training loss=0.183]\u001b[A\n",
      "Epoch 3:   3%|▎         | 21/604 [00:08<03:45,  2.58it/s, training loss=0.137]\u001b[A\n",
      "Epoch 3:   4%|▎         | 22/604 [00:08<03:46,  2.57it/s, training loss=0.137]\u001b[A\n",
      "Epoch 3:   4%|▎         | 22/604 [00:08<03:46,  2.57it/s, training loss=0.139]\u001b[A\n",
      "Epoch 3:   4%|▍         | 23/604 [00:08<03:45,  2.57it/s, training loss=0.139]\u001b[A\n",
      "Epoch 3:   4%|▍         | 23/604 [00:09<03:45,  2.57it/s, training loss=0.148]\u001b[A\n",
      "Epoch 3:   4%|▍         | 24/604 [00:09<03:50,  2.52it/s, training loss=0.148]\u001b[A\n",
      "Epoch 3:   4%|▍         | 24/604 [00:09<03:50,  2.52it/s, training loss=0.169]\u001b[A\n",
      "Epoch 3:   4%|▍         | 25/604 [00:09<03:50,  2.52it/s, training loss=0.169]\u001b[A\n",
      "Epoch 3:   4%|▍         | 25/604 [00:10<03:50,  2.52it/s, training loss=0.264]\u001b[A\n",
      "Epoch 3:   4%|▍         | 26/604 [00:10<03:48,  2.53it/s, training loss=0.264]\u001b[A\n",
      "Epoch 3:   4%|▍         | 26/604 [00:10<03:48,  2.53it/s, training loss=0.224]\u001b[A\n",
      "Epoch 3:   4%|▍         | 27/604 [00:10<03:46,  2.55it/s, training loss=0.224]\u001b[A\n",
      "Epoch 3:   4%|▍         | 27/604 [00:10<03:46,  2.55it/s, training loss=0.160]\u001b[A\n",
      "Epoch 3:   5%|▍         | 28/604 [00:10<03:44,  2.56it/s, training loss=0.160]\u001b[A\n",
      "Epoch 3:   5%|▍         | 28/604 [00:11<03:44,  2.56it/s, training loss=0.137]\u001b[A\n",
      "Epoch 3:   5%|▍         | 29/604 [00:11<03:43,  2.57it/s, training loss=0.137]\u001b[A\n",
      "Epoch 3:   5%|▍         | 29/604 [00:11<03:43,  2.57it/s, training loss=0.183]\u001b[A\n",
      "Epoch 3:   5%|▍         | 30/604 [00:11<03:43,  2.57it/s, training loss=0.183]\u001b[A\n",
      "Epoch 3:   5%|▍         | 30/604 [00:12<03:43,  2.57it/s, training loss=0.277]\u001b[A\n",
      "Epoch 3:   5%|▌         | 31/604 [00:12<03:43,  2.57it/s, training loss=0.277]\u001b[A\n",
      "Epoch 3:   5%|▌         | 31/604 [00:12<03:43,  2.57it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:   5%|▌         | 32/604 [00:12<03:41,  2.58it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:   5%|▌         | 32/604 [00:12<03:41,  2.58it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:   5%|▌         | 33/604 [00:12<03:41,  2.58it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:   5%|▌         | 33/604 [00:13<03:41,  2.58it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:   6%|▌         | 34/604 [00:13<03:40,  2.59it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:   6%|▌         | 34/604 [00:13<03:40,  2.59it/s, training loss=0.238]\u001b[A\n",
      "Epoch 3:   6%|▌         | 35/604 [00:13<03:40,  2.59it/s, training loss=0.238]\u001b[A\n",
      "Epoch 3:   6%|▌         | 35/604 [00:13<03:40,  2.59it/s, training loss=0.175]\u001b[A\n",
      "Epoch 3:   6%|▌         | 36/604 [00:13<03:39,  2.59it/s, training loss=0.175]\u001b[A\n",
      "Epoch 3:   6%|▌         | 36/604 [00:14<03:39,  2.59it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:   6%|▌         | 37/604 [00:14<03:38,  2.59it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:   6%|▌         | 37/604 [00:14<03:38,  2.59it/s, training loss=0.104]\u001b[A\n",
      "Epoch 3:   6%|▋         | 38/604 [00:14<03:38,  2.59it/s, training loss=0.104]\u001b[A\n",
      "Epoch 3:   6%|▋         | 38/604 [00:15<03:38,  2.59it/s, training loss=0.144]\u001b[A\n",
      "Epoch 3:   6%|▋         | 39/604 [00:15<03:37,  2.59it/s, training loss=0.144]\u001b[A\n",
      "Epoch 3:   6%|▋         | 39/604 [00:15<03:37,  2.59it/s, training loss=0.217]\u001b[A\n",
      "Epoch 3:   7%|▋         | 40/604 [00:15<03:37,  2.60it/s, training loss=0.217]\u001b[A\n",
      "Epoch 3:   7%|▋         | 40/604 [00:15<03:37,  2.60it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:   7%|▋         | 41/604 [00:15<03:37,  2.59it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:   7%|▋         | 41/604 [00:16<03:37,  2.59it/s, training loss=0.150]\u001b[A\n",
      "Epoch 3:   7%|▋         | 42/604 [00:16<03:36,  2.59it/s, training loss=0.150]\u001b[A\n",
      "Epoch 3:   7%|▋         | 42/604 [00:16<03:36,  2.59it/s, training loss=0.096]\u001b[A\n",
      "Epoch 3:   7%|▋         | 43/604 [00:16<03:36,  2.59it/s, training loss=0.096]\u001b[A\n",
      "Epoch 3:   7%|▋         | 43/604 [00:17<03:36,  2.59it/s, training loss=0.110]\u001b[A\n",
      "Epoch 3:   7%|▋         | 44/604 [00:17<03:37,  2.57it/s, training loss=0.110]\u001b[A\n",
      "Epoch 3:   7%|▋         | 44/604 [00:17<03:37,  2.57it/s, training loss=0.155]\u001b[A\n",
      "Epoch 3:   7%|▋         | 45/604 [00:17<03:36,  2.58it/s, training loss=0.155]\u001b[A\n",
      "Epoch 3:   7%|▋         | 45/604 [00:17<03:36,  2.58it/s, training loss=0.194]\u001b[A\n",
      "Epoch 3:   8%|▊         | 46/604 [00:17<03:36,  2.58it/s, training loss=0.194]\u001b[A\n",
      "Epoch 3:   8%|▊         | 46/604 [00:18<03:36,  2.58it/s, training loss=0.074]\u001b[A\n",
      "Epoch 3:   8%|▊         | 47/604 [00:18<03:36,  2.58it/s, training loss=0.074]\u001b[A\n",
      "Epoch 3:   8%|▊         | 47/604 [00:18<03:36,  2.58it/s, training loss=0.094]\u001b[A\n",
      "Epoch 3:   8%|▊         | 48/604 [00:18<03:35,  2.58it/s, training loss=0.094]\u001b[A\n",
      "Epoch 3:   8%|▊         | 48/604 [00:18<03:35,  2.58it/s, training loss=0.167]\u001b[A\n",
      "Epoch 3:   8%|▊         | 49/604 [00:18<03:34,  2.58it/s, training loss=0.167]\u001b[A\n",
      "Epoch 3:   8%|▊         | 49/604 [00:19<03:34,  2.58it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:   8%|▊         | 50/604 [00:19<03:34,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:   8%|▊         | 50/604 [00:19<03:34,  2.59it/s, training loss=0.214]\u001b[A\n",
      "Epoch 3:   8%|▊         | 51/604 [00:19<03:33,  2.59it/s, training loss=0.214]\u001b[A\n",
      "Epoch 3:   8%|▊         | 51/604 [00:20<03:33,  2.59it/s, training loss=0.112]\u001b[A\n",
      "Epoch 3:   9%|▊         | 52/604 [00:20<03:34,  2.57it/s, training loss=0.112]\u001b[A\n",
      "Epoch 3:   9%|▊         | 52/604 [00:20<03:34,  2.57it/s, training loss=0.173]\u001b[A\n",
      "Epoch 3:   9%|▉         | 53/604 [00:20<03:34,  2.57it/s, training loss=0.173]\u001b[A\n",
      "Epoch 3:   9%|▉         | 53/604 [00:20<03:34,  2.57it/s, training loss=0.149]\u001b[A\n",
      "Epoch 3:   9%|▉         | 54/604 [00:20<03:34,  2.57it/s, training loss=0.149]\u001b[A\n",
      "Epoch 3:   9%|▉         | 54/604 [00:21<03:34,  2.57it/s, training loss=0.179]\u001b[A\n",
      "Epoch 3:   9%|▉         | 55/604 [00:21<03:33,  2.57it/s, training loss=0.179]\u001b[A\n",
      "Epoch 3:   9%|▉         | 55/604 [00:21<03:33,  2.57it/s, training loss=0.118]\u001b[A\n",
      "Epoch 3:   9%|▉         | 56/604 [00:21<03:32,  2.58it/s, training loss=0.118]\u001b[A\n",
      "Epoch 3:   9%|▉         | 56/604 [00:22<03:32,  2.58it/s, training loss=0.092]\u001b[A\n",
      "Epoch 3:   9%|▉         | 57/604 [00:22<03:31,  2.58it/s, training loss=0.092]\u001b[A\n",
      "Epoch 3:   9%|▉         | 57/604 [00:22<03:31,  2.58it/s, training loss=0.234]\u001b[A\n",
      "Epoch 3:  10%|▉         | 58/604 [00:22<03:30,  2.59it/s, training loss=0.234]\u001b[A\n",
      "Epoch 3:  10%|▉         | 58/604 [00:22<03:30,  2.59it/s, training loss=0.160]\u001b[A\n",
      "Epoch 3:  10%|▉         | 59/604 [00:22<03:30,  2.59it/s, training loss=0.160]\u001b[A\n",
      "Epoch 3:  10%|▉         | 59/604 [00:23<03:30,  2.59it/s, training loss=0.192]\u001b[A\n",
      "Epoch 3:  10%|▉         | 60/604 [00:23<03:29,  2.59it/s, training loss=0.192]\u001b[A\n",
      "Epoch 3:  10%|▉         | 60/604 [00:23<03:29,  2.59it/s, training loss=0.219]\u001b[A\n",
      "Epoch 3:  10%|█         | 61/604 [00:23<03:29,  2.59it/s, training loss=0.219]\u001b[A\n",
      "Epoch 3:  10%|█         | 61/604 [00:24<03:29,  2.59it/s, training loss=0.149]\u001b[A\n",
      "Epoch 3:  10%|█         | 62/604 [00:24<03:29,  2.59it/s, training loss=0.149]\u001b[A\n",
      "Epoch 3:  10%|█         | 62/604 [00:24<03:29,  2.59it/s, training loss=0.162]\u001b[A\n",
      "Epoch 3:  10%|█         | 63/604 [00:24<03:28,  2.59it/s, training loss=0.162]\u001b[A\n",
      "Epoch 3:  10%|█         | 63/604 [00:24<03:28,  2.59it/s, training loss=0.204]\u001b[A\n",
      "Epoch 3:  11%|█         | 64/604 [00:24<03:28,  2.59it/s, training loss=0.204]\u001b[A\n",
      "Epoch 3:  11%|█         | 64/604 [00:25<03:28,  2.59it/s, training loss=0.136]\u001b[A\n",
      "Epoch 3:  11%|█         | 65/604 [00:25<03:27,  2.59it/s, training loss=0.136]\u001b[A\n",
      "Epoch 3:  11%|█         | 65/604 [00:25<03:27,  2.59it/s, training loss=0.184]\u001b[A\n",
      "Epoch 3:  11%|█         | 66/604 [00:25<03:28,  2.58it/s, training loss=0.184]\u001b[A\n",
      "Epoch 3:  11%|█         | 66/604 [00:25<03:28,  2.58it/s, training loss=0.094]\u001b[A\n",
      "Epoch 3:  11%|█         | 67/604 [00:25<03:27,  2.58it/s, training loss=0.094]\u001b[A\n",
      "Epoch 3:  11%|█         | 67/604 [00:26<03:27,  2.58it/s, training loss=0.198]\u001b[A\n",
      "Epoch 3:  11%|█▏        | 68/604 [00:26<03:27,  2.59it/s, training loss=0.198]\u001b[A\n",
      "Epoch 3:  11%|█▏        | 68/604 [00:26<03:27,  2.59it/s, training loss=0.197]\u001b[A\n",
      "Epoch 3:  11%|█▏        | 69/604 [00:26<03:26,  2.59it/s, training loss=0.197]\u001b[A\n",
      "Epoch 3:  11%|█▏        | 69/604 [00:27<03:26,  2.59it/s, training loss=0.123]\u001b[A\n",
      "Epoch 3:  12%|█▏        | 70/604 [00:27<03:26,  2.59it/s, training loss=0.123]\u001b[A\n",
      "Epoch 3:  12%|█▏        | 70/604 [00:27<03:26,  2.59it/s, training loss=0.199]\u001b[A\n",
      "Epoch 3:  12%|█▏        | 71/604 [00:27<03:29,  2.55it/s, training loss=0.199]\u001b[A\n",
      "Epoch 3:  12%|█▏        | 71/604 [00:27<03:29,  2.55it/s, training loss=0.129]\u001b[A\n",
      "Epoch 3:  12%|█▏        | 72/604 [00:27<03:29,  2.54it/s, training loss=0.129]\u001b[A\n",
      "Epoch 3:  12%|█▏        | 72/604 [00:28<03:29,  2.54it/s, training loss=0.171]\u001b[A\n",
      "Epoch 3:  12%|█▏        | 73/604 [00:28<03:28,  2.55it/s, training loss=0.171]\u001b[A\n",
      "Epoch 3:  12%|█▏        | 73/604 [00:28<03:28,  2.55it/s, training loss=0.223]\u001b[A\n",
      "Epoch 3:  12%|█▏        | 74/604 [00:28<03:27,  2.56it/s, training loss=0.223]\u001b[A\n",
      "Epoch 3:  12%|█▏        | 74/604 [00:29<03:27,  2.56it/s, training loss=0.139]\u001b[A\n",
      "Epoch 3:  12%|█▏        | 75/604 [00:29<03:25,  2.57it/s, training loss=0.139]\u001b[A\n",
      "Epoch 3:  12%|█▏        | 75/604 [00:29<03:25,  2.57it/s, training loss=0.168]\u001b[A\n",
      "Epoch 3:  13%|█▎        | 76/604 [00:29<03:24,  2.58it/s, training loss=0.168]\u001b[A\n",
      "Epoch 3:  13%|█▎        | 76/604 [00:29<03:24,  2.58it/s, training loss=0.172]\u001b[A\n",
      "Epoch 3:  13%|█▎        | 77/604 [00:29<03:23,  2.58it/s, training loss=0.172]\u001b[A\n",
      "Epoch 3:  13%|█▎        | 77/604 [00:30<03:23,  2.58it/s, training loss=0.181]\u001b[A\n",
      "Epoch 3:  13%|█▎        | 78/604 [00:30<03:23,  2.59it/s, training loss=0.181]\u001b[A\n",
      "Epoch 3:  13%|█▎        | 78/604 [00:30<03:23,  2.59it/s, training loss=0.138]\u001b[A\n",
      "Epoch 3:  13%|█▎        | 79/604 [00:30<03:22,  2.59it/s, training loss=0.138]\u001b[A\n",
      "Epoch 3:  13%|█▎        | 79/604 [00:31<03:22,  2.59it/s, training loss=0.125]\u001b[A\n",
      "Epoch 3:  13%|█▎        | 80/604 [00:31<03:25,  2.55it/s, training loss=0.125]\u001b[A\n",
      "Epoch 3:  13%|█▎        | 80/604 [00:31<03:25,  2.55it/s, training loss=0.233]\u001b[A\n",
      "Epoch 3:  13%|█▎        | 81/604 [00:31<03:25,  2.54it/s, training loss=0.233]\u001b[A\n",
      "Epoch 3:  13%|█▎        | 81/604 [00:31<03:25,  2.54it/s, training loss=0.182]\u001b[A\n",
      "Epoch 3:  14%|█▎        | 82/604 [00:31<03:24,  2.56it/s, training loss=0.182]\u001b[A\n",
      "Epoch 3:  14%|█▎        | 82/604 [00:32<03:24,  2.56it/s, training loss=0.198]\u001b[A\n",
      "Epoch 3:  14%|█▎        | 83/604 [00:32<03:22,  2.57it/s, training loss=0.198]\u001b[A\n",
      "Epoch 3:  14%|█▎        | 83/604 [00:32<03:22,  2.57it/s, training loss=0.266]\u001b[A\n",
      "Epoch 3:  14%|█▍        | 84/604 [00:32<03:30,  2.47it/s, training loss=0.266]\u001b[A\n",
      "Epoch 3:  14%|█▍        | 84/604 [00:33<03:30,  2.47it/s, training loss=0.181]\u001b[A\n",
      "Epoch 3:  14%|█▍        | 85/604 [00:33<03:31,  2.46it/s, training loss=0.181]\u001b[A\n",
      "Epoch 3:  14%|█▍        | 85/604 [00:33<03:31,  2.46it/s, training loss=0.210]\u001b[A\n",
      "Epoch 3:  14%|█▍        | 86/604 [00:33<03:28,  2.48it/s, training loss=0.210]\u001b[A\n",
      "Epoch 3:  14%|█▍        | 86/604 [00:33<03:28,  2.48it/s, training loss=0.188]\u001b[A\n",
      "Epoch 3:  14%|█▍        | 87/604 [00:33<03:25,  2.51it/s, training loss=0.188]\u001b[A\n",
      "Epoch 3:  14%|█▍        | 87/604 [00:34<03:25,  2.51it/s, training loss=0.164]\u001b[A\n",
      "Epoch 3:  15%|█▍        | 88/604 [00:34<03:23,  2.54it/s, training loss=0.164]\u001b[A\n",
      "Epoch 3:  15%|█▍        | 88/604 [00:34<03:23,  2.54it/s, training loss=0.155]\u001b[A\n",
      "Epoch 3:  15%|█▍        | 89/604 [00:34<03:21,  2.56it/s, training loss=0.155]\u001b[A\n",
      "Epoch 3:  15%|█▍        | 89/604 [00:34<03:21,  2.56it/s, training loss=0.156]\u001b[A\n",
      "Epoch 3:  15%|█▍        | 90/604 [00:34<03:22,  2.54it/s, training loss=0.156]\u001b[A\n",
      "Epoch 3:  15%|█▍        | 90/604 [00:35<03:22,  2.54it/s, training loss=0.263]\u001b[A\n",
      "Epoch 3:  15%|█▌        | 91/604 [00:35<03:20,  2.56it/s, training loss=0.263]\u001b[A\n",
      "Epoch 3:  15%|█▌        | 91/604 [00:35<03:20,  2.56it/s, training loss=0.128]\u001b[A\n",
      "Epoch 3:  15%|█▌        | 92/604 [00:35<03:19,  2.57it/s, training loss=0.128]\u001b[A\n",
      "Epoch 3:  15%|█▌        | 92/604 [00:36<03:19,  2.57it/s, training loss=0.127]\u001b[A\n",
      "Epoch 3:  15%|█▌        | 93/604 [00:36<03:18,  2.57it/s, training loss=0.127]\u001b[A\n",
      "Epoch 3:  15%|█▌        | 93/604 [00:36<03:18,  2.57it/s, training loss=0.184]\u001b[A\n",
      "Epoch 3:  16%|█▌        | 94/604 [00:36<03:17,  2.58it/s, training loss=0.184]\u001b[A\n",
      "Epoch 3:  16%|█▌        | 94/604 [00:36<03:17,  2.58it/s, training loss=0.189]\u001b[A\n",
      "Epoch 3:  16%|█▌        | 95/604 [00:36<03:21,  2.53it/s, training loss=0.189]\u001b[A\n",
      "Epoch 3:  16%|█▌        | 95/604 [00:37<03:21,  2.53it/s, training loss=0.115]\u001b[A\n",
      "Epoch 3:  16%|█▌        | 96/604 [00:37<03:21,  2.52it/s, training loss=0.115]\u001b[A\n",
      "Epoch 3:  16%|█▌        | 96/604 [00:37<03:21,  2.52it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:  16%|█▌        | 97/604 [00:37<03:19,  2.54it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:  16%|█▌        | 97/604 [00:38<03:19,  2.54it/s, training loss=0.204]\u001b[A\n",
      "Epoch 3:  16%|█▌        | 98/604 [00:38<03:18,  2.55it/s, training loss=0.204]\u001b[A\n",
      "Epoch 3:  16%|█▌        | 98/604 [00:38<03:18,  2.55it/s, training loss=0.250]\u001b[A\n",
      "Epoch 3:  16%|█▋        | 99/604 [00:38<03:18,  2.54it/s, training loss=0.250]\u001b[A\n",
      "Epoch 3:  16%|█▋        | 99/604 [00:38<03:18,  2.54it/s, training loss=0.218]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  17%|█▋        | 100/604 [00:38<03:17,  2.55it/s, training loss=0.218]\u001b[A\n",
      "Epoch 3:  17%|█▋        | 100/604 [00:39<03:17,  2.55it/s, training loss=0.103]\u001b[A\n",
      "Epoch 3:  17%|█▋        | 101/604 [00:39<03:16,  2.56it/s, training loss=0.103]\u001b[A\n",
      "Epoch 3:  17%|█▋        | 101/604 [00:39<03:16,  2.56it/s, training loss=0.185]\u001b[A\n",
      "Epoch 3:  17%|█▋        | 102/604 [00:39<03:15,  2.57it/s, training loss=0.185]\u001b[A\n",
      "Epoch 3:  17%|█▋        | 102/604 [00:40<03:15,  2.57it/s, training loss=0.179]\u001b[A\n",
      "Epoch 3:  17%|█▋        | 103/604 [00:40<03:14,  2.57it/s, training loss=0.179]\u001b[A\n",
      "Epoch 3:  17%|█▋        | 103/604 [00:40<03:14,  2.57it/s, training loss=0.218]\u001b[A\n",
      "Epoch 3:  17%|█▋        | 104/604 [00:40<03:13,  2.58it/s, training loss=0.218]\u001b[A\n",
      "Epoch 3:  17%|█▋        | 104/604 [00:40<03:13,  2.58it/s, training loss=0.111]\u001b[A\n",
      "Epoch 3:  17%|█▋        | 105/604 [00:40<03:13,  2.58it/s, training loss=0.111]\u001b[A\n",
      "Epoch 3:  17%|█▋        | 105/604 [00:41<03:13,  2.58it/s, training loss=0.146]\u001b[A\n",
      "Epoch 3:  18%|█▊        | 106/604 [00:41<03:12,  2.59it/s, training loss=0.146]\u001b[A\n",
      "Epoch 3:  18%|█▊        | 106/604 [00:41<03:12,  2.59it/s, training loss=0.222]\u001b[A\n",
      "Epoch 3:  18%|█▊        | 107/604 [00:41<03:11,  2.59it/s, training loss=0.222]\u001b[A\n",
      "Epoch 3:  18%|█▊        | 107/604 [00:42<03:11,  2.59it/s, training loss=0.126]\u001b[A\n",
      "Epoch 3:  18%|█▊        | 108/604 [00:42<03:15,  2.53it/s, training loss=0.126]\u001b[A\n",
      "Epoch 3:  18%|█▊        | 108/604 [00:42<03:15,  2.53it/s, training loss=0.103]\u001b[A\n",
      "Epoch 3:  18%|█▊        | 109/604 [00:42<03:19,  2.48it/s, training loss=0.103]\u001b[A\n",
      "Epoch 3:  18%|█▊        | 109/604 [00:42<03:19,  2.48it/s, training loss=0.280]\u001b[A\n",
      "Epoch 3:  18%|█▊        | 110/604 [00:42<03:17,  2.51it/s, training loss=0.280]\u001b[A\n",
      "Epoch 3:  18%|█▊        | 110/604 [00:43<03:17,  2.51it/s, training loss=0.147]\u001b[A\n",
      "Epoch 3:  18%|█▊        | 111/604 [00:43<03:14,  2.53it/s, training loss=0.147]\u001b[A\n",
      "Epoch 3:  18%|█▊        | 111/604 [00:43<03:14,  2.53it/s, training loss=0.222]\u001b[A\n",
      "Epoch 3:  19%|█▊        | 112/604 [00:43<03:13,  2.54it/s, training loss=0.222]\u001b[A\n",
      "Epoch 3:  19%|█▊        | 112/604 [00:44<03:13,  2.54it/s, training loss=0.149]\u001b[A\n",
      "Epoch 3:  19%|█▊        | 113/604 [00:44<03:11,  2.56it/s, training loss=0.149]\u001b[A\n",
      "Epoch 3:  19%|█▊        | 113/604 [00:44<03:11,  2.56it/s, training loss=0.121]\u001b[A\n",
      "Epoch 3:  19%|█▉        | 114/604 [00:44<03:11,  2.56it/s, training loss=0.121]\u001b[A\n",
      "Epoch 3:  19%|█▉        | 114/604 [00:44<03:11,  2.56it/s, training loss=0.220]\u001b[A\n",
      "Epoch 3:  19%|█▉        | 115/604 [00:44<03:10,  2.57it/s, training loss=0.220]\u001b[A\n",
      "Epoch 3:  19%|█▉        | 115/604 [00:45<03:10,  2.57it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:  19%|█▉        | 116/604 [00:45<03:09,  2.58it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:  19%|█▉        | 116/604 [00:45<03:09,  2.58it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  19%|█▉        | 117/604 [00:45<03:08,  2.59it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  19%|█▉        | 117/604 [00:45<03:08,  2.59it/s, training loss=0.196]\u001b[A\n",
      "Epoch 3:  20%|█▉        | 118/604 [00:45<03:07,  2.59it/s, training loss=0.196]\u001b[A\n",
      "Epoch 3:  20%|█▉        | 118/604 [00:46<03:07,  2.59it/s, training loss=0.185]\u001b[A\n",
      "Epoch 3:  20%|█▉        | 119/604 [00:46<03:07,  2.59it/s, training loss=0.185]\u001b[A\n",
      "Epoch 3:  20%|█▉        | 119/604 [00:46<03:07,  2.59it/s, training loss=0.264]\u001b[A\n",
      "Epoch 3:  20%|█▉        | 120/604 [00:46<03:06,  2.59it/s, training loss=0.264]\u001b[A\n",
      "Epoch 3:  20%|█▉        | 120/604 [00:47<03:06,  2.59it/s, training loss=0.136]\u001b[A\n",
      "Epoch 3:  20%|██        | 121/604 [00:47<03:06,  2.60it/s, training loss=0.136]\u001b[A\n",
      "Epoch 3:  20%|██        | 121/604 [00:47<03:06,  2.60it/s, training loss=0.152]\u001b[A\n",
      "Epoch 3:  20%|██        | 122/604 [00:47<03:05,  2.60it/s, training loss=0.152]\u001b[A\n",
      "Epoch 3:  20%|██        | 122/604 [00:47<03:05,  2.60it/s, training loss=0.193]\u001b[A\n",
      "Epoch 3:  20%|██        | 123/604 [00:47<03:06,  2.58it/s, training loss=0.193]\u001b[A\n",
      "Epoch 3:  20%|██        | 123/604 [00:48<03:06,  2.58it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:  21%|██        | 124/604 [00:48<03:05,  2.59it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:  21%|██        | 124/604 [00:48<03:05,  2.59it/s, training loss=0.277]\u001b[A\n",
      "Epoch 3:  21%|██        | 125/604 [00:48<03:04,  2.59it/s, training loss=0.277]\u001b[A\n",
      "Epoch 3:  21%|██        | 125/604 [00:49<03:04,  2.59it/s, training loss=0.147]\u001b[A\n",
      "Epoch 3:  21%|██        | 126/604 [00:49<03:04,  2.59it/s, training loss=0.147]\u001b[A\n",
      "Epoch 3:  21%|██        | 126/604 [00:49<03:04,  2.59it/s, training loss=0.160]\u001b[A\n",
      "Epoch 3:  21%|██        | 127/604 [00:49<03:04,  2.58it/s, training loss=0.160]\u001b[A\n",
      "Epoch 3:  21%|██        | 127/604 [00:49<03:04,  2.58it/s, training loss=0.213]\u001b[A\n",
      "Epoch 3:  21%|██        | 128/604 [00:49<03:04,  2.59it/s, training loss=0.213]\u001b[A\n",
      "Epoch 3:  21%|██        | 128/604 [00:50<03:04,  2.59it/s, training loss=0.258]\u001b[A\n",
      "Epoch 3:  21%|██▏       | 129/604 [00:50<03:03,  2.59it/s, training loss=0.258]\u001b[A\n",
      "Epoch 3:  21%|██▏       | 129/604 [00:50<03:03,  2.59it/s, training loss=0.050]\u001b[A\n",
      "Epoch 3:  22%|██▏       | 130/604 [00:50<03:02,  2.59it/s, training loss=0.050]\u001b[A\n",
      "Epoch 3:  22%|██▏       | 130/604 [00:50<03:02,  2.59it/s, training loss=0.127]\u001b[A\n",
      "Epoch 3:  22%|██▏       | 131/604 [00:50<03:03,  2.58it/s, training loss=0.127]\u001b[A\n",
      "Epoch 3:  22%|██▏       | 131/604 [00:51<03:03,  2.58it/s, training loss=0.203]\u001b[A\n",
      "Epoch 3:  22%|██▏       | 132/604 [00:51<03:02,  2.59it/s, training loss=0.203]\u001b[A\n",
      "Epoch 3:  22%|██▏       | 132/604 [00:51<03:02,  2.59it/s, training loss=0.171]\u001b[A\n",
      "Epoch 3:  22%|██▏       | 133/604 [00:51<03:01,  2.59it/s, training loss=0.171]\u001b[A\n",
      "Epoch 3:  22%|██▏       | 133/604 [00:52<03:01,  2.59it/s, training loss=0.127]\u001b[A\n",
      "Epoch 3:  22%|██▏       | 134/604 [00:52<03:01,  2.59it/s, training loss=0.127]\u001b[A\n",
      "Epoch 3:  22%|██▏       | 134/604 [00:52<03:01,  2.59it/s, training loss=0.128]\u001b[A\n",
      "Epoch 3:  22%|██▏       | 135/604 [00:52<03:00,  2.59it/s, training loss=0.128]\u001b[A\n",
      "Epoch 3:  22%|██▏       | 135/604 [00:52<03:00,  2.59it/s, training loss=0.164]\u001b[A\n",
      "Epoch 3:  23%|██▎       | 136/604 [00:52<03:01,  2.58it/s, training loss=0.164]\u001b[A\n",
      "Epoch 3:  23%|██▎       | 136/604 [00:53<03:01,  2.58it/s, training loss=0.203]\u001b[A\n",
      "Epoch 3:  23%|██▎       | 137/604 [00:53<03:03,  2.55it/s, training loss=0.203]\u001b[A\n",
      "Epoch 3:  23%|██▎       | 137/604 [00:53<03:03,  2.55it/s, training loss=0.174]\u001b[A\n",
      "Epoch 3:  23%|██▎       | 138/604 [00:53<03:02,  2.55it/s, training loss=0.174]\u001b[A\n",
      "Epoch 3:  23%|██▎       | 138/604 [00:54<03:02,  2.55it/s, training loss=0.218]\u001b[A\n",
      "Epoch 3:  23%|██▎       | 139/604 [00:54<03:01,  2.56it/s, training loss=0.218]\u001b[A\n",
      "Epoch 3:  23%|██▎       | 139/604 [00:54<03:01,  2.56it/s, training loss=0.146]\u001b[A\n",
      "Epoch 3:  23%|██▎       | 140/604 [00:54<03:00,  2.57it/s, training loss=0.146]\u001b[A\n",
      "Epoch 3:  23%|██▎       | 140/604 [00:54<03:00,  2.57it/s, training loss=0.133]\u001b[A\n",
      "Epoch 3:  23%|██▎       | 141/604 [00:54<02:59,  2.58it/s, training loss=0.133]\u001b[A\n",
      "Epoch 3:  23%|██▎       | 141/604 [00:55<02:59,  2.58it/s, training loss=0.138]\u001b[A\n",
      "Epoch 3:  24%|██▎       | 142/604 [00:55<02:58,  2.59it/s, training loss=0.138]\u001b[A\n",
      "Epoch 3:  24%|██▎       | 142/604 [00:55<02:58,  2.59it/s, training loss=0.187]\u001b[A\n",
      "Epoch 3:  24%|██▎       | 143/604 [00:55<02:57,  2.59it/s, training loss=0.187]\u001b[A\n",
      "Epoch 3:  24%|██▎       | 143/604 [00:55<02:57,  2.59it/s, training loss=0.182]\u001b[A\n",
      "Epoch 3:  24%|██▍       | 144/604 [00:55<02:57,  2.59it/s, training loss=0.182]\u001b[A\n",
      "Epoch 3:  24%|██▍       | 144/604 [00:56<02:57,  2.59it/s, training loss=0.199]\u001b[A\n",
      "Epoch 3:  24%|██▍       | 145/604 [00:56<02:57,  2.59it/s, training loss=0.199]\u001b[A\n",
      "Epoch 3:  24%|██▍       | 145/604 [00:56<02:57,  2.59it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:  24%|██▍       | 146/604 [00:56<02:56,  2.59it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:  24%|██▍       | 146/604 [00:57<02:56,  2.59it/s, training loss=0.164]\u001b[A\n",
      "Epoch 3:  24%|██▍       | 147/604 [00:57<02:56,  2.60it/s, training loss=0.164]\u001b[A\n",
      "Epoch 3:  24%|██▍       | 147/604 [00:57<02:56,  2.60it/s, training loss=0.183]\u001b[A\n",
      "Epoch 3:  25%|██▍       | 148/604 [00:57<02:58,  2.56it/s, training loss=0.183]\u001b[A\n",
      "Epoch 3:  25%|██▍       | 148/604 [00:57<02:58,  2.56it/s, training loss=0.114]\u001b[A\n",
      "Epoch 3:  25%|██▍       | 149/604 [00:57<02:58,  2.55it/s, training loss=0.114]\u001b[A\n",
      "Epoch 3:  25%|██▍       | 149/604 [00:58<02:58,  2.55it/s, training loss=0.280]\u001b[A\n",
      "Epoch 3:  25%|██▍       | 150/604 [00:58<02:56,  2.57it/s, training loss=0.280]\u001b[A\n",
      "Epoch 3:  25%|██▍       | 150/604 [00:58<02:56,  2.57it/s, training loss=0.182]\u001b[A\n",
      "Epoch 3:  25%|██▌       | 151/604 [00:58<02:56,  2.57it/s, training loss=0.182]\u001b[A\n",
      "Epoch 3:  25%|██▌       | 151/604 [00:59<02:56,  2.57it/s, training loss=0.187]\u001b[A\n",
      "Epoch 3:  25%|██▌       | 152/604 [00:59<02:58,  2.53it/s, training loss=0.187]\u001b[A\n",
      "Epoch 3:  25%|██▌       | 152/604 [00:59<02:58,  2.53it/s, training loss=0.137]\u001b[A\n",
      "Epoch 3:  25%|██▌       | 153/604 [00:59<02:58,  2.53it/s, training loss=0.137]\u001b[A\n",
      "Epoch 3:  25%|██▌       | 153/604 [00:59<02:58,  2.53it/s, training loss=0.140]\u001b[A\n",
      "Epoch 3:  25%|██▌       | 154/604 [00:59<02:56,  2.54it/s, training loss=0.140]\u001b[A\n",
      "Epoch 3:  25%|██▌       | 154/604 [01:00<02:56,  2.54it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:  26%|██▌       | 155/604 [01:00<02:55,  2.55it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:  26%|██▌       | 155/604 [01:00<02:55,  2.55it/s, training loss=0.145]\u001b[A\n",
      "Epoch 3:  26%|██▌       | 156/604 [01:00<02:54,  2.56it/s, training loss=0.145]\u001b[A\n",
      "Epoch 3:  26%|██▌       | 156/604 [01:01<02:54,  2.56it/s, training loss=0.137]\u001b[A\n",
      "Epoch 3:  26%|██▌       | 157/604 [01:01<02:53,  2.57it/s, training loss=0.137]\u001b[A\n",
      "Epoch 3:  26%|██▌       | 157/604 [01:01<02:53,  2.57it/s, training loss=0.177]\u001b[A\n",
      "Epoch 3:  26%|██▌       | 158/604 [01:01<02:53,  2.57it/s, training loss=0.177]\u001b[A\n",
      "Epoch 3:  26%|██▌       | 158/604 [01:01<02:53,  2.57it/s, training loss=0.172]\u001b[A\n",
      "Epoch 3:  26%|██▋       | 159/604 [01:01<02:52,  2.58it/s, training loss=0.172]\u001b[A\n",
      "Epoch 3:  26%|██▋       | 159/604 [01:02<02:52,  2.58it/s, training loss=0.184]\u001b[A\n",
      "Epoch 3:  26%|██▋       | 160/604 [01:02<02:51,  2.59it/s, training loss=0.184]\u001b[A\n",
      "Epoch 3:  26%|██▋       | 160/604 [01:02<02:51,  2.59it/s, training loss=0.138]\u001b[A\n",
      "Epoch 3:  27%|██▋       | 161/604 [01:02<02:51,  2.59it/s, training loss=0.138]\u001b[A\n",
      "Epoch 3:  27%|██▋       | 161/604 [01:03<02:51,  2.59it/s, training loss=0.198]\u001b[A\n",
      "Epoch 3:  27%|██▋       | 162/604 [01:03<02:52,  2.57it/s, training loss=0.198]\u001b[A\n",
      "Epoch 3:  27%|██▋       | 162/604 [01:03<02:52,  2.57it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  27%|██▋       | 163/604 [01:03<02:51,  2.58it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  27%|██▋       | 163/604 [01:03<02:51,  2.58it/s, training loss=0.128]\u001b[A\n",
      "Epoch 3:  27%|██▋       | 164/604 [01:03<02:50,  2.58it/s, training loss=0.128]\u001b[A\n",
      "Epoch 3:  27%|██▋       | 164/604 [01:04<02:50,  2.58it/s, training loss=0.137]\u001b[A\n",
      "Epoch 3:  27%|██▋       | 165/604 [01:04<02:51,  2.56it/s, training loss=0.137]\u001b[A\n",
      "Epoch 3:  27%|██▋       | 165/604 [01:04<02:51,  2.56it/s, training loss=0.168]\u001b[A\n",
      "Epoch 3:  27%|██▋       | 166/604 [01:04<02:52,  2.54it/s, training loss=0.168]\u001b[A\n",
      "Epoch 3:  27%|██▋       | 166/604 [01:04<02:52,  2.54it/s, training loss=0.095]\u001b[A\n",
      "Epoch 3:  28%|██▊       | 167/604 [01:04<02:50,  2.56it/s, training loss=0.095]\u001b[A\n",
      "Epoch 3:  28%|██▊       | 167/604 [01:05<02:50,  2.56it/s, training loss=0.268]\u001b[A\n",
      "Epoch 3:  28%|██▊       | 168/604 [01:05<02:49,  2.57it/s, training loss=0.268]\u001b[A\n",
      "Epoch 3:  28%|██▊       | 168/604 [01:05<02:49,  2.57it/s, training loss=0.120]\u001b[A\n",
      "Epoch 3:  28%|██▊       | 169/604 [01:05<02:48,  2.58it/s, training loss=0.120]\u001b[A\n",
      "Epoch 3:  28%|██▊       | 169/604 [01:06<02:48,  2.58it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:  28%|██▊       | 170/604 [01:06<02:47,  2.58it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:  28%|██▊       | 170/604 [01:06<02:47,  2.58it/s, training loss=0.169]\u001b[A\n",
      "Epoch 3:  28%|██▊       | 171/604 [01:06<02:47,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 3:  28%|██▊       | 171/604 [01:06<02:47,  2.59it/s, training loss=0.202]\u001b[A\n",
      "Epoch 3:  28%|██▊       | 172/604 [01:06<02:46,  2.59it/s, training loss=0.202]\u001b[A\n",
      "Epoch 3:  28%|██▊       | 172/604 [01:07<02:46,  2.59it/s, training loss=0.164]\u001b[A\n",
      "Epoch 3:  29%|██▊       | 173/604 [01:07<02:46,  2.59it/s, training loss=0.164]\u001b[A\n",
      "Epoch 3:  29%|██▊       | 173/604 [01:07<02:46,  2.59it/s, training loss=0.132]\u001b[A\n",
      "Epoch 3:  29%|██▉       | 174/604 [01:07<02:46,  2.59it/s, training loss=0.132]\u001b[A\n",
      "Epoch 3:  29%|██▉       | 174/604 [01:08<02:46,  2.59it/s, training loss=0.138]\u001b[A\n",
      "Epoch 3:  29%|██▉       | 175/604 [01:08<02:45,  2.59it/s, training loss=0.138]\u001b[A\n",
      "Epoch 3:  29%|██▉       | 175/604 [01:08<02:45,  2.59it/s, training loss=0.195]\u001b[A\n",
      "Epoch 3:  29%|██▉       | 176/604 [01:08<02:45,  2.59it/s, training loss=0.195]\u001b[A\n",
      "Epoch 3:  29%|██▉       | 176/604 [01:08<02:45,  2.59it/s, training loss=0.166]\u001b[A\n",
      "Epoch 3:  29%|██▉       | 177/604 [01:08<02:45,  2.58it/s, training loss=0.166]\u001b[A\n",
      "Epoch 3:  29%|██▉       | 177/604 [01:09<02:45,  2.58it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  29%|██▉       | 178/604 [01:09<02:44,  2.58it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  29%|██▉       | 178/604 [01:09<02:44,  2.58it/s, training loss=0.200]\u001b[A\n",
      "Epoch 3:  30%|██▉       | 179/604 [01:09<02:44,  2.59it/s, training loss=0.200]\u001b[A\n",
      "Epoch 3:  30%|██▉       | 179/604 [01:09<02:44,  2.59it/s, training loss=0.176]\u001b[A\n",
      "Epoch 3:  30%|██▉       | 180/604 [01:09<02:43,  2.59it/s, training loss=0.176]\u001b[A\n",
      "Epoch 3:  30%|██▉       | 180/604 [01:10<02:43,  2.59it/s, training loss=0.190]\u001b[A\n",
      "Epoch 3:  30%|██▉       | 181/604 [01:10<02:43,  2.59it/s, training loss=0.190]\u001b[A\n",
      "Epoch 3:  30%|██▉       | 181/604 [01:10<02:43,  2.59it/s, training loss=0.103]\u001b[A\n",
      "Epoch 3:  30%|███       | 182/604 [01:10<02:42,  2.60it/s, training loss=0.103]\u001b[A\n",
      "Epoch 3:  30%|███       | 182/604 [01:11<02:42,  2.60it/s, training loss=0.143]\u001b[A\n",
      "Epoch 3:  30%|███       | 183/604 [01:11<02:42,  2.59it/s, training loss=0.143]\u001b[A\n",
      "Epoch 3:  30%|███       | 183/604 [01:11<02:42,  2.59it/s, training loss=0.211]\u001b[A\n",
      "Epoch 3:  30%|███       | 184/604 [01:11<02:41,  2.59it/s, training loss=0.211]\u001b[A\n",
      "Epoch 3:  30%|███       | 184/604 [01:11<02:41,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  31%|███       | 185/604 [01:11<02:41,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  31%|███       | 185/604 [01:12<02:41,  2.59it/s, training loss=0.137]\u001b[A\n",
      "Epoch 3:  31%|███       | 186/604 [01:12<02:41,  2.59it/s, training loss=0.137]\u001b[A\n",
      "Epoch 3:  31%|███       | 186/604 [01:12<02:41,  2.59it/s, training loss=0.185]\u001b[A\n",
      "Epoch 3:  31%|███       | 187/604 [01:12<02:40,  2.59it/s, training loss=0.185]\u001b[A\n",
      "Epoch 3:  31%|███       | 187/604 [01:13<02:40,  2.59it/s, training loss=0.191]\u001b[A\n",
      "Epoch 3:  31%|███       | 188/604 [01:13<02:40,  2.59it/s, training loss=0.191]\u001b[A\n",
      "Epoch 3:  31%|███       | 188/604 [01:13<02:40,  2.59it/s, training loss=0.166]\u001b[A\n",
      "Epoch 3:  31%|███▏      | 189/604 [01:13<02:39,  2.60it/s, training loss=0.166]\u001b[A\n",
      "Epoch 3:  31%|███▏      | 189/604 [01:13<02:39,  2.60it/s, training loss=0.141]\u001b[A\n",
      "Epoch 3:  31%|███▏      | 190/604 [01:13<02:39,  2.59it/s, training loss=0.141]\u001b[A\n",
      "Epoch 3:  31%|███▏      | 190/604 [01:14<02:39,  2.59it/s, training loss=0.267]\u001b[A\n",
      "Epoch 3:  32%|███▏      | 191/604 [01:14<02:39,  2.59it/s, training loss=0.267]\u001b[A\n",
      "Epoch 3:  32%|███▏      | 191/604 [01:14<02:39,  2.59it/s, training loss=0.143]\u001b[A\n",
      "Epoch 3:  32%|███▏      | 192/604 [01:14<02:39,  2.59it/s, training loss=0.143]\u001b[A\n",
      "Epoch 3:  32%|███▏      | 192/604 [01:14<02:39,  2.59it/s, training loss=0.204]\u001b[A\n",
      "Epoch 3:  32%|███▏      | 193/604 [01:14<02:38,  2.59it/s, training loss=0.204]\u001b[A\n",
      "Epoch 3:  32%|███▏      | 193/604 [01:15<02:38,  2.59it/s, training loss=0.145]\u001b[A\n",
      "Epoch 3:  32%|███▏      | 194/604 [01:15<02:40,  2.56it/s, training loss=0.145]\u001b[A\n",
      "Epoch 3:  32%|███▏      | 194/604 [01:15<02:40,  2.56it/s, training loss=0.181]\u001b[A\n",
      "Epoch 3:  32%|███▏      | 195/604 [01:15<02:40,  2.55it/s, training loss=0.181]\u001b[A\n",
      "Epoch 3:  32%|███▏      | 195/604 [01:16<02:40,  2.55it/s, training loss=0.164]\u001b[A\n",
      "Epoch 3:  32%|███▏      | 196/604 [01:16<02:39,  2.56it/s, training loss=0.164]\u001b[A\n",
      "Epoch 3:  32%|███▏      | 196/604 [01:16<02:39,  2.56it/s, training loss=0.262]\u001b[A\n",
      "Epoch 3:  33%|███▎      | 197/604 [01:16<02:39,  2.56it/s, training loss=0.262]\u001b[A\n",
      "Epoch 3:  33%|███▎      | 197/604 [01:16<02:39,  2.56it/s, training loss=0.129]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  33%|███▎      | 198/604 [01:16<02:38,  2.56it/s, training loss=0.129]\u001b[A\n",
      "Epoch 3:  33%|███▎      | 198/604 [01:17<02:38,  2.56it/s, training loss=0.199]\u001b[A\n",
      "Epoch 3:  33%|███▎      | 199/604 [01:17<02:37,  2.57it/s, training loss=0.199]\u001b[A\n",
      "Epoch 3:  33%|███▎      | 199/604 [01:17<02:37,  2.57it/s, training loss=0.188]\u001b[A\n",
      "Epoch 3:  33%|███▎      | 200/604 [01:17<02:36,  2.58it/s, training loss=0.188]\u001b[A\n",
      "Epoch 3:  33%|███▎      | 200/604 [01:18<02:36,  2.58it/s, training loss=0.163]\u001b[A\n",
      "Epoch 3:  33%|███▎      | 201/604 [01:18<02:35,  2.58it/s, training loss=0.163]\u001b[A\n",
      "Epoch 3:  33%|███▎      | 201/604 [01:18<02:35,  2.58it/s, training loss=0.141]\u001b[A\n",
      "Epoch 3:  33%|███▎      | 202/604 [01:18<02:35,  2.59it/s, training loss=0.141]\u001b[A\n",
      "Epoch 3:  33%|███▎      | 202/604 [01:18<02:35,  2.59it/s, training loss=0.139]\u001b[A\n",
      "Epoch 3:  34%|███▎      | 203/604 [01:18<02:34,  2.59it/s, training loss=0.139]\u001b[A\n",
      "Epoch 3:  34%|███▎      | 203/604 [01:19<02:34,  2.59it/s, training loss=0.149]\u001b[A\n",
      "Epoch 3:  34%|███▍      | 204/604 [01:19<02:34,  2.59it/s, training loss=0.149]\u001b[A\n",
      "Epoch 3:  34%|███▍      | 204/604 [01:19<02:34,  2.59it/s, training loss=0.178]\u001b[A\n",
      "Epoch 3:  34%|███▍      | 205/604 [01:19<02:33,  2.59it/s, training loss=0.178]\u001b[A\n",
      "Epoch 3:  34%|███▍      | 205/604 [01:20<02:33,  2.59it/s, training loss=0.223]\u001b[A\n",
      "Epoch 3:  34%|███▍      | 206/604 [01:20<02:33,  2.60it/s, training loss=0.223]\u001b[A\n",
      "Epoch 3:  34%|███▍      | 206/604 [01:20<02:33,  2.60it/s, training loss=0.137]\u001b[A\n",
      "Epoch 3:  34%|███▍      | 207/604 [01:20<02:32,  2.60it/s, training loss=0.137]\u001b[A\n",
      "Epoch 3:  34%|███▍      | 207/604 [01:20<02:32,  2.60it/s, training loss=0.091]\u001b[A\n",
      "Epoch 3:  34%|███▍      | 208/604 [01:20<02:32,  2.59it/s, training loss=0.091]\u001b[A\n",
      "Epoch 3:  34%|███▍      | 208/604 [01:21<02:32,  2.59it/s, training loss=0.156]\u001b[A\n",
      "Epoch 3:  35%|███▍      | 209/604 [01:21<02:32,  2.59it/s, training loss=0.156]\u001b[A\n",
      "Epoch 3:  35%|███▍      | 209/604 [01:21<02:32,  2.59it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:  35%|███▍      | 210/604 [01:21<02:31,  2.59it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:  35%|███▍      | 210/604 [01:21<02:31,  2.59it/s, training loss=0.230]\u001b[A\n",
      "Epoch 3:  35%|███▍      | 211/604 [01:21<02:31,  2.60it/s, training loss=0.230]\u001b[A\n",
      "Epoch 3:  35%|███▍      | 211/604 [01:22<02:31,  2.60it/s, training loss=0.202]\u001b[A\n",
      "Epoch 3:  35%|███▌      | 212/604 [01:22<02:30,  2.60it/s, training loss=0.202]\u001b[A\n",
      "Epoch 3:  35%|███▌      | 212/604 [01:22<02:30,  2.60it/s, training loss=0.136]\u001b[A\n",
      "Epoch 3:  35%|███▌      | 213/604 [01:22<02:30,  2.60it/s, training loss=0.136]\u001b[A\n",
      "Epoch 3:  35%|███▌      | 213/604 [01:23<02:30,  2.60it/s, training loss=0.140]\u001b[A\n",
      "Epoch 3:  35%|███▌      | 214/604 [01:23<02:31,  2.58it/s, training loss=0.140]\u001b[A\n",
      "Epoch 3:  35%|███▌      | 214/604 [01:23<02:31,  2.58it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  36%|███▌      | 215/604 [01:23<02:30,  2.58it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  36%|███▌      | 215/604 [01:23<02:30,  2.58it/s, training loss=0.152]\u001b[A\n",
      "Epoch 3:  36%|███▌      | 216/604 [01:23<02:30,  2.59it/s, training loss=0.152]\u001b[A\n",
      "Epoch 3:  36%|███▌      | 216/604 [01:24<02:30,  2.59it/s, training loss=0.179]\u001b[A\n",
      "Epoch 3:  36%|███▌      | 217/604 [01:24<02:29,  2.59it/s, training loss=0.179]\u001b[A\n",
      "Epoch 3:  36%|███▌      | 217/604 [01:24<02:29,  2.59it/s, training loss=0.111]\u001b[A\n",
      "Epoch 3:  36%|███▌      | 218/604 [01:24<02:29,  2.59it/s, training loss=0.111]\u001b[A\n",
      "Epoch 3:  36%|███▌      | 218/604 [01:25<02:29,  2.59it/s, training loss=0.125]\u001b[A\n",
      "Epoch 3:  36%|███▋      | 219/604 [01:25<02:29,  2.58it/s, training loss=0.125]\u001b[A\n",
      "Epoch 3:  36%|███▋      | 219/604 [01:25<02:29,  2.58it/s, training loss=0.123]\u001b[A\n",
      "Epoch 3:  36%|███▋      | 220/604 [01:25<02:28,  2.59it/s, training loss=0.123]\u001b[A\n",
      "Epoch 3:  36%|███▋      | 220/604 [01:25<02:28,  2.59it/s, training loss=0.158]\u001b[A\n",
      "Epoch 3:  37%|███▋      | 221/604 [01:25<02:28,  2.57it/s, training loss=0.158]\u001b[A\n",
      "Epoch 3:  37%|███▋      | 221/604 [01:26<02:28,  2.57it/s, training loss=0.169]\u001b[A\n",
      "Epoch 3:  37%|███▋      | 222/604 [01:26<02:30,  2.53it/s, training loss=0.169]\u001b[A\n",
      "Epoch 3:  37%|███▋      | 222/604 [01:26<02:30,  2.53it/s, training loss=0.179]\u001b[A\n",
      "Epoch 3:  37%|███▋      | 223/604 [01:26<02:30,  2.53it/s, training loss=0.179]\u001b[A\n",
      "Epoch 3:  37%|███▋      | 223/604 [01:27<02:30,  2.53it/s, training loss=0.148]\u001b[A\n",
      "Epoch 3:  37%|███▋      | 224/604 [01:27<02:29,  2.55it/s, training loss=0.148]\u001b[A\n",
      "Epoch 3:  37%|███▋      | 224/604 [01:27<02:29,  2.55it/s, training loss=0.157]\u001b[A\n",
      "Epoch 3:  37%|███▋      | 225/604 [01:27<02:27,  2.56it/s, training loss=0.157]\u001b[A\n",
      "Epoch 3:  37%|███▋      | 225/604 [01:27<02:27,  2.56it/s, training loss=0.171]\u001b[A\n",
      "Epoch 3:  37%|███▋      | 226/604 [01:27<02:26,  2.57it/s, training loss=0.171]\u001b[A\n",
      "Epoch 3:  37%|███▋      | 226/604 [01:28<02:26,  2.57it/s, training loss=0.122]\u001b[A\n",
      "Epoch 3:  38%|███▊      | 227/604 [01:28<02:26,  2.58it/s, training loss=0.122]\u001b[A\n",
      "Epoch 3:  38%|███▊      | 227/604 [01:28<02:26,  2.58it/s, training loss=0.160]\u001b[A\n",
      "Epoch 3:  38%|███▊      | 228/604 [01:28<02:25,  2.58it/s, training loss=0.160]\u001b[A\n",
      "Epoch 3:  38%|███▊      | 228/604 [01:28<02:25,  2.58it/s, training loss=0.157]\u001b[A\n",
      "Epoch 3:  38%|███▊      | 229/604 [01:28<02:25,  2.58it/s, training loss=0.157]\u001b[A\n",
      "Epoch 3:  38%|███▊      | 229/604 [01:29<02:25,  2.58it/s, training loss=0.175]\u001b[A\n",
      "Epoch 3:  38%|███▊      | 230/604 [01:29<02:24,  2.59it/s, training loss=0.175]\u001b[A\n",
      "Epoch 3:  38%|███▊      | 230/604 [01:29<02:24,  2.59it/s, training loss=0.142]\u001b[A\n",
      "Epoch 3:  38%|███▊      | 231/604 [01:29<02:23,  2.59it/s, training loss=0.142]\u001b[A\n",
      "Epoch 3:  38%|███▊      | 231/604 [01:30<02:23,  2.59it/s, training loss=0.182]\u001b[A\n",
      "Epoch 3:  38%|███▊      | 232/604 [01:30<02:23,  2.59it/s, training loss=0.182]\u001b[A\n",
      "Epoch 3:  38%|███▊      | 232/604 [01:30<02:23,  2.59it/s, training loss=0.132]\u001b[A\n",
      "Epoch 3:  39%|███▊      | 233/604 [01:30<02:25,  2.55it/s, training loss=0.132]\u001b[A\n",
      "Epoch 3:  39%|███▊      | 233/604 [01:30<02:25,  2.55it/s, training loss=0.107]\u001b[A\n",
      "Epoch 3:  39%|███▊      | 234/604 [01:30<02:24,  2.56it/s, training loss=0.107]\u001b[A\n",
      "Epoch 3:  39%|███▊      | 234/604 [01:31<02:24,  2.56it/s, training loss=0.207]\u001b[A\n",
      "Epoch 3:  39%|███▉      | 235/604 [01:31<02:23,  2.57it/s, training loss=0.207]\u001b[A\n",
      "Epoch 3:  39%|███▉      | 235/604 [01:31<02:23,  2.57it/s, training loss=0.092]\u001b[A\n",
      "Epoch 3:  39%|███▉      | 236/604 [01:31<02:23,  2.57it/s, training loss=0.092]\u001b[A\n",
      "Epoch 3:  39%|███▉      | 236/604 [01:32<02:23,  2.57it/s, training loss=0.102]\u001b[A\n",
      "Epoch 3:  39%|███▉      | 237/604 [01:32<02:22,  2.57it/s, training loss=0.102]\u001b[A\n",
      "Epoch 3:  39%|███▉      | 237/604 [01:32<02:22,  2.57it/s, training loss=0.188]\u001b[A\n",
      "Epoch 3:  39%|███▉      | 238/604 [01:32<02:30,  2.43it/s, training loss=0.188]\u001b[A\n",
      "Epoch 3:  39%|███▉      | 238/604 [01:32<02:30,  2.43it/s, training loss=0.135]\u001b[A\n",
      "Epoch 3:  40%|███▉      | 239/604 [01:32<02:33,  2.38it/s, training loss=0.135]\u001b[A\n",
      "Epoch 3:  40%|███▉      | 239/604 [01:33<02:33,  2.38it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:  40%|███▉      | 240/604 [01:33<02:30,  2.41it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:  40%|███▉      | 240/604 [01:33<02:30,  2.41it/s, training loss=0.172]\u001b[A\n",
      "Epoch 3:  40%|███▉      | 241/604 [01:33<02:27,  2.46it/s, training loss=0.172]\u001b[A\n",
      "Epoch 3:  40%|███▉      | 241/604 [01:34<02:27,  2.46it/s, training loss=0.222]\u001b[A\n",
      "Epoch 3:  40%|████      | 242/604 [01:34<02:25,  2.49it/s, training loss=0.222]\u001b[A\n",
      "Epoch 3:  40%|████      | 242/604 [01:34<02:25,  2.49it/s, training loss=0.119]\u001b[A\n",
      "Epoch 3:  40%|████      | 243/604 [01:34<02:23,  2.52it/s, training loss=0.119]\u001b[A\n",
      "Epoch 3:  40%|████      | 243/604 [01:34<02:23,  2.52it/s, training loss=0.218]\u001b[A\n",
      "Epoch 3:  40%|████      | 244/604 [01:34<02:21,  2.54it/s, training loss=0.218]\u001b[A\n",
      "Epoch 3:  40%|████      | 244/604 [01:35<02:21,  2.54it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:  41%|████      | 245/604 [01:35<02:20,  2.55it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:  41%|████      | 245/604 [01:35<02:20,  2.55it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:  41%|████      | 246/604 [01:35<02:19,  2.57it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:  41%|████      | 246/604 [01:36<02:19,  2.57it/s, training loss=0.158]\u001b[A\n",
      "Epoch 3:  41%|████      | 247/604 [01:36<02:19,  2.56it/s, training loss=0.158]\u001b[A\n",
      "Epoch 3:  41%|████      | 247/604 [01:36<02:19,  2.56it/s, training loss=0.156]\u001b[A\n",
      "Epoch 3:  41%|████      | 248/604 [01:36<02:18,  2.56it/s, training loss=0.156]\u001b[A\n",
      "Epoch 3:  41%|████      | 248/604 [01:36<02:18,  2.56it/s, training loss=0.217]\u001b[A\n",
      "Epoch 3:  41%|████      | 249/604 [01:36<02:20,  2.52it/s, training loss=0.217]\u001b[A\n",
      "Epoch 3:  41%|████      | 249/604 [01:37<02:20,  2.52it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  41%|████▏     | 250/604 [01:37<02:25,  2.43it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  41%|████▏     | 250/604 [01:37<02:25,  2.43it/s, training loss=0.217]\u001b[A\n",
      "Epoch 3:  42%|████▏     | 251/604 [01:37<02:25,  2.42it/s, training loss=0.217]\u001b[A\n",
      "Epoch 3:  42%|████▏     | 251/604 [01:38<02:25,  2.42it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:  42%|████▏     | 252/604 [01:38<02:23,  2.46it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:  42%|████▏     | 252/604 [01:38<02:23,  2.46it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:  42%|████▏     | 253/604 [01:38<02:21,  2.48it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:  42%|████▏     | 253/604 [01:38<02:21,  2.48it/s, training loss=0.099]\u001b[A\n",
      "Epoch 3:  42%|████▏     | 254/604 [01:38<02:19,  2.51it/s, training loss=0.099]\u001b[A\n",
      "Epoch 3:  42%|████▏     | 254/604 [01:39<02:19,  2.51it/s, training loss=0.289]\u001b[A\n",
      "Epoch 3:  42%|████▏     | 255/604 [01:39<02:17,  2.54it/s, training loss=0.289]\u001b[A\n",
      "Epoch 3:  42%|████▏     | 255/604 [01:39<02:17,  2.54it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:  42%|████▏     | 256/604 [01:39<02:16,  2.55it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:  42%|████▏     | 256/604 [01:40<02:16,  2.55it/s, training loss=0.106]\u001b[A\n",
      "Epoch 3:  43%|████▎     | 257/604 [01:40<02:17,  2.52it/s, training loss=0.106]\u001b[A\n",
      "Epoch 3:  43%|████▎     | 257/604 [01:40<02:17,  2.52it/s, training loss=0.125]\u001b[A\n",
      "Epoch 3:  43%|████▎     | 258/604 [01:40<02:16,  2.53it/s, training loss=0.125]\u001b[A\n",
      "Epoch 3:  43%|████▎     | 258/604 [01:40<02:16,  2.53it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:  43%|████▎     | 259/604 [01:40<02:15,  2.55it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:  43%|████▎     | 259/604 [01:41<02:15,  2.55it/s, training loss=0.120]\u001b[A\n",
      "Epoch 3:  43%|████▎     | 260/604 [01:41<02:14,  2.56it/s, training loss=0.120]\u001b[A\n",
      "Epoch 3:  43%|████▎     | 260/604 [01:41<02:14,  2.56it/s, training loss=0.177]\u001b[A\n",
      "Epoch 3:  43%|████▎     | 261/604 [01:41<02:13,  2.57it/s, training loss=0.177]\u001b[A\n",
      "Epoch 3:  43%|████▎     | 261/604 [01:42<02:13,  2.57it/s, training loss=0.117]\u001b[A\n",
      "Epoch 3:  43%|████▎     | 262/604 [01:42<02:12,  2.58it/s, training loss=0.117]\u001b[A\n",
      "Epoch 3:  43%|████▎     | 262/604 [01:42<02:12,  2.58it/s, training loss=0.158]\u001b[A\n",
      "Epoch 3:  44%|████▎     | 263/604 [01:42<02:12,  2.58it/s, training loss=0.158]\u001b[A\n",
      "Epoch 3:  44%|████▎     | 263/604 [01:42<02:12,  2.58it/s, training loss=0.182]\u001b[A\n",
      "Epoch 3:  44%|████▎     | 264/604 [01:42<02:11,  2.58it/s, training loss=0.182]\u001b[A\n",
      "Epoch 3:  44%|████▎     | 264/604 [01:43<02:11,  2.58it/s, training loss=0.171]\u001b[A\n",
      "Epoch 3:  44%|████▍     | 265/604 [01:43<02:11,  2.58it/s, training loss=0.171]\u001b[A\n",
      "Epoch 3:  44%|████▍     | 265/604 [01:43<02:11,  2.58it/s, training loss=0.162]\u001b[A\n",
      "Epoch 3:  44%|████▍     | 266/604 [01:43<02:10,  2.59it/s, training loss=0.162]\u001b[A\n",
      "Epoch 3:  44%|████▍     | 266/604 [01:43<02:10,  2.59it/s, training loss=0.203]\u001b[A\n",
      "Epoch 3:  44%|████▍     | 267/604 [01:43<02:10,  2.59it/s, training loss=0.203]\u001b[A\n",
      "Epoch 3:  44%|████▍     | 267/604 [01:44<02:10,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 3:  44%|████▍     | 268/604 [01:44<02:09,  2.60it/s, training loss=0.169]\u001b[A\n",
      "Epoch 3:  44%|████▍     | 268/604 [01:44<02:09,  2.60it/s, training loss=0.257]\u001b[A\n",
      "Epoch 3:  45%|████▍     | 269/604 [01:44<02:08,  2.60it/s, training loss=0.257]\u001b[A\n",
      "Epoch 3:  45%|████▍     | 269/604 [01:45<02:08,  2.60it/s, training loss=0.082]\u001b[A\n",
      "Epoch 3:  45%|████▍     | 270/604 [01:45<02:08,  2.60it/s, training loss=0.082]\u001b[A\n",
      "Epoch 3:  45%|████▍     | 270/604 [01:45<02:08,  2.60it/s, training loss=0.168]\u001b[A\n",
      "Epoch 3:  45%|████▍     | 271/604 [01:45<02:08,  2.58it/s, training loss=0.168]\u001b[A\n",
      "Epoch 3:  45%|████▍     | 271/604 [01:45<02:08,  2.58it/s, training loss=0.227]\u001b[A\n",
      "Epoch 3:  45%|████▌     | 272/604 [01:45<02:08,  2.58it/s, training loss=0.227]\u001b[A\n",
      "Epoch 3:  45%|████▌     | 272/604 [01:46<02:08,  2.58it/s, training loss=0.172]\u001b[A\n",
      "Epoch 3:  45%|████▌     | 273/604 [01:46<02:07,  2.59it/s, training loss=0.172]\u001b[A\n",
      "Epoch 3:  45%|████▌     | 273/604 [01:46<02:07,  2.59it/s, training loss=0.144]\u001b[A\n",
      "Epoch 3:  45%|████▌     | 274/604 [01:46<02:07,  2.59it/s, training loss=0.144]\u001b[A\n",
      "Epoch 3:  45%|████▌     | 274/604 [01:47<02:07,  2.59it/s, training loss=0.197]\u001b[A\n",
      "Epoch 3:  46%|████▌     | 275/604 [01:47<02:06,  2.60it/s, training loss=0.197]\u001b[A\n",
      "Epoch 3:  46%|████▌     | 275/604 [01:47<02:06,  2.60it/s, training loss=0.243]\u001b[A\n",
      "Epoch 3:  46%|████▌     | 276/604 [01:47<02:06,  2.60it/s, training loss=0.243]\u001b[A\n",
      "Epoch 3:  46%|████▌     | 276/604 [01:47<02:06,  2.60it/s, training loss=0.136]\u001b[A\n",
      "Epoch 3:  46%|████▌     | 277/604 [01:47<02:05,  2.60it/s, training loss=0.136]\u001b[A\n",
      "Epoch 3:  46%|████▌     | 277/604 [01:48<02:05,  2.60it/s, training loss=0.180]\u001b[A\n",
      "Epoch 3:  46%|████▌     | 278/604 [01:48<02:07,  2.55it/s, training loss=0.180]\u001b[A\n",
      "Epoch 3:  46%|████▌     | 278/604 [01:48<02:07,  2.55it/s, training loss=0.136]\u001b[A\n",
      "Epoch 3:  46%|████▌     | 279/604 [01:48<02:08,  2.53it/s, training loss=0.136]\u001b[A\n",
      "Epoch 3:  46%|████▌     | 279/604 [01:49<02:08,  2.53it/s, training loss=0.149]\u001b[A\n",
      "Epoch 3:  46%|████▋     | 280/604 [01:49<02:07,  2.54it/s, training loss=0.149]\u001b[A\n",
      "Epoch 3:  46%|████▋     | 280/604 [01:49<02:07,  2.54it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  47%|████▋     | 281/604 [01:49<02:06,  2.55it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  47%|████▋     | 281/604 [01:49<02:06,  2.55it/s, training loss=0.297]\u001b[A\n",
      "Epoch 3:  47%|████▋     | 282/604 [01:49<02:05,  2.56it/s, training loss=0.297]\u001b[A\n",
      "Epoch 3:  47%|████▋     | 282/604 [01:50<02:05,  2.56it/s, training loss=0.241]\u001b[A\n",
      "Epoch 3:  47%|████▋     | 283/604 [01:50<02:05,  2.57it/s, training loss=0.241]\u001b[A\n",
      "Epoch 3:  47%|████▋     | 283/604 [01:50<02:05,  2.57it/s, training loss=0.194]\u001b[A\n",
      "Epoch 3:  47%|████▋     | 284/604 [01:50<02:04,  2.58it/s, training loss=0.194]\u001b[A\n",
      "Epoch 3:  47%|████▋     | 284/604 [01:50<02:04,  2.58it/s, training loss=0.129]\u001b[A\n",
      "Epoch 3:  47%|████▋     | 285/604 [01:50<02:04,  2.56it/s, training loss=0.129]\u001b[A\n",
      "Epoch 3:  47%|████▋     | 285/604 [01:51<02:04,  2.56it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:  47%|████▋     | 286/604 [01:51<02:03,  2.57it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:  47%|████▋     | 286/604 [01:51<02:03,  2.57it/s, training loss=0.171]\u001b[A\n",
      "Epoch 3:  48%|████▊     | 287/604 [01:51<02:02,  2.58it/s, training loss=0.171]\u001b[A\n",
      "Epoch 3:  48%|████▊     | 287/604 [01:52<02:02,  2.58it/s, training loss=0.153]\u001b[A\n",
      "Epoch 3:  48%|████▊     | 288/604 [01:52<02:02,  2.58it/s, training loss=0.153]\u001b[A\n",
      "Epoch 3:  48%|████▊     | 288/604 [01:52<02:02,  2.58it/s, training loss=0.241]\u001b[A\n",
      "Epoch 3:  48%|████▊     | 289/604 [01:52<02:01,  2.58it/s, training loss=0.241]\u001b[A\n",
      "Epoch 3:  48%|████▊     | 289/604 [01:52<02:01,  2.58it/s, training loss=0.141]\u001b[A\n",
      "Epoch 3:  48%|████▊     | 290/604 [01:52<02:01,  2.59it/s, training loss=0.141]\u001b[A\n",
      "Epoch 3:  48%|████▊     | 290/604 [01:53<02:01,  2.59it/s, training loss=0.267]\u001b[A\n",
      "Epoch 3:  48%|████▊     | 291/604 [01:53<02:00,  2.59it/s, training loss=0.267]\u001b[A\n",
      "Epoch 3:  48%|████▊     | 291/604 [01:53<02:00,  2.59it/s, training loss=0.183]\u001b[A\n",
      "Epoch 3:  48%|████▊     | 292/604 [01:53<02:01,  2.58it/s, training loss=0.183]\u001b[A\n",
      "Epoch 3:  48%|████▊     | 292/604 [01:54<02:01,  2.58it/s, training loss=0.203]\u001b[A\n",
      "Epoch 3:  49%|████▊     | 293/604 [01:54<02:00,  2.58it/s, training loss=0.203]\u001b[A\n",
      "Epoch 3:  49%|████▊     | 293/604 [01:54<02:00,  2.58it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  49%|████▊     | 294/604 [01:54<01:59,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  49%|████▊     | 294/604 [01:54<01:59,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 3:  49%|████▉     | 295/604 [01:54<01:59,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 3:  49%|████▉     | 295/604 [01:55<01:59,  2.59it/s, training loss=0.131]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  49%|████▉     | 296/604 [01:55<01:58,  2.59it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:  49%|████▉     | 296/604 [01:55<01:58,  2.59it/s, training loss=0.216]\u001b[A\n",
      "Epoch 3:  49%|████▉     | 297/604 [01:55<01:58,  2.59it/s, training loss=0.216]\u001b[A\n",
      "Epoch 3:  49%|████▉     | 297/604 [01:55<01:58,  2.59it/s, training loss=0.142]\u001b[A\n",
      "Epoch 3:  49%|████▉     | 298/604 [01:55<01:58,  2.59it/s, training loss=0.142]\u001b[A\n",
      "Epoch 3:  49%|████▉     | 298/604 [01:56<01:58,  2.59it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:  50%|████▉     | 299/604 [01:56<01:57,  2.59it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:  50%|████▉     | 299/604 [01:56<01:57,  2.59it/s, training loss=0.257]\u001b[A\n",
      "Epoch 3:  50%|████▉     | 300/604 [01:56<01:57,  2.59it/s, training loss=0.257]\u001b[A\n",
      "Epoch 3:  50%|████▉     | 300/604 [01:57<01:57,  2.59it/s, training loss=0.153]\u001b[A\n",
      "Epoch 3:  50%|████▉     | 301/604 [01:57<01:57,  2.59it/s, training loss=0.153]\u001b[A\n",
      "Epoch 3:  50%|████▉     | 301/604 [01:57<01:57,  2.59it/s, training loss=0.202]\u001b[A\n",
      "Epoch 3:  50%|█████     | 302/604 [01:57<01:56,  2.59it/s, training loss=0.202]\u001b[A\n",
      "Epoch 3:  50%|█████     | 302/604 [01:57<01:56,  2.59it/s, training loss=0.164]\u001b[A\n",
      "Epoch 3:  50%|█████     | 303/604 [01:57<01:56,  2.59it/s, training loss=0.164]\u001b[A\n",
      "Epoch 3:  50%|█████     | 303/604 [01:58<01:56,  2.59it/s, training loss=0.114]\u001b[A\n",
      "Epoch 3:  50%|█████     | 304/604 [01:58<01:56,  2.58it/s, training loss=0.114]\u001b[A\n",
      "Epoch 3:  50%|█████     | 304/604 [01:58<01:56,  2.58it/s, training loss=0.147]\u001b[A\n",
      "Epoch 3:  50%|█████     | 305/604 [01:58<01:55,  2.58it/s, training loss=0.147]\u001b[A\n",
      "Epoch 3:  50%|█████     | 305/604 [01:59<01:55,  2.58it/s, training loss=0.173]\u001b[A\n",
      "Epoch 3:  51%|█████     | 306/604 [01:59<01:55,  2.58it/s, training loss=0.173]\u001b[A\n",
      "Epoch 3:  51%|█████     | 306/604 [01:59<01:55,  2.58it/s, training loss=0.127]\u001b[A\n",
      "Epoch 3:  51%|█████     | 307/604 [01:59<01:57,  2.54it/s, training loss=0.127]\u001b[A\n",
      "Epoch 3:  51%|█████     | 307/604 [01:59<01:57,  2.54it/s, training loss=0.157]\u001b[A\n",
      "Epoch 3:  51%|█████     | 308/604 [01:59<01:56,  2.54it/s, training loss=0.157]\u001b[A\n",
      "Epoch 3:  51%|█████     | 308/604 [02:00<01:56,  2.54it/s, training loss=0.249]\u001b[A\n",
      "Epoch 3:  51%|█████     | 309/604 [02:00<01:55,  2.55it/s, training loss=0.249]\u001b[A\n",
      "Epoch 3:  51%|█████     | 309/604 [02:00<01:55,  2.55it/s, training loss=0.167]\u001b[A\n",
      "Epoch 3:  51%|█████▏    | 310/604 [02:00<01:54,  2.56it/s, training loss=0.167]\u001b[A\n",
      "Epoch 3:  51%|█████▏    | 310/604 [02:01<01:54,  2.56it/s, training loss=0.213]\u001b[A\n",
      "Epoch 3:  51%|█████▏    | 311/604 [02:01<01:54,  2.55it/s, training loss=0.213]\u001b[A\n",
      "Epoch 3:  51%|█████▏    | 311/604 [02:01<01:54,  2.55it/s, training loss=0.146]\u001b[A\n",
      "Epoch 3:  52%|█████▏    | 312/604 [02:01<01:53,  2.57it/s, training loss=0.146]\u001b[A\n",
      "Epoch 3:  52%|█████▏    | 312/604 [02:01<01:53,  2.57it/s, training loss=0.104]\u001b[A\n",
      "Epoch 3:  52%|█████▏    | 313/604 [02:01<01:53,  2.57it/s, training loss=0.104]\u001b[A\n",
      "Epoch 3:  52%|█████▏    | 313/604 [02:02<01:53,  2.57it/s, training loss=0.156]\u001b[A\n",
      "Epoch 3:  52%|█████▏    | 314/604 [02:02<01:54,  2.54it/s, training loss=0.156]\u001b[A\n",
      "Epoch 3:  52%|█████▏    | 314/604 [02:02<01:54,  2.54it/s, training loss=0.080]\u001b[A\n",
      "Epoch 3:  52%|█████▏    | 315/604 [02:02<01:53,  2.54it/s, training loss=0.080]\u001b[A\n",
      "Epoch 3:  52%|█████▏    | 315/604 [02:03<01:53,  2.54it/s, training loss=0.184]\u001b[A\n",
      "Epoch 3:  52%|█████▏    | 316/604 [02:03<01:52,  2.56it/s, training loss=0.184]\u001b[A\n",
      "Epoch 3:  52%|█████▏    | 316/604 [02:03<01:52,  2.56it/s, training loss=0.112]\u001b[A\n",
      "Epoch 3:  52%|█████▏    | 317/604 [02:03<01:51,  2.57it/s, training loss=0.112]\u001b[A\n",
      "Epoch 3:  52%|█████▏    | 317/604 [02:03<01:51,  2.57it/s, training loss=0.220]\u001b[A\n",
      "Epoch 3:  53%|█████▎    | 318/604 [02:03<01:51,  2.57it/s, training loss=0.220]\u001b[A\n",
      "Epoch 3:  53%|█████▎    | 318/604 [02:04<01:51,  2.57it/s, training loss=0.237]\u001b[A\n",
      "Epoch 3:  53%|█████▎    | 319/604 [02:04<01:50,  2.58it/s, training loss=0.237]\u001b[A\n",
      "Epoch 3:  53%|█████▎    | 319/604 [02:04<01:50,  2.58it/s, training loss=0.144]\u001b[A\n",
      "Epoch 3:  53%|█████▎    | 320/604 [02:04<01:49,  2.58it/s, training loss=0.144]\u001b[A\n",
      "Epoch 3:  53%|█████▎    | 320/604 [02:04<01:49,  2.58it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:  53%|█████▎    | 321/604 [02:04<01:49,  2.59it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:  53%|█████▎    | 321/604 [02:05<01:49,  2.59it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:  53%|█████▎    | 322/604 [02:05<01:48,  2.59it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:  53%|█████▎    | 322/604 [02:05<01:48,  2.59it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  53%|█████▎    | 323/604 [02:05<01:48,  2.60it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  53%|█████▎    | 323/604 [02:06<01:48,  2.60it/s, training loss=0.192]\u001b[A\n",
      "Epoch 3:  54%|█████▎    | 324/604 [02:06<01:47,  2.60it/s, training loss=0.192]\u001b[A\n",
      "Epoch 3:  54%|█████▎    | 324/604 [02:06<01:47,  2.60it/s, training loss=0.168]\u001b[A\n",
      "Epoch 3:  54%|█████▍    | 325/604 [02:06<01:47,  2.60it/s, training loss=0.168]\u001b[A\n",
      "Epoch 3:  54%|█████▍    | 325/604 [02:06<01:47,  2.60it/s, training loss=0.157]\u001b[A\n",
      "Epoch 3:  54%|█████▍    | 326/604 [02:06<01:47,  2.60it/s, training loss=0.157]\u001b[A\n",
      "Epoch 3:  54%|█████▍    | 326/604 [02:07<01:47,  2.60it/s, training loss=0.127]\u001b[A\n",
      "Epoch 3:  54%|█████▍    | 327/604 [02:07<01:46,  2.60it/s, training loss=0.127]\u001b[A\n",
      "Epoch 3:  54%|█████▍    | 327/604 [02:07<01:46,  2.60it/s, training loss=0.173]\u001b[A\n",
      "Epoch 3:  54%|█████▍    | 328/604 [02:07<01:46,  2.60it/s, training loss=0.173]\u001b[A\n",
      "Epoch 3:  54%|█████▍    | 328/604 [02:08<01:46,  2.60it/s, training loss=0.239]\u001b[A\n",
      "Epoch 3:  54%|█████▍    | 329/604 [02:08<01:45,  2.60it/s, training loss=0.239]\u001b[A\n",
      "Epoch 3:  54%|█████▍    | 329/604 [02:08<01:45,  2.60it/s, training loss=0.262]\u001b[A\n",
      "Epoch 3:  55%|█████▍    | 330/604 [02:08<01:45,  2.60it/s, training loss=0.262]\u001b[A\n",
      "Epoch 3:  55%|█████▍    | 330/604 [02:08<01:45,  2.60it/s, training loss=0.107]\u001b[A\n",
      "Epoch 3:  55%|█████▍    | 331/604 [02:08<01:45,  2.59it/s, training loss=0.107]\u001b[A\n",
      "Epoch 3:  55%|█████▍    | 331/604 [02:09<01:45,  2.59it/s, training loss=0.126]\u001b[A\n",
      "Epoch 3:  55%|█████▍    | 332/604 [02:09<01:44,  2.59it/s, training loss=0.126]\u001b[A\n",
      "Epoch 3:  55%|█████▍    | 332/604 [02:09<01:44,  2.59it/s, training loss=0.152]\u001b[A\n",
      "Epoch 3:  55%|█████▌    | 333/604 [02:09<01:44,  2.58it/s, training loss=0.152]\u001b[A\n",
      "Epoch 3:  55%|█████▌    | 333/604 [02:09<01:44,  2.58it/s, training loss=0.175]\u001b[A\n",
      "Epoch 3:  55%|█████▌    | 334/604 [02:09<01:45,  2.57it/s, training loss=0.175]\u001b[A\n",
      "Epoch 3:  55%|█████▌    | 334/604 [02:10<01:45,  2.57it/s, training loss=0.158]\u001b[A\n",
      "Epoch 3:  55%|█████▌    | 335/604 [02:10<01:45,  2.55it/s, training loss=0.158]\u001b[A\n",
      "Epoch 3:  55%|█████▌    | 335/604 [02:10<01:45,  2.55it/s, training loss=0.138]\u001b[A\n",
      "Epoch 3:  56%|█████▌    | 336/604 [02:10<01:44,  2.56it/s, training loss=0.138]\u001b[A\n",
      "Epoch 3:  56%|█████▌    | 336/604 [02:11<01:44,  2.56it/s, training loss=0.196]\u001b[A\n",
      "Epoch 3:  56%|█████▌    | 337/604 [02:11<01:44,  2.55it/s, training loss=0.196]\u001b[A\n",
      "Epoch 3:  56%|█████▌    | 337/604 [02:11<01:44,  2.55it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:  56%|█████▌    | 338/604 [02:11<01:43,  2.56it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:  56%|█████▌    | 338/604 [02:11<01:43,  2.56it/s, training loss=0.237]\u001b[A\n",
      "Epoch 3:  56%|█████▌    | 339/604 [02:11<01:43,  2.57it/s, training loss=0.237]\u001b[A\n",
      "Epoch 3:  56%|█████▌    | 339/604 [02:12<01:43,  2.57it/s, training loss=0.113]\u001b[A\n",
      "Epoch 3:  56%|█████▋    | 340/604 [02:12<01:42,  2.58it/s, training loss=0.113]\u001b[A\n",
      "Epoch 3:  56%|█████▋    | 340/604 [02:12<01:42,  2.58it/s, training loss=0.205]\u001b[A\n",
      "Epoch 3:  56%|█████▋    | 341/604 [02:12<01:41,  2.58it/s, training loss=0.205]\u001b[A\n",
      "Epoch 3:  56%|█████▋    | 341/604 [02:13<01:41,  2.58it/s, training loss=0.126]\u001b[A\n",
      "Epoch 3:  57%|█████▋    | 342/604 [02:13<01:41,  2.59it/s, training loss=0.126]\u001b[A\n",
      "Epoch 3:  57%|█████▋    | 342/604 [02:13<01:41,  2.59it/s, training loss=0.150]\u001b[A\n",
      "Epoch 3:  57%|█████▋    | 343/604 [02:13<01:40,  2.59it/s, training loss=0.150]\u001b[A\n",
      "Epoch 3:  57%|█████▋    | 343/604 [02:13<01:40,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  57%|█████▋    | 344/604 [02:13<01:40,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  57%|█████▋    | 344/604 [02:14<01:40,  2.59it/s, training loss=0.127]\u001b[A\n",
      "Epoch 3:  57%|█████▋    | 345/604 [02:14<01:40,  2.59it/s, training loss=0.127]\u001b[A\n",
      "Epoch 3:  57%|█████▋    | 345/604 [02:14<01:40,  2.59it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:  57%|█████▋    | 346/604 [02:14<01:39,  2.59it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:  57%|█████▋    | 346/604 [02:15<01:39,  2.59it/s, training loss=0.156]\u001b[A\n",
      "Epoch 3:  57%|█████▋    | 347/604 [02:15<01:39,  2.59it/s, training loss=0.156]\u001b[A\n",
      "Epoch 3:  57%|█████▋    | 347/604 [02:15<01:39,  2.59it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:  58%|█████▊    | 348/604 [02:15<01:38,  2.60it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:  58%|█████▊    | 348/604 [02:15<01:38,  2.60it/s, training loss=0.223]\u001b[A\n",
      "Epoch 3:  58%|█████▊    | 349/604 [02:15<01:38,  2.60it/s, training loss=0.223]\u001b[A\n",
      "Epoch 3:  58%|█████▊    | 349/604 [02:16<01:38,  2.60it/s, training loss=0.151]\u001b[A\n",
      "Epoch 3:  58%|█████▊    | 350/604 [02:16<01:37,  2.60it/s, training loss=0.151]\u001b[A\n",
      "Epoch 3:  58%|█████▊    | 350/604 [02:16<01:37,  2.60it/s, training loss=0.217]\u001b[A\n",
      "Epoch 3:  58%|█████▊    | 351/604 [02:16<01:37,  2.60it/s, training loss=0.217]\u001b[A\n",
      "Epoch 3:  58%|█████▊    | 351/604 [02:16<01:37,  2.60it/s, training loss=0.118]\u001b[A\n",
      "Epoch 3:  58%|█████▊    | 352/604 [02:16<01:36,  2.60it/s, training loss=0.118]\u001b[A\n",
      "Epoch 3:  58%|█████▊    | 352/604 [02:17<01:36,  2.60it/s, training loss=0.194]\u001b[A\n",
      "Epoch 3:  58%|█████▊    | 353/604 [02:17<01:36,  2.60it/s, training loss=0.194]\u001b[A\n",
      "Epoch 3:  58%|█████▊    | 353/604 [02:17<01:36,  2.60it/s, training loss=0.172]\u001b[A\n",
      "Epoch 3:  59%|█████▊    | 354/604 [02:17<01:36,  2.60it/s, training loss=0.172]\u001b[A\n",
      "Epoch 3:  59%|█████▊    | 354/604 [02:18<01:36,  2.60it/s, training loss=0.145]\u001b[A\n",
      "Epoch 3:  59%|█████▉    | 355/604 [02:18<01:36,  2.59it/s, training loss=0.145]\u001b[A\n",
      "Epoch 3:  59%|█████▉    | 355/604 [02:18<01:36,  2.59it/s, training loss=0.174]\u001b[A\n",
      "Epoch 3:  59%|█████▉    | 356/604 [02:18<01:35,  2.59it/s, training loss=0.174]\u001b[A\n",
      "Epoch 3:  59%|█████▉    | 356/604 [02:18<01:35,  2.59it/s, training loss=0.115]\u001b[A\n",
      "Epoch 3:  59%|█████▉    | 357/604 [02:18<01:35,  2.59it/s, training loss=0.115]\u001b[A\n",
      "Epoch 3:  59%|█████▉    | 357/604 [02:19<01:35,  2.59it/s, training loss=0.188]\u001b[A\n",
      "Epoch 3:  59%|█████▉    | 358/604 [02:19<01:34,  2.59it/s, training loss=0.188]\u001b[A\n",
      "Epoch 3:  59%|█████▉    | 358/604 [02:19<01:34,  2.59it/s, training loss=0.189]\u001b[A\n",
      "Epoch 3:  59%|█████▉    | 359/604 [02:19<01:34,  2.59it/s, training loss=0.189]\u001b[A\n",
      "Epoch 3:  59%|█████▉    | 359/604 [02:20<01:34,  2.59it/s, training loss=0.164]\u001b[A\n",
      "Epoch 3:  60%|█████▉    | 360/604 [02:20<01:34,  2.59it/s, training loss=0.164]\u001b[A\n",
      "Epoch 3:  60%|█████▉    | 360/604 [02:20<01:34,  2.59it/s, training loss=0.140]\u001b[A\n",
      "Epoch 3:  60%|█████▉    | 361/604 [02:20<01:33,  2.60it/s, training loss=0.140]\u001b[A\n",
      "Epoch 3:  60%|█████▉    | 361/604 [02:20<01:33,  2.60it/s, training loss=0.163]\u001b[A\n",
      "Epoch 3:  60%|█████▉    | 362/604 [02:20<01:33,  2.60it/s, training loss=0.163]\u001b[A\n",
      "Epoch 3:  60%|█████▉    | 362/604 [02:21<01:33,  2.60it/s, training loss=0.096]\u001b[A\n",
      "Epoch 3:  60%|██████    | 363/604 [02:21<01:34,  2.56it/s, training loss=0.096]\u001b[A\n",
      "Epoch 3:  60%|██████    | 363/604 [02:21<01:34,  2.56it/s, training loss=0.171]\u001b[A\n",
      "Epoch 3:  60%|██████    | 364/604 [02:21<01:36,  2.49it/s, training loss=0.171]\u001b[A\n",
      "Epoch 3:  60%|██████    | 364/604 [02:22<01:36,  2.49it/s, training loss=0.206]\u001b[A\n",
      "Epoch 3:  60%|██████    | 365/604 [02:22<01:35,  2.51it/s, training loss=0.206]\u001b[A\n",
      "Epoch 3:  60%|██████    | 365/604 [02:22<01:35,  2.51it/s, training loss=0.220]\u001b[A\n",
      "Epoch 3:  61%|██████    | 366/604 [02:22<01:33,  2.53it/s, training loss=0.220]\u001b[A\n",
      "Epoch 3:  61%|██████    | 366/604 [02:22<01:33,  2.53it/s, training loss=0.150]\u001b[A\n",
      "Epoch 3:  61%|██████    | 367/604 [02:22<01:32,  2.55it/s, training loss=0.150]\u001b[A\n",
      "Epoch 3:  61%|██████    | 367/604 [02:23<01:32,  2.55it/s, training loss=0.179]\u001b[A\n",
      "Epoch 3:  61%|██████    | 368/604 [02:23<01:32,  2.55it/s, training loss=0.179]\u001b[A\n",
      "Epoch 3:  61%|██████    | 368/604 [02:23<01:32,  2.55it/s, training loss=0.210]\u001b[A\n",
      "Epoch 3:  61%|██████    | 369/604 [02:23<01:31,  2.56it/s, training loss=0.210]\u001b[A\n",
      "Epoch 3:  61%|██████    | 369/604 [02:23<01:31,  2.56it/s, training loss=0.203]\u001b[A\n",
      "Epoch 3:  61%|██████▏   | 370/604 [02:23<01:31,  2.56it/s, training loss=0.203]\u001b[A\n",
      "Epoch 3:  61%|██████▏   | 370/604 [02:24<01:31,  2.56it/s, training loss=0.243]\u001b[A\n",
      "Epoch 3:  61%|██████▏   | 371/604 [02:24<01:30,  2.57it/s, training loss=0.243]\u001b[A\n",
      "Epoch 3:  61%|██████▏   | 371/604 [02:24<01:30,  2.57it/s, training loss=0.147]\u001b[A\n",
      "Epoch 3:  62%|██████▏   | 372/604 [02:24<01:30,  2.57it/s, training loss=0.147]\u001b[A\n",
      "Epoch 3:  62%|██████▏   | 372/604 [02:25<01:30,  2.57it/s, training loss=0.201]\u001b[A\n",
      "Epoch 3:  62%|██████▏   | 373/604 [02:25<01:29,  2.58it/s, training loss=0.201]\u001b[A\n",
      "Epoch 3:  62%|██████▏   | 373/604 [02:25<01:29,  2.58it/s, training loss=0.179]\u001b[A\n",
      "Epoch 3:  62%|██████▏   | 374/604 [02:25<01:29,  2.58it/s, training loss=0.179]\u001b[A\n",
      "Epoch 3:  62%|██████▏   | 374/604 [02:25<01:29,  2.58it/s, training loss=0.207]\u001b[A\n",
      "Epoch 3:  62%|██████▏   | 375/604 [02:25<01:28,  2.59it/s, training loss=0.207]\u001b[A\n",
      "Epoch 3:  62%|██████▏   | 375/604 [02:26<01:28,  2.59it/s, training loss=0.115]\u001b[A\n",
      "Epoch 3:  62%|██████▏   | 376/604 [02:26<01:28,  2.59it/s, training loss=0.115]\u001b[A\n",
      "Epoch 3:  62%|██████▏   | 376/604 [02:26<01:28,  2.59it/s, training loss=0.112]\u001b[A\n",
      "Epoch 3:  62%|██████▏   | 377/604 [02:26<01:27,  2.59it/s, training loss=0.112]\u001b[A\n",
      "Epoch 3:  62%|██████▏   | 377/604 [02:27<01:27,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 3:  63%|██████▎   | 378/604 [02:27<01:27,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 3:  63%|██████▎   | 378/604 [02:27<01:27,  2.59it/s, training loss=0.142]\u001b[A\n",
      "Epoch 3:  63%|██████▎   | 379/604 [02:27<01:26,  2.60it/s, training loss=0.142]\u001b[A\n",
      "Epoch 3:  63%|██████▎   | 379/604 [02:27<01:26,  2.60it/s, training loss=0.185]\u001b[A\n",
      "Epoch 3:  63%|██████▎   | 380/604 [02:27<01:26,  2.60it/s, training loss=0.185]\u001b[A\n",
      "Epoch 3:  63%|██████▎   | 380/604 [02:28<01:26,  2.60it/s, training loss=0.133]\u001b[A\n",
      "Epoch 3:  63%|██████▎   | 381/604 [02:28<01:26,  2.59it/s, training loss=0.133]\u001b[A\n",
      "Epoch 3:  63%|██████▎   | 381/604 [02:28<01:26,  2.59it/s, training loss=0.160]\u001b[A\n",
      "Epoch 3:  63%|██████▎   | 382/604 [02:28<01:25,  2.59it/s, training loss=0.160]\u001b[A\n",
      "Epoch 3:  63%|██████▎   | 382/604 [02:28<01:25,  2.59it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:  63%|██████▎   | 383/604 [02:28<01:25,  2.58it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:  63%|██████▎   | 383/604 [02:29<01:25,  2.58it/s, training loss=0.211]\u001b[A\n",
      "Epoch 3:  64%|██████▎   | 384/604 [02:29<01:25,  2.58it/s, training loss=0.211]\u001b[A\n",
      "Epoch 3:  64%|██████▎   | 384/604 [02:29<01:25,  2.58it/s, training loss=0.081]\u001b[A\n",
      "Epoch 3:  64%|██████▎   | 385/604 [02:29<01:24,  2.59it/s, training loss=0.081]\u001b[A\n",
      "Epoch 3:  64%|██████▎   | 385/604 [02:30<01:24,  2.59it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:  64%|██████▍   | 386/604 [02:30<01:24,  2.59it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:  64%|██████▍   | 386/604 [02:30<01:24,  2.59it/s, training loss=0.190]\u001b[A\n",
      "Epoch 3:  64%|██████▍   | 387/604 [02:30<01:23,  2.59it/s, training loss=0.190]\u001b[A\n",
      "Epoch 3:  64%|██████▍   | 387/604 [02:30<01:23,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  64%|██████▍   | 388/604 [02:30<01:23,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  64%|██████▍   | 388/604 [02:31<01:23,  2.59it/s, training loss=0.176]\u001b[A\n",
      "Epoch 3:  64%|██████▍   | 389/604 [02:31<01:22,  2.59it/s, training loss=0.176]\u001b[A\n",
      "Epoch 3:  64%|██████▍   | 389/604 [02:31<01:22,  2.59it/s, training loss=0.120]\u001b[A\n",
      "Epoch 3:  65%|██████▍   | 390/604 [02:31<01:22,  2.60it/s, training loss=0.120]\u001b[A\n",
      "Epoch 3:  65%|██████▍   | 390/604 [02:32<01:22,  2.60it/s, training loss=0.126]\u001b[A\n",
      "Epoch 3:  65%|██████▍   | 391/604 [02:32<01:22,  2.58it/s, training loss=0.126]\u001b[A\n",
      "Epoch 3:  65%|██████▍   | 391/604 [02:32<01:22,  2.58it/s, training loss=0.116]\u001b[A\n",
      "Epoch 3:  65%|██████▍   | 392/604 [02:32<01:31,  2.31it/s, training loss=0.116]\u001b[A\n",
      "Epoch 3:  65%|██████▍   | 392/604 [02:33<01:31,  2.31it/s, training loss=0.151]\u001b[A\n",
      "Epoch 3:  65%|██████▌   | 393/604 [02:33<01:30,  2.34it/s, training loss=0.151]\u001b[A\n",
      "Epoch 3:  65%|██████▌   | 393/604 [02:33<01:30,  2.34it/s, training loss=0.094]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  65%|██████▌   | 394/604 [02:33<01:27,  2.41it/s, training loss=0.094]\u001b[A\n",
      "Epoch 3:  65%|██████▌   | 394/604 [02:33<01:27,  2.41it/s, training loss=0.160]\u001b[A\n",
      "Epoch 3:  65%|██████▌   | 395/604 [02:33<01:31,  2.29it/s, training loss=0.160]\u001b[A\n",
      "Epoch 3:  65%|██████▌   | 395/604 [02:34<01:31,  2.29it/s, training loss=0.108]\u001b[A\n",
      "Epoch 3:  66%|██████▌   | 396/604 [02:34<01:30,  2.30it/s, training loss=0.108]\u001b[A\n",
      "Epoch 3:  66%|██████▌   | 396/604 [02:34<01:30,  2.30it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:  66%|██████▌   | 397/604 [02:34<01:27,  2.38it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:  66%|██████▌   | 397/604 [02:35<01:27,  2.38it/s, training loss=0.191]\u001b[A\n",
      "Epoch 3:  66%|██████▌   | 398/604 [02:35<01:25,  2.41it/s, training loss=0.191]\u001b[A\n",
      "Epoch 3:  66%|██████▌   | 398/604 [02:35<01:25,  2.41it/s, training loss=0.170]\u001b[A\n",
      "Epoch 3:  66%|██████▌   | 399/604 [02:35<01:23,  2.46it/s, training loss=0.170]\u001b[A\n",
      "Epoch 3:  66%|██████▌   | 399/604 [02:35<01:23,  2.46it/s, training loss=0.189]\u001b[A\n",
      "Epoch 3:  66%|██████▌   | 400/604 [02:35<01:21,  2.49it/s, training loss=0.189]\u001b[A\n",
      "Epoch 3:  66%|██████▌   | 400/604 [02:36<01:21,  2.49it/s, training loss=0.205]\u001b[A\n",
      "Epoch 3:  66%|██████▋   | 401/604 [02:36<01:20,  2.52it/s, training loss=0.205]\u001b[A\n",
      "Epoch 3:  66%|██████▋   | 401/604 [02:36<01:20,  2.52it/s, training loss=0.144]\u001b[A\n",
      "Epoch 3:  67%|██████▋   | 402/604 [02:36<01:19,  2.54it/s, training loss=0.144]\u001b[A\n",
      "Epoch 3:  67%|██████▋   | 402/604 [02:37<01:19,  2.54it/s, training loss=0.170]\u001b[A\n",
      "Epoch 3:  67%|██████▋   | 403/604 [02:37<01:19,  2.54it/s, training loss=0.170]\u001b[A\n",
      "Epoch 3:  67%|██████▋   | 403/604 [02:37<01:19,  2.54it/s, training loss=0.111]\u001b[A\n",
      "Epoch 3:  67%|██████▋   | 404/604 [02:37<01:18,  2.56it/s, training loss=0.111]\u001b[A\n",
      "Epoch 3:  67%|██████▋   | 404/604 [02:37<01:18,  2.56it/s, training loss=0.118]\u001b[A\n",
      "Epoch 3:  67%|██████▋   | 405/604 [02:37<01:17,  2.57it/s, training loss=0.118]\u001b[A\n",
      "Epoch 3:  67%|██████▋   | 405/604 [02:38<01:17,  2.57it/s, training loss=0.092]\u001b[A\n",
      "Epoch 3:  67%|██████▋   | 406/604 [02:38<01:17,  2.55it/s, training loss=0.092]\u001b[A\n",
      "Epoch 3:  67%|██████▋   | 406/604 [02:38<01:17,  2.55it/s, training loss=0.140]\u001b[A\n",
      "Epoch 3:  67%|██████▋   | 407/604 [02:38<01:18,  2.52it/s, training loss=0.140]\u001b[A\n",
      "Epoch 3:  67%|██████▋   | 407/604 [02:39<01:18,  2.52it/s, training loss=0.225]\u001b[A\n",
      "Epoch 3:  68%|██████▊   | 408/604 [02:39<01:17,  2.53it/s, training loss=0.225]\u001b[A\n",
      "Epoch 3:  68%|██████▊   | 408/604 [02:39<01:17,  2.53it/s, training loss=0.140]\u001b[A\n",
      "Epoch 3:  68%|██████▊   | 409/604 [02:39<01:16,  2.55it/s, training loss=0.140]\u001b[A\n",
      "Epoch 3:  68%|██████▊   | 409/604 [02:39<01:16,  2.55it/s, training loss=0.173]\u001b[A\n",
      "Epoch 3:  68%|██████▊   | 410/604 [02:39<01:15,  2.56it/s, training loss=0.173]\u001b[A\n",
      "Epoch 3:  68%|██████▊   | 410/604 [02:40<01:15,  2.56it/s, training loss=0.191]\u001b[A\n",
      "Epoch 3:  68%|██████▊   | 411/604 [02:40<01:16,  2.52it/s, training loss=0.191]\u001b[A\n",
      "Epoch 3:  68%|██████▊   | 411/604 [02:40<01:16,  2.52it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:  68%|██████▊   | 412/604 [02:40<01:15,  2.53it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:  68%|██████▊   | 412/604 [02:40<01:15,  2.53it/s, training loss=0.190]\u001b[A\n",
      "Epoch 3:  68%|██████▊   | 413/604 [02:40<01:14,  2.55it/s, training loss=0.190]\u001b[A\n",
      "Epoch 3:  68%|██████▊   | 413/604 [02:41<01:14,  2.55it/s, training loss=0.174]\u001b[A\n",
      "Epoch 3:  69%|██████▊   | 414/604 [02:41<01:14,  2.56it/s, training loss=0.174]\u001b[A\n",
      "Epoch 3:  69%|██████▊   | 414/604 [02:41<01:14,  2.56it/s, training loss=0.148]\u001b[A\n",
      "Epoch 3:  69%|██████▊   | 415/604 [02:41<01:13,  2.57it/s, training loss=0.148]\u001b[A\n",
      "Epoch 3:  69%|██████▊   | 415/604 [02:42<01:13,  2.57it/s, training loss=0.121]\u001b[A\n",
      "Epoch 3:  69%|██████▉   | 416/604 [02:42<01:12,  2.58it/s, training loss=0.121]\u001b[A\n",
      "Epoch 3:  69%|██████▉   | 416/604 [02:42<01:12,  2.58it/s, training loss=0.096]\u001b[A\n",
      "Epoch 3:  69%|██████▉   | 417/604 [02:42<01:12,  2.58it/s, training loss=0.096]\u001b[A\n",
      "Epoch 3:  69%|██████▉   | 417/604 [02:42<01:12,  2.58it/s, training loss=0.155]\u001b[A\n",
      "Epoch 3:  69%|██████▉   | 418/604 [02:42<01:12,  2.58it/s, training loss=0.155]\u001b[A\n",
      "Epoch 3:  69%|██████▉   | 418/604 [02:43<01:12,  2.58it/s, training loss=0.205]\u001b[A\n",
      "Epoch 3:  69%|██████▉   | 419/604 [02:43<01:12,  2.55it/s, training loss=0.205]\u001b[A\n",
      "Epoch 3:  69%|██████▉   | 419/604 [02:43<01:12,  2.55it/s, training loss=0.140]\u001b[A\n",
      "Epoch 3:  70%|██████▉   | 420/604 [02:43<01:15,  2.45it/s, training loss=0.140]\u001b[A\n",
      "Epoch 3:  70%|██████▉   | 420/604 [02:44<01:15,  2.45it/s, training loss=0.252]\u001b[A\n",
      "Epoch 3:  70%|██████▉   | 421/604 [02:44<01:14,  2.47it/s, training loss=0.252]\u001b[A\n",
      "Epoch 3:  70%|██████▉   | 421/604 [02:44<01:14,  2.47it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  70%|██████▉   | 422/604 [02:44<01:12,  2.51it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  70%|██████▉   | 422/604 [02:44<01:12,  2.51it/s, training loss=0.146]\u001b[A\n",
      "Epoch 3:  70%|███████   | 423/604 [02:44<01:11,  2.53it/s, training loss=0.146]\u001b[A\n",
      "Epoch 3:  70%|███████   | 423/604 [02:45<01:11,  2.53it/s, training loss=0.173]\u001b[A\n",
      "Epoch 3:  70%|███████   | 424/604 [02:45<01:10,  2.55it/s, training loss=0.173]\u001b[A\n",
      "Epoch 3:  70%|███████   | 424/604 [02:45<01:10,  2.55it/s, training loss=0.204]\u001b[A\n",
      "Epoch 3:  70%|███████   | 425/604 [02:45<01:09,  2.56it/s, training loss=0.204]\u001b[A\n",
      "Epoch 3:  70%|███████   | 425/604 [02:46<01:09,  2.56it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  71%|███████   | 426/604 [02:46<01:09,  2.57it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  71%|███████   | 426/604 [02:46<01:09,  2.57it/s, training loss=0.215]\u001b[A\n",
      "Epoch 3:  71%|███████   | 427/604 [02:46<01:08,  2.58it/s, training loss=0.215]\u001b[A\n",
      "Epoch 3:  71%|███████   | 427/604 [02:46<01:08,  2.58it/s, training loss=0.107]\u001b[A\n",
      "Epoch 3:  71%|███████   | 428/604 [02:46<01:08,  2.58it/s, training loss=0.107]\u001b[A\n",
      "Epoch 3:  71%|███████   | 428/604 [02:47<01:08,  2.58it/s, training loss=0.122]\u001b[A\n",
      "Epoch 3:  71%|███████   | 429/604 [02:47<01:07,  2.59it/s, training loss=0.122]\u001b[A\n",
      "Epoch 3:  71%|███████   | 429/604 [02:47<01:07,  2.59it/s, training loss=0.183]\u001b[A\n",
      "Epoch 3:  71%|███████   | 430/604 [02:47<01:07,  2.59it/s, training loss=0.183]\u001b[A\n",
      "Epoch 3:  71%|███████   | 430/604 [02:48<01:07,  2.59it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  71%|███████▏  | 431/604 [02:48<01:06,  2.59it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  71%|███████▏  | 431/604 [02:48<01:06,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  72%|███████▏  | 432/604 [02:48<01:06,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  72%|███████▏  | 432/604 [02:48<01:06,  2.59it/s, training loss=0.148]\u001b[A\n",
      "Epoch 3:  72%|███████▏  | 433/604 [02:48<01:06,  2.59it/s, training loss=0.148]\u001b[A\n",
      "Epoch 3:  72%|███████▏  | 433/604 [02:49<01:06,  2.59it/s, training loss=0.170]\u001b[A\n",
      "Epoch 3:  72%|███████▏  | 434/604 [02:49<01:05,  2.59it/s, training loss=0.170]\u001b[A\n",
      "Epoch 3:  72%|███████▏  | 434/604 [02:49<01:05,  2.59it/s, training loss=0.199]\u001b[A\n",
      "Epoch 3:  72%|███████▏  | 435/604 [02:49<01:05,  2.59it/s, training loss=0.199]\u001b[A\n",
      "Epoch 3:  72%|███████▏  | 435/604 [02:49<01:05,  2.59it/s, training loss=0.151]\u001b[A\n",
      "Epoch 3:  72%|███████▏  | 436/604 [02:49<01:04,  2.59it/s, training loss=0.151]\u001b[A\n",
      "Epoch 3:  72%|███████▏  | 436/604 [02:50<01:04,  2.59it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:  72%|███████▏  | 437/604 [02:50<01:04,  2.59it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:  72%|███████▏  | 437/604 [02:50<01:04,  2.59it/s, training loss=0.187]\u001b[A\n",
      "Epoch 3:  73%|███████▎  | 438/604 [02:50<01:04,  2.59it/s, training loss=0.187]\u001b[A\n",
      "Epoch 3:  73%|███████▎  | 438/604 [02:51<01:04,  2.59it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  73%|███████▎  | 439/604 [02:51<01:03,  2.59it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  73%|███████▎  | 439/604 [02:51<01:03,  2.59it/s, training loss=0.117]\u001b[A\n",
      "Epoch 3:  73%|███████▎  | 440/604 [02:51<01:03,  2.60it/s, training loss=0.117]\u001b[A\n",
      "Epoch 3:  73%|███████▎  | 440/604 [02:51<01:03,  2.60it/s, training loss=0.180]\u001b[A\n",
      "Epoch 3:  73%|███████▎  | 441/604 [02:51<01:03,  2.58it/s, training loss=0.180]\u001b[A\n",
      "Epoch 3:  73%|███████▎  | 441/604 [02:52<01:03,  2.58it/s, training loss=0.157]\u001b[A\n",
      "Epoch 3:  73%|███████▎  | 442/604 [02:52<01:02,  2.58it/s, training loss=0.157]\u001b[A\n",
      "Epoch 3:  73%|███████▎  | 442/604 [02:52<01:02,  2.58it/s, training loss=0.204]\u001b[A\n",
      "Epoch 3:  73%|███████▎  | 443/604 [02:52<01:02,  2.58it/s, training loss=0.204]\u001b[A\n",
      "Epoch 3:  73%|███████▎  | 443/604 [02:53<01:02,  2.58it/s, training loss=0.264]\u001b[A\n",
      "Epoch 3:  74%|███████▎  | 444/604 [02:53<01:01,  2.59it/s, training loss=0.264]\u001b[A\n",
      "Epoch 3:  74%|███████▎  | 444/604 [02:53<01:01,  2.59it/s, training loss=0.174]\u001b[A\n",
      "Epoch 3:  74%|███████▎  | 445/604 [02:53<01:01,  2.59it/s, training loss=0.174]\u001b[A\n",
      "Epoch 3:  74%|███████▎  | 445/604 [02:53<01:01,  2.59it/s, training loss=0.146]\u001b[A\n",
      "Epoch 3:  74%|███████▍  | 446/604 [02:53<01:00,  2.59it/s, training loss=0.146]\u001b[A\n",
      "Epoch 3:  74%|███████▍  | 446/604 [02:54<01:00,  2.59it/s, training loss=0.115]\u001b[A\n",
      "Epoch 3:  74%|███████▍  | 447/604 [02:54<01:00,  2.59it/s, training loss=0.115]\u001b[A\n",
      "Epoch 3:  74%|███████▍  | 447/604 [02:54<01:00,  2.59it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:  74%|███████▍  | 448/604 [02:54<01:01,  2.53it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:  74%|███████▍  | 448/604 [02:54<01:01,  2.53it/s, training loss=0.172]\u001b[A\n",
      "Epoch 3:  74%|███████▍  | 449/604 [02:55<01:01,  2.52it/s, training loss=0.172]\u001b[A\n",
      "Epoch 3:  74%|███████▍  | 449/604 [02:55<01:01,  2.52it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:  75%|███████▍  | 450/604 [02:55<01:00,  2.53it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:  75%|███████▍  | 450/604 [02:55<01:00,  2.53it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:  75%|███████▍  | 451/604 [02:55<01:00,  2.54it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:  75%|███████▍  | 451/604 [02:56<01:00,  2.54it/s, training loss=0.128]\u001b[A\n",
      "Epoch 3:  75%|███████▍  | 452/604 [02:56<00:59,  2.55it/s, training loss=0.128]\u001b[A\n",
      "Epoch 3:  75%|███████▍  | 452/604 [02:56<00:59,  2.55it/s, training loss=0.170]\u001b[A\n",
      "Epoch 3:  75%|███████▌  | 453/604 [02:56<00:59,  2.56it/s, training loss=0.170]\u001b[A\n",
      "Epoch 3:  75%|███████▌  | 453/604 [02:56<00:59,  2.56it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:  75%|███████▌  | 454/604 [02:56<00:58,  2.55it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:  75%|███████▌  | 454/604 [02:57<00:58,  2.55it/s, training loss=0.193]\u001b[A\n",
      "Epoch 3:  75%|███████▌  | 455/604 [02:57<00:58,  2.57it/s, training loss=0.193]\u001b[A\n",
      "Epoch 3:  75%|███████▌  | 455/604 [02:57<00:58,  2.57it/s, training loss=0.155]\u001b[A\n",
      "Epoch 3:  75%|███████▌  | 456/604 [02:57<00:57,  2.57it/s, training loss=0.155]\u001b[A\n",
      "Epoch 3:  75%|███████▌  | 456/604 [02:58<00:57,  2.57it/s, training loss=0.138]\u001b[A\n",
      "Epoch 3:  76%|███████▌  | 457/604 [02:58<00:57,  2.57it/s, training loss=0.138]\u001b[A\n",
      "Epoch 3:  76%|███████▌  | 457/604 [02:58<00:57,  2.57it/s, training loss=0.238]\u001b[A\n",
      "Epoch 3:  76%|███████▌  | 458/604 [02:58<00:56,  2.58it/s, training loss=0.238]\u001b[A\n",
      "Epoch 3:  76%|███████▌  | 458/604 [02:58<00:56,  2.58it/s, training loss=0.238]\u001b[A\n",
      "Epoch 3:  76%|███████▌  | 459/604 [02:58<00:56,  2.58it/s, training loss=0.238]\u001b[A\n",
      "Epoch 3:  76%|███████▌  | 459/604 [02:59<00:56,  2.58it/s, training loss=0.187]\u001b[A\n",
      "Epoch 3:  76%|███████▌  | 460/604 [02:59<00:55,  2.59it/s, training loss=0.187]\u001b[A\n",
      "Epoch 3:  76%|███████▌  | 460/604 [02:59<00:55,  2.59it/s, training loss=0.222]\u001b[A\n",
      "Epoch 3:  76%|███████▋  | 461/604 [02:59<00:55,  2.59it/s, training loss=0.222]\u001b[A\n",
      "Epoch 3:  76%|███████▋  | 461/604 [03:00<00:55,  2.59it/s, training loss=0.197]\u001b[A\n",
      "Epoch 3:  76%|███████▋  | 462/604 [03:00<00:54,  2.59it/s, training loss=0.197]\u001b[A\n",
      "Epoch 3:  76%|███████▋  | 462/604 [03:00<00:54,  2.59it/s, training loss=0.187]\u001b[A\n",
      "Epoch 3:  77%|███████▋  | 463/604 [03:00<00:54,  2.60it/s, training loss=0.187]\u001b[A\n",
      "Epoch 3:  77%|███████▋  | 463/604 [03:00<00:54,  2.60it/s, training loss=0.128]\u001b[A\n",
      "Epoch 3:  77%|███████▋  | 464/604 [03:00<00:53,  2.60it/s, training loss=0.128]\u001b[A\n",
      "Epoch 3:  77%|███████▋  | 464/604 [03:01<00:53,  2.60it/s, training loss=0.172]\u001b[A\n",
      "Epoch 3:  77%|███████▋  | 465/604 [03:01<00:53,  2.60it/s, training loss=0.172]\u001b[A\n",
      "Epoch 3:  77%|███████▋  | 465/604 [03:01<00:53,  2.60it/s, training loss=0.149]\u001b[A\n",
      "Epoch 3:  77%|███████▋  | 466/604 [03:01<00:53,  2.60it/s, training loss=0.149]\u001b[A\n",
      "Epoch 3:  77%|███████▋  | 466/604 [03:01<00:53,  2.60it/s, training loss=0.157]\u001b[A\n",
      "Epoch 3:  77%|███████▋  | 467/604 [03:01<00:52,  2.60it/s, training loss=0.157]\u001b[A\n",
      "Epoch 3:  77%|███████▋  | 467/604 [03:02<00:52,  2.60it/s, training loss=0.112]\u001b[A\n",
      "Epoch 3:  77%|███████▋  | 468/604 [03:02<00:52,  2.60it/s, training loss=0.112]\u001b[A\n",
      "Epoch 3:  77%|███████▋  | 468/604 [03:02<00:52,  2.60it/s, training loss=0.206]\u001b[A\n",
      "Epoch 3:  78%|███████▊  | 469/604 [03:02<00:51,  2.60it/s, training loss=0.206]\u001b[A\n",
      "Epoch 3:  78%|███████▊  | 469/604 [03:03<00:51,  2.60it/s, training loss=0.099]\u001b[A\n",
      "Epoch 3:  78%|███████▊  | 470/604 [03:03<00:51,  2.60it/s, training loss=0.099]\u001b[A\n",
      "Epoch 3:  78%|███████▊  | 470/604 [03:03<00:51,  2.60it/s, training loss=0.163]\u001b[A\n",
      "Epoch 3:  78%|███████▊  | 471/604 [03:03<00:51,  2.60it/s, training loss=0.163]\u001b[A\n",
      "Epoch 3:  78%|███████▊  | 471/604 [03:03<00:51,  2.60it/s, training loss=0.163]\u001b[A\n",
      "Epoch 3:  78%|███████▊  | 472/604 [03:03<00:50,  2.60it/s, training loss=0.163]\u001b[A\n",
      "Epoch 3:  78%|███████▊  | 472/604 [03:04<00:50,  2.60it/s, training loss=0.243]\u001b[A\n",
      "Epoch 3:  78%|███████▊  | 473/604 [03:04<00:50,  2.60it/s, training loss=0.243]\u001b[A\n",
      "Epoch 3:  78%|███████▊  | 473/604 [03:04<00:50,  2.60it/s, training loss=0.162]\u001b[A\n",
      "Epoch 3:  78%|███████▊  | 474/604 [03:04<00:50,  2.60it/s, training loss=0.162]\u001b[A\n",
      "Epoch 3:  78%|███████▊  | 474/604 [03:05<00:50,  2.60it/s, training loss=0.180]\u001b[A\n",
      "Epoch 3:  79%|███████▊  | 475/604 [03:05<00:50,  2.57it/s, training loss=0.180]\u001b[A\n",
      "Epoch 3:  79%|███████▊  | 475/604 [03:05<00:50,  2.57it/s, training loss=0.162]\u001b[A\n",
      "Epoch 3:  79%|███████▉  | 476/604 [03:05<00:50,  2.53it/s, training loss=0.162]\u001b[A\n",
      "Epoch 3:  79%|███████▉  | 476/604 [03:05<00:50,  2.53it/s, training loss=0.230]\u001b[A\n",
      "Epoch 3:  79%|███████▉  | 477/604 [03:05<00:49,  2.54it/s, training loss=0.230]\u001b[A\n",
      "Epoch 3:  79%|███████▉  | 477/604 [03:06<00:49,  2.54it/s, training loss=0.186]\u001b[A\n",
      "Epoch 3:  79%|███████▉  | 478/604 [03:06<00:49,  2.56it/s, training loss=0.186]\u001b[A\n",
      "Epoch 3:  79%|███████▉  | 478/604 [03:06<00:49,  2.56it/s, training loss=0.256]\u001b[A\n",
      "Epoch 3:  79%|███████▉  | 479/604 [03:06<00:48,  2.57it/s, training loss=0.256]\u001b[A\n",
      "Epoch 3:  79%|███████▉  | 479/604 [03:07<00:48,  2.57it/s, training loss=0.110]\u001b[A\n",
      "Epoch 3:  79%|███████▉  | 480/604 [03:07<00:48,  2.57it/s, training loss=0.110]\u001b[A\n",
      "Epoch 3:  79%|███████▉  | 480/604 [03:07<00:48,  2.57it/s, training loss=0.145]\u001b[A\n",
      "Epoch 3:  80%|███████▉  | 481/604 [03:07<00:47,  2.58it/s, training loss=0.145]\u001b[A\n",
      "Epoch 3:  80%|███████▉  | 481/604 [03:07<00:47,  2.58it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  80%|███████▉  | 482/604 [03:07<00:47,  2.57it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  80%|███████▉  | 482/604 [03:08<00:47,  2.57it/s, training loss=0.186]\u001b[A\n",
      "Epoch 3:  80%|███████▉  | 483/604 [03:08<00:46,  2.58it/s, training loss=0.186]\u001b[A\n",
      "Epoch 3:  80%|███████▉  | 483/604 [03:08<00:46,  2.58it/s, training loss=0.222]\u001b[A\n",
      "Epoch 3:  80%|████████  | 484/604 [03:08<00:46,  2.58it/s, training loss=0.222]\u001b[A\n",
      "Epoch 3:  80%|████████  | 484/604 [03:08<00:46,  2.58it/s, training loss=0.192]\u001b[A\n",
      "Epoch 3:  80%|████████  | 485/604 [03:08<00:46,  2.57it/s, training loss=0.192]\u001b[A\n",
      "Epoch 3:  80%|████████  | 485/604 [03:09<00:46,  2.57it/s, training loss=0.158]\u001b[A\n",
      "Epoch 3:  80%|████████  | 486/604 [03:09<00:45,  2.58it/s, training loss=0.158]\u001b[A\n",
      "Epoch 3:  80%|████████  | 486/604 [03:09<00:45,  2.58it/s, training loss=0.160]\u001b[A\n",
      "Epoch 3:  81%|████████  | 487/604 [03:09<00:45,  2.57it/s, training loss=0.160]\u001b[A\n",
      "Epoch 3:  81%|████████  | 487/604 [03:10<00:45,  2.57it/s, training loss=0.105]\u001b[A\n",
      "Epoch 3:  81%|████████  | 488/604 [03:10<00:44,  2.58it/s, training loss=0.105]\u001b[A\n",
      "Epoch 3:  81%|████████  | 488/604 [03:10<00:44,  2.58it/s, training loss=0.152]\u001b[A\n",
      "Epoch 3:  81%|████████  | 489/604 [03:10<00:44,  2.59it/s, training loss=0.152]\u001b[A\n",
      "Epoch 3:  81%|████████  | 489/604 [03:10<00:44,  2.59it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:  81%|████████  | 490/604 [03:10<00:43,  2.59it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:  81%|████████  | 490/604 [03:11<00:43,  2.59it/s, training loss=0.188]\u001b[A\n",
      "Epoch 3:  81%|████████▏ | 491/604 [03:11<00:43,  2.59it/s, training loss=0.188]\u001b[A\n",
      "Epoch 3:  81%|████████▏ | 491/604 [03:11<00:43,  2.59it/s, training loss=0.154]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  81%|████████▏ | 492/604 [03:11<00:43,  2.59it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:  81%|████████▏ | 492/604 [03:12<00:43,  2.59it/s, training loss=0.262]\u001b[A\n",
      "Epoch 3:  82%|████████▏ | 493/604 [03:12<00:43,  2.57it/s, training loss=0.262]\u001b[A\n",
      "Epoch 3:  82%|████████▏ | 493/604 [03:12<00:43,  2.57it/s, training loss=0.188]\u001b[A\n",
      "Epoch 3:  82%|████████▏ | 494/604 [03:12<00:42,  2.57it/s, training loss=0.188]\u001b[A\n",
      "Epoch 3:  82%|████████▏ | 494/604 [03:12<00:42,  2.57it/s, training loss=0.117]\u001b[A\n",
      "Epoch 3:  82%|████████▏ | 495/604 [03:12<00:42,  2.57it/s, training loss=0.117]\u001b[A\n",
      "Epoch 3:  82%|████████▏ | 495/604 [03:13<00:42,  2.57it/s, training loss=0.189]\u001b[A\n",
      "Epoch 3:  82%|████████▏ | 496/604 [03:13<00:42,  2.57it/s, training loss=0.189]\u001b[A\n",
      "Epoch 3:  82%|████████▏ | 496/604 [03:13<00:42,  2.57it/s, training loss=0.109]\u001b[A\n",
      "Epoch 3:  82%|████████▏ | 497/604 [03:13<00:41,  2.58it/s, training loss=0.109]\u001b[A\n",
      "Epoch 3:  82%|████████▏ | 497/604 [03:13<00:41,  2.58it/s, training loss=0.192]\u001b[A\n",
      "Epoch 3:  82%|████████▏ | 498/604 [03:13<00:41,  2.58it/s, training loss=0.192]\u001b[A\n",
      "Epoch 3:  82%|████████▏ | 498/604 [03:14<00:41,  2.58it/s, training loss=0.194]\u001b[A\n",
      "Epoch 3:  83%|████████▎ | 499/604 [03:14<00:40,  2.59it/s, training loss=0.194]\u001b[A\n",
      "Epoch 3:  83%|████████▎ | 499/604 [03:14<00:40,  2.59it/s, training loss=0.140]\u001b[A\n",
      "Epoch 3:  83%|████████▎ | 500/604 [03:14<00:40,  2.59it/s, training loss=0.140]\u001b[A\n",
      "Epoch 3:  83%|████████▎ | 500/604 [03:15<00:40,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 3:  83%|████████▎ | 501/604 [03:15<00:39,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 3:  83%|████████▎ | 501/604 [03:15<00:39,  2.59it/s, training loss=0.232]\u001b[A\n",
      "Epoch 3:  83%|████████▎ | 502/604 [03:15<00:39,  2.59it/s, training loss=0.232]\u001b[A\n",
      "Epoch 3:  83%|████████▎ | 502/604 [03:15<00:39,  2.59it/s, training loss=0.193]\u001b[A\n",
      "Epoch 3:  83%|████████▎ | 503/604 [03:15<00:38,  2.60it/s, training loss=0.193]\u001b[A\n",
      "Epoch 3:  83%|████████▎ | 503/604 [03:16<00:38,  2.60it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:  83%|████████▎ | 504/604 [03:16<00:39,  2.55it/s, training loss=0.130]\u001b[A\n",
      "Epoch 3:  83%|████████▎ | 504/604 [03:16<00:39,  2.55it/s, training loss=0.208]\u001b[A\n",
      "Epoch 3:  84%|████████▎ | 505/604 [03:16<00:39,  2.51it/s, training loss=0.208]\u001b[A\n",
      "Epoch 3:  84%|████████▎ | 505/604 [03:17<00:39,  2.51it/s, training loss=0.129]\u001b[A\n",
      "Epoch 3:  84%|████████▍ | 506/604 [03:17<00:39,  2.51it/s, training loss=0.129]\u001b[A\n",
      "Epoch 3:  84%|████████▍ | 506/604 [03:17<00:39,  2.51it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  84%|████████▍ | 507/604 [03:17<00:38,  2.54it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  84%|████████▍ | 507/604 [03:17<00:38,  2.54it/s, training loss=0.151]\u001b[A\n",
      "Epoch 3:  84%|████████▍ | 508/604 [03:17<00:37,  2.55it/s, training loss=0.151]\u001b[A\n",
      "Epoch 3:  84%|████████▍ | 508/604 [03:18<00:37,  2.55it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  84%|████████▍ | 509/604 [03:18<00:37,  2.55it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  84%|████████▍ | 509/604 [03:18<00:37,  2.55it/s, training loss=0.239]\u001b[A\n",
      "Epoch 3:  84%|████████▍ | 510/604 [03:18<00:36,  2.57it/s, training loss=0.239]\u001b[A\n",
      "Epoch 3:  84%|████████▍ | 510/604 [03:19<00:36,  2.57it/s, training loss=0.076]\u001b[A\n",
      "Epoch 3:  85%|████████▍ | 511/604 [03:19<00:36,  2.58it/s, training loss=0.076]\u001b[A\n",
      "Epoch 3:  85%|████████▍ | 511/604 [03:19<00:36,  2.58it/s, training loss=0.187]\u001b[A\n",
      "Epoch 3:  85%|████████▍ | 512/604 [03:19<00:35,  2.58it/s, training loss=0.187]\u001b[A\n",
      "Epoch 3:  85%|████████▍ | 512/604 [03:19<00:35,  2.58it/s, training loss=0.127]\u001b[A\n",
      "Epoch 3:  85%|████████▍ | 513/604 [03:19<00:35,  2.58it/s, training loss=0.127]\u001b[A\n",
      "Epoch 3:  85%|████████▍ | 513/604 [03:20<00:35,  2.58it/s, training loss=0.150]\u001b[A\n",
      "Epoch 3:  85%|████████▌ | 514/604 [03:20<00:34,  2.59it/s, training loss=0.150]\u001b[A\n",
      "Epoch 3:  85%|████████▌ | 514/604 [03:20<00:34,  2.59it/s, training loss=0.133]\u001b[A\n",
      "Epoch 3:  85%|████████▌ | 515/604 [03:20<00:34,  2.59it/s, training loss=0.133]\u001b[A\n",
      "Epoch 3:  85%|████████▌ | 515/604 [03:20<00:34,  2.59it/s, training loss=0.128]\u001b[A\n",
      "Epoch 3:  85%|████████▌ | 516/604 [03:20<00:33,  2.59it/s, training loss=0.128]\u001b[A\n",
      "Epoch 3:  85%|████████▌ | 516/604 [03:21<00:33,  2.59it/s, training loss=0.166]\u001b[A\n",
      "Epoch 3:  86%|████████▌ | 517/604 [03:21<00:33,  2.59it/s, training loss=0.166]\u001b[A\n",
      "Epoch 3:  86%|████████▌ | 517/604 [03:21<00:33,  2.59it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:  86%|████████▌ | 518/604 [03:21<00:33,  2.59it/s, training loss=0.131]\u001b[A\n",
      "Epoch 3:  86%|████████▌ | 518/604 [03:22<00:33,  2.59it/s, training loss=0.174]\u001b[A\n",
      "Epoch 3:  86%|████████▌ | 519/604 [03:22<00:32,  2.60it/s, training loss=0.174]\u001b[A\n",
      "Epoch 3:  86%|████████▌ | 519/604 [03:22<00:32,  2.60it/s, training loss=0.183]\u001b[A\n",
      "Epoch 3:  86%|████████▌ | 520/604 [03:22<00:32,  2.60it/s, training loss=0.183]\u001b[A\n",
      "Epoch 3:  86%|████████▌ | 520/604 [03:22<00:32,  2.60it/s, training loss=0.163]\u001b[A\n",
      "Epoch 3:  86%|████████▋ | 521/604 [03:22<00:31,  2.60it/s, training loss=0.163]\u001b[A\n",
      "Epoch 3:  86%|████████▋ | 521/604 [03:23<00:31,  2.60it/s, training loss=0.125]\u001b[A\n",
      "Epoch 3:  86%|████████▋ | 522/604 [03:23<00:31,  2.60it/s, training loss=0.125]\u001b[A\n",
      "Epoch 3:  86%|████████▋ | 522/604 [03:23<00:31,  2.60it/s, training loss=0.179]\u001b[A\n",
      "Epoch 3:  87%|████████▋ | 523/604 [03:23<00:31,  2.60it/s, training loss=0.179]\u001b[A\n",
      "Epoch 3:  87%|████████▋ | 523/604 [03:24<00:31,  2.60it/s, training loss=0.271]\u001b[A\n",
      "Epoch 3:  87%|████████▋ | 524/604 [03:24<00:30,  2.60it/s, training loss=0.271]\u001b[A\n",
      "Epoch 3:  87%|████████▋ | 524/604 [03:24<00:30,  2.60it/s, training loss=0.124]\u001b[A\n",
      "Epoch 3:  87%|████████▋ | 525/604 [03:24<00:30,  2.60it/s, training loss=0.124]\u001b[A\n",
      "Epoch 3:  87%|████████▋ | 525/604 [03:24<00:30,  2.60it/s, training loss=0.078]\u001b[A\n",
      "Epoch 3:  87%|████████▋ | 526/604 [03:24<00:30,  2.60it/s, training loss=0.078]\u001b[A\n",
      "Epoch 3:  87%|████████▋ | 526/604 [03:25<00:30,  2.60it/s, training loss=0.190]\u001b[A\n",
      "Epoch 3:  87%|████████▋ | 527/604 [03:25<00:29,  2.60it/s, training loss=0.190]\u001b[A\n",
      "Epoch 3:  87%|████████▋ | 527/604 [03:25<00:29,  2.60it/s, training loss=0.182]\u001b[A\n",
      "Epoch 3:  87%|████████▋ | 528/604 [03:25<00:29,  2.60it/s, training loss=0.182]\u001b[A\n",
      "Epoch 3:  87%|████████▋ | 528/604 [03:25<00:29,  2.60it/s, training loss=0.269]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 529/604 [03:26<00:28,  2.60it/s, training loss=0.269]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 529/604 [03:26<00:28,  2.60it/s, training loss=0.242]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 530/604 [03:26<00:28,  2.59it/s, training loss=0.242]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 530/604 [03:26<00:28,  2.59it/s, training loss=0.242]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 531/604 [03:26<00:28,  2.59it/s, training loss=0.242]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 531/604 [03:27<00:28,  2.59it/s, training loss=0.121]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 532/604 [03:27<00:27,  2.59it/s, training loss=0.121]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 532/604 [03:27<00:27,  2.59it/s, training loss=0.251]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 533/604 [03:27<00:27,  2.54it/s, training loss=0.251]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 533/604 [03:27<00:27,  2.54it/s, training loss=0.162]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 534/604 [03:27<00:27,  2.54it/s, training loss=0.162]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 534/604 [03:28<00:27,  2.54it/s, training loss=0.197]\u001b[A\n",
      "Epoch 3:  89%|████████▊ | 535/604 [03:28<00:27,  2.55it/s, training loss=0.197]\u001b[A\n",
      "Epoch 3:  89%|████████▊ | 535/604 [03:28<00:27,  2.55it/s, training loss=0.125]\u001b[A\n",
      "Epoch 3:  89%|████████▊ | 536/604 [03:28<00:26,  2.57it/s, training loss=0.125]\u001b[A\n",
      "Epoch 3:  89%|████████▊ | 536/604 [03:29<00:26,  2.57it/s, training loss=0.158]\u001b[A\n",
      "Epoch 3:  89%|████████▉ | 537/604 [03:29<00:26,  2.57it/s, training loss=0.158]\u001b[A\n",
      "Epoch 3:  89%|████████▉ | 537/604 [03:29<00:26,  2.57it/s, training loss=0.175]\u001b[A\n",
      "Epoch 3:  89%|████████▉ | 538/604 [03:29<00:25,  2.58it/s, training loss=0.175]\u001b[A\n",
      "Epoch 3:  89%|████████▉ | 538/604 [03:29<00:25,  2.58it/s, training loss=0.146]\u001b[A\n",
      "Epoch 3:  89%|████████▉ | 539/604 [03:29<00:25,  2.56it/s, training loss=0.146]\u001b[A\n",
      "Epoch 3:  89%|████████▉ | 539/604 [03:30<00:25,  2.56it/s, training loss=0.152]\u001b[A\n",
      "Epoch 3:  89%|████████▉ | 540/604 [03:30<00:24,  2.57it/s, training loss=0.152]\u001b[A\n",
      "Epoch 3:  89%|████████▉ | 540/604 [03:30<00:24,  2.57it/s, training loss=0.180]\u001b[A\n",
      "Epoch 3:  90%|████████▉ | 541/604 [03:30<00:24,  2.58it/s, training loss=0.180]\u001b[A\n",
      "Epoch 3:  90%|████████▉ | 541/604 [03:31<00:24,  2.58it/s, training loss=0.171]\u001b[A\n",
      "Epoch 3:  90%|████████▉ | 542/604 [03:31<00:24,  2.58it/s, training loss=0.171]\u001b[A\n",
      "Epoch 3:  90%|████████▉ | 542/604 [03:31<00:24,  2.58it/s, training loss=0.115]\u001b[A\n",
      "Epoch 3:  90%|████████▉ | 543/604 [03:31<00:23,  2.59it/s, training loss=0.115]\u001b[A\n",
      "Epoch 3:  90%|████████▉ | 543/604 [03:31<00:23,  2.59it/s, training loss=0.126]\u001b[A\n",
      "Epoch 3:  90%|█████████ | 544/604 [03:31<00:23,  2.59it/s, training loss=0.126]\u001b[A\n",
      "Epoch 3:  90%|█████████ | 544/604 [03:32<00:23,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 3:  90%|█████████ | 545/604 [03:32<00:22,  2.59it/s, training loss=0.169]\u001b[A\n",
      "Epoch 3:  90%|█████████ | 545/604 [03:32<00:22,  2.59it/s, training loss=0.188]\u001b[A\n",
      "Epoch 3:  90%|█████████ | 546/604 [03:32<00:23,  2.50it/s, training loss=0.188]\u001b[A\n",
      "Epoch 3:  90%|█████████ | 546/604 [03:33<00:23,  2.50it/s, training loss=0.180]\u001b[A\n",
      "Epoch 3:  91%|█████████ | 547/604 [03:33<00:23,  2.45it/s, training loss=0.180]\u001b[A\n",
      "Epoch 3:  91%|█████████ | 547/604 [03:33<00:23,  2.45it/s, training loss=0.139]\u001b[A\n",
      "Epoch 3:  91%|█████████ | 548/604 [03:33<00:22,  2.48it/s, training loss=0.139]\u001b[A\n",
      "Epoch 3:  91%|█████████ | 548/604 [03:33<00:22,  2.48it/s, training loss=0.123]\u001b[A\n",
      "Epoch 3:  91%|█████████ | 549/604 [03:33<00:21,  2.51it/s, training loss=0.123]\u001b[A\n",
      "Epoch 3:  91%|█████████ | 549/604 [03:34<00:21,  2.51it/s, training loss=0.217]\u001b[A\n",
      "Epoch 3:  91%|█████████ | 550/604 [03:34<00:21,  2.48it/s, training loss=0.217]\u001b[A\n",
      "Epoch 3:  91%|█████████ | 550/604 [03:34<00:21,  2.48it/s, training loss=0.133]\u001b[A\n",
      "Epoch 3:  91%|█████████ | 551/604 [03:34<00:21,  2.52it/s, training loss=0.133]\u001b[A\n",
      "Epoch 3:  91%|█████████ | 551/604 [03:35<00:21,  2.52it/s, training loss=0.170]\u001b[A\n",
      "Epoch 3:  91%|█████████▏| 552/604 [03:35<00:20,  2.52it/s, training loss=0.170]\u001b[A\n",
      "Epoch 3:  91%|█████████▏| 552/604 [03:35<00:20,  2.52it/s, training loss=0.105]\u001b[A\n",
      "Epoch 3:  92%|█████████▏| 553/604 [03:35<00:20,  2.54it/s, training loss=0.105]\u001b[A\n",
      "Epoch 3:  92%|█████████▏| 553/604 [03:35<00:20,  2.54it/s, training loss=0.141]\u001b[A\n",
      "Epoch 3:  92%|█████████▏| 554/604 [03:35<00:19,  2.56it/s, training loss=0.141]\u001b[A\n",
      "Epoch 3:  92%|█████████▏| 554/604 [03:36<00:19,  2.56it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:  92%|█████████▏| 555/604 [03:36<00:19,  2.57it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:  92%|█████████▏| 555/604 [03:36<00:19,  2.57it/s, training loss=0.177]\u001b[A\n",
      "Epoch 3:  92%|█████████▏| 556/604 [03:36<00:19,  2.51it/s, training loss=0.177]\u001b[A\n",
      "Epoch 3:  92%|█████████▏| 556/604 [03:37<00:19,  2.51it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:  92%|█████████▏| 557/604 [03:37<00:18,  2.52it/s, training loss=0.154]\u001b[A\n",
      "Epoch 3:  92%|█████████▏| 557/604 [03:37<00:18,  2.52it/s, training loss=0.110]\u001b[A\n",
      "Epoch 3:  92%|█████████▏| 558/604 [03:37<00:18,  2.55it/s, training loss=0.110]\u001b[A\n",
      "Epoch 3:  92%|█████████▏| 558/604 [03:37<00:18,  2.55it/s, training loss=0.127]\u001b[A\n",
      "Epoch 3:  93%|█████████▎| 559/604 [03:37<00:17,  2.51it/s, training loss=0.127]\u001b[A\n",
      "Epoch 3:  93%|█████████▎| 559/604 [03:38<00:17,  2.51it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  93%|█████████▎| 560/604 [03:38<00:17,  2.53it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  93%|█████████▎| 560/604 [03:38<00:17,  2.53it/s, training loss=0.151]\u001b[A\n",
      "Epoch 3:  93%|█████████▎| 561/604 [03:38<00:17,  2.46it/s, training loss=0.151]\u001b[A\n",
      "Epoch 3:  93%|█████████▎| 561/604 [03:39<00:17,  2.46it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:  93%|█████████▎| 562/604 [03:39<00:16,  2.49it/s, training loss=0.165]\u001b[A\n",
      "Epoch 3:  93%|█████████▎| 562/604 [03:39<00:16,  2.49it/s, training loss=0.192]\u001b[A\n",
      "Epoch 3:  93%|█████████▎| 563/604 [03:39<00:16,  2.51it/s, training loss=0.192]\u001b[A\n",
      "Epoch 3:  93%|█████████▎| 563/604 [03:39<00:16,  2.51it/s, training loss=0.083]\u001b[A\n",
      "Epoch 3:  93%|█████████▎| 564/604 [03:39<00:15,  2.54it/s, training loss=0.083]\u001b[A\n",
      "Epoch 3:  93%|█████████▎| 564/604 [03:40<00:15,  2.54it/s, training loss=0.185]\u001b[A\n",
      "Epoch 3:  94%|█████████▎| 565/604 [03:40<00:15,  2.55it/s, training loss=0.185]\u001b[A\n",
      "Epoch 3:  94%|█████████▎| 565/604 [03:40<00:15,  2.55it/s, training loss=0.213]\u001b[A\n",
      "Epoch 3:  94%|█████████▎| 566/604 [03:40<00:14,  2.56it/s, training loss=0.213]\u001b[A\n",
      "Epoch 3:  94%|█████████▎| 566/604 [03:40<00:14,  2.56it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 567/604 [03:40<00:14,  2.57it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 567/604 [03:41<00:14,  2.57it/s, training loss=0.119]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 568/604 [03:41<00:14,  2.57it/s, training loss=0.119]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 568/604 [03:41<00:14,  2.57it/s, training loss=0.145]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 569/604 [03:41<00:13,  2.57it/s, training loss=0.145]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 569/604 [03:42<00:13,  2.57it/s, training loss=0.136]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 570/604 [03:42<00:13,  2.58it/s, training loss=0.136]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 570/604 [03:42<00:13,  2.58it/s, training loss=0.180]\u001b[A\n",
      "Epoch 3:  95%|█████████▍| 571/604 [03:42<00:12,  2.58it/s, training loss=0.180]\u001b[A\n",
      "Epoch 3:  95%|█████████▍| 571/604 [03:42<00:12,  2.58it/s, training loss=0.183]\u001b[A\n",
      "Epoch 3:  95%|█████████▍| 572/604 [03:42<00:12,  2.57it/s, training loss=0.183]\u001b[A\n",
      "Epoch 3:  95%|█████████▍| 572/604 [03:43<00:12,  2.57it/s, training loss=0.179]\u001b[A\n",
      "Epoch 3:  95%|█████████▍| 573/604 [03:43<00:12,  2.58it/s, training loss=0.179]\u001b[A\n",
      "Epoch 3:  95%|█████████▍| 573/604 [03:43<00:12,  2.58it/s, training loss=0.228]\u001b[A\n",
      "Epoch 3:  95%|█████████▌| 574/604 [03:43<00:11,  2.58it/s, training loss=0.228]\u001b[A\n",
      "Epoch 3:  95%|█████████▌| 574/604 [03:44<00:11,  2.58it/s, training loss=0.174]\u001b[A\n",
      "Epoch 3:  95%|█████████▌| 575/604 [03:44<00:11,  2.58it/s, training loss=0.174]\u001b[A\n",
      "Epoch 3:  95%|█████████▌| 575/604 [03:44<00:11,  2.58it/s, training loss=0.200]\u001b[A\n",
      "Epoch 3:  95%|█████████▌| 576/604 [03:44<00:10,  2.58it/s, training loss=0.200]\u001b[A\n",
      "Epoch 3:  95%|█████████▌| 576/604 [03:44<00:10,  2.58it/s, training loss=0.108]\u001b[A\n",
      "Epoch 3:  96%|█████████▌| 577/604 [03:44<00:10,  2.58it/s, training loss=0.108]\u001b[A\n",
      "Epoch 3:  96%|█████████▌| 577/604 [03:45<00:10,  2.58it/s, training loss=0.114]\u001b[A\n",
      "Epoch 3:  96%|█████████▌| 578/604 [03:45<00:10,  2.59it/s, training loss=0.114]\u001b[A\n",
      "Epoch 3:  96%|█████████▌| 578/604 [03:45<00:10,  2.59it/s, training loss=0.121]\u001b[A\n",
      "Epoch 3:  96%|█████████▌| 579/604 [03:45<00:09,  2.59it/s, training loss=0.121]\u001b[A\n",
      "Epoch 3:  96%|█████████▌| 579/604 [03:45<00:09,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  96%|█████████▌| 580/604 [03:45<00:09,  2.59it/s, training loss=0.161]\u001b[A\n",
      "Epoch 3:  96%|█████████▌| 580/604 [03:46<00:09,  2.59it/s, training loss=0.173]\u001b[A\n",
      "Epoch 3:  96%|█████████▌| 581/604 [03:46<00:08,  2.59it/s, training loss=0.173]\u001b[A\n",
      "Epoch 3:  96%|█████████▌| 581/604 [03:46<00:08,  2.59it/s, training loss=0.202]\u001b[A\n",
      "Epoch 3:  96%|█████████▋| 582/604 [03:46<00:08,  2.59it/s, training loss=0.202]\u001b[A\n",
      "Epoch 3:  96%|█████████▋| 582/604 [03:47<00:08,  2.59it/s, training loss=0.187]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 583/604 [03:47<00:08,  2.59it/s, training loss=0.187]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 583/604 [03:47<00:08,  2.59it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 584/604 [03:47<00:07,  2.60it/s, training loss=0.159]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 584/604 [03:47<00:07,  2.60it/s, training loss=0.184]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 585/604 [03:47<00:07,  2.60it/s, training loss=0.184]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 585/604 [03:48<00:07,  2.60it/s, training loss=0.124]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 586/604 [03:48<00:06,  2.59it/s, training loss=0.124]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 586/604 [03:48<00:06,  2.59it/s, training loss=0.198]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 587/604 [03:48<00:06,  2.59it/s, training loss=0.198]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 587/604 [03:49<00:06,  2.59it/s, training loss=0.224]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 588/604 [03:49<00:06,  2.58it/s, training loss=0.224]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 588/604 [03:49<00:06,  2.58it/s, training loss=0.135]\u001b[A\n",
      "Epoch 3:  98%|█████████▊| 589/604 [03:49<00:05,  2.53it/s, training loss=0.135]\u001b[A\n",
      "Epoch 3:  98%|█████████▊| 589/604 [03:49<00:05,  2.53it/s, training loss=0.203]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  98%|█████████▊| 590/604 [03:49<00:05,  2.55it/s, training loss=0.203]\u001b[A\n",
      "Epoch 3:  98%|█████████▊| 590/604 [03:50<00:05,  2.55it/s, training loss=0.080]\u001b[A\n",
      "Epoch 3:  98%|█████████▊| 591/604 [03:50<00:05,  2.56it/s, training loss=0.080]\u001b[A\n",
      "Epoch 3:  98%|█████████▊| 591/604 [03:50<00:05,  2.56it/s, training loss=0.109]\u001b[A\n",
      "Epoch 3:  98%|█████████▊| 592/604 [03:50<00:04,  2.57it/s, training loss=0.109]\u001b[A\n",
      "Epoch 3:  98%|█████████▊| 592/604 [03:51<00:04,  2.57it/s, training loss=0.191]\u001b[A\n",
      "Epoch 3:  98%|█████████▊| 593/604 [03:51<00:04,  2.58it/s, training loss=0.191]\u001b[A\n",
      "Epoch 3:  98%|█████████▊| 593/604 [03:51<00:04,  2.58it/s, training loss=0.219]\u001b[A\n",
      "Epoch 3:  98%|█████████▊| 594/604 [03:51<00:03,  2.58it/s, training loss=0.219]\u001b[A\n",
      "Epoch 3:  98%|█████████▊| 594/604 [03:51<00:03,  2.58it/s, training loss=0.128]\u001b[A\n",
      "Epoch 3:  99%|█████████▊| 595/604 [03:51<00:03,  2.58it/s, training loss=0.128]\u001b[A\n",
      "Epoch 3:  99%|█████████▊| 595/604 [03:52<00:03,  2.58it/s, training loss=0.183]\u001b[A\n",
      "Epoch 3:  99%|█████████▊| 596/604 [03:52<00:03,  2.58it/s, training loss=0.183]\u001b[A\n",
      "Epoch 3:  99%|█████████▊| 596/604 [03:52<00:03,  2.58it/s, training loss=0.195]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 597/604 [03:52<00:02,  2.59it/s, training loss=0.195]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 597/604 [03:52<00:02,  2.59it/s, training loss=0.071]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 598/604 [03:52<00:02,  2.59it/s, training loss=0.071]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 598/604 [03:53<00:02,  2.59it/s, training loss=0.136]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 599/604 [03:53<00:01,  2.59it/s, training loss=0.136]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 599/604 [03:53<00:01,  2.59it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 600/604 [03:53<00:01,  2.60it/s, training loss=0.134]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 600/604 [03:54<00:01,  2.60it/s, training loss=0.156]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 601/604 [03:54<00:01,  2.60it/s, training loss=0.156]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 601/604 [03:54<00:01,  2.60it/s, training loss=0.138]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 602/604 [03:54<00:00,  2.60it/s, training loss=0.138]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 602/604 [03:54<00:00,  2.60it/s, training loss=0.208]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 603/604 [03:54<00:00,  2.60it/s, training loss=0.208]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 603/604 [03:55<00:00,  2.60it/s, training loss=0.236]\u001b[A\n",
      "Epoch 3: 100%|██████████| 604/604 [03:55<00:00,  2.79it/s, training loss=0.236]\u001b[A\n",
      " 67%|██████▋   | 2/3 [12:03<04:03, 243.93s/it]                                 \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch {epoch}\n",
      "Training loss: 0.496852647414468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [12:10<00:00, 243.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.6573591258595971\n",
      "F1 Score (weighted): 0.7141756869961964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  loss_train_total = 0\n",
    "\n",
    "  progress_bar = tqdm(dataloader_train,\n",
    "                      desc = 'Epoch {:1d}'.format(epoch),\n",
    "                      leave = False,\n",
    "                      disable = False\n",
    "                      )\n",
    "  \n",
    "  for batch in progress_bar:\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "    inputs = {\n",
    "        'input_ids': batch[0],\n",
    "        'attention_mask': batch[1],\n",
    "        'labels': batch[2],\n",
    "    }\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    loss = outputs[0]\n",
    "    loss_train_total += loss.item()\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    progress_bar.set_postfix({'training loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "\n",
    "  torch.save(model.state_dict(), f'BERT_ft_epoch_{epoch}.bin')\n",
    "\n",
    "  tqdm.write('\\nEpoch {epoch}')\n",
    "\n",
    "  loss_train_avg = loss_train_total/len(dataloader_train)\n",
    "  tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "\n",
    "  val_loss, predictions, true_vals = evaluate(dataloader_val)\n",
    "  val_f1 = f1_score_func(predictions, true_vals)\n",
    "  tqdm.write(f'validation loss: {val_loss}')\n",
    "  tqdm.write(f'F1 Score (weighted): {val_f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load('./BERT_ft_epoch_1.bin')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: positive\n",
      "accuracy: 664/907\n",
      "\n",
      "class: negative\n",
      "accuracy: 209/339\n",
      "\n",
      "class: neutral\n",
      "accuracy: 660/901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264238274963451904</td>\n",
       "      <td>@jjuueellzz down in the Atlantic city, ventnor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>218775148495515649</td>\n",
       "      <td>Musical awareness: Great Big Beautiful Tomorro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>258965201766998017</td>\n",
       "      <td>On Radio786 100.4fm 7:10 Fri Oct 19 Labour ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262926411352903682</td>\n",
       "      <td>Kapan sih lo ngebuktiin,jan ngomong doang Susa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171874368908050432</td>\n",
       "      <td>Excuse the connectivity of this live stream, f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                         tweet_text\n",
       "0  264238274963451904  @jjuueellzz down in the Atlantic city, ventnor...\n",
       "1  218775148495515649  Musical awareness: Great Big Beautiful Tomorro...\n",
       "2  258965201766998017  On Radio786 100.4fm 7:10 Fri Oct 19 Labour ana...\n",
       "3  262926411352903682  Kapan sih lo ngebuktiin,jan ngomong doang Susa...\n",
       "4  171874368908050432  Excuse the connectivity of this live stream, f..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_test = tokenizer.batch_encode_plus(\n",
    "    test.tweet_text.values,\n",
    "    add_special_tokens = True,\n",
    "    return_attention_mask = True,\n",
    "    pad_to_max_length = True,\n",
    "    max_length = 80,\n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_mask_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(test.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = TensorDataset(input_ids_test,\n",
    "                              attention_mask_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5398\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_test = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5398, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_val = np.argmax(predictions, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2, ..., 0, 2, 0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['predict'] = predictions_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_rat_to_sen(rating):\n",
    "    if(rating == 0):\n",
    "        return \"positive\"\n",
    "    elif(rating == 1):\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['sentiment'] = test['predict'].apply(con_rat_to_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label</th>\n",
       "      <th>predict</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264238274963451904</td>\n",
       "      <td>@jjuueellzz down in the Atlantic city, ventnor...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>218775148495515649</td>\n",
       "      <td>Musical awareness: Great Big Beautiful Tomorro...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>258965201766998017</td>\n",
       "      <td>On Radio786 100.4fm 7:10 Fri Oct 19 Labour ana...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262926411352903682</td>\n",
       "      <td>Kapan sih lo ngebuktiin,jan ngomong doang Susa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171874368908050432</td>\n",
       "      <td>Excuse the connectivity of this live stream, f...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                         tweet_text  \\\n",
       "0  264238274963451904  @jjuueellzz down in the Atlantic city, ventnor...   \n",
       "1  218775148495515649  Musical awareness: Great Big Beautiful Tomorro...   \n",
       "2  258965201766998017  On Radio786 100.4fm 7:10 Fri Oct 19 Labour ana...   \n",
       "3  262926411352903682  Kapan sih lo ngebuktiin,jan ngomong doang Susa...   \n",
       "4  171874368908050432  Excuse the connectivity of this live stream, f...   \n",
       "\n",
       "   label  predict sentiment  \n",
       "0      0        2   neutral  \n",
       "1      0        0  positive  \n",
       "2      0        2   neutral  \n",
       "3      0        0  positive  \n",
       "4      0        1  negative  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f90e3ac56d0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 7))\n",
    "sns.countplot(data = test, x = \"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    2574\n",
       "neutral     2341\n",
       "negative     483\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = {'tweet_id': test['tweet_id'], 'sentiment': test['sentiment']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264238274963451904</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>218775148495515649</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>258965201766998017</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262926411352903682</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171874368908050432</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5393</th>\n",
       "      <td>210378118865756160</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5394</th>\n",
       "      <td>245177521304399872</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5395</th>\n",
       "      <td>259280987089932288</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5396</th>\n",
       "      <td>201113950211940352</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5397</th>\n",
       "      <td>237999067286876160</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5398 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id sentiment\n",
       "0     264238274963451904   neutral\n",
       "1     218775148495515649  positive\n",
       "2     258965201766998017   neutral\n",
       "3     262926411352903682  positive\n",
       "4     171874368908050432  negative\n",
       "...                  ...       ...\n",
       "5393  210378118865756160  positive\n",
       "5394  245177521304399872  positive\n",
       "5395  259280987089932288  positive\n",
       "5396  201113950211940352   neutral\n",
       "5397  237999067286876160  positive\n",
       "\n",
       "[5398 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('submission.csv', header=True, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
